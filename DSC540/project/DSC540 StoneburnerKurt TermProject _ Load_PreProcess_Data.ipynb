{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSC540 Term Project\n",
    "Kurt Stoneburner\n",
    "\n",
    "California COVID019 Ethnicity Data\n",
    "https://data.ca.gov/dataset/covid-19-cases/resource/7e477adb-d7ab-4d4b-a198-dc4c6dc634c9\n",
    "\n",
    "API Example: https://data.ca.gov/api/3/action/datastore_search?resource_id=7e477adb-d7ab-4d4b-a198-dc4c6dc634c9&limit=5\n",
    "\n",
    "Requests Documentation: https://www.w3schools.com/python/ref_requests_response.asp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Build Dictionary to hold Global values.\n",
    "#//*** Placing Globals in a dictionary, keeps things tidy and helps with scope.\n",
    "g = {\n",
    "    #//*** Values for the API call\n",
    "    \"api\" : {\n",
    "        \"url\" : \"https://data.ca.gov\",\n",
    "        \"ethnic\" : {\n",
    "            \"url\" : \"/api/3/action/datastore_search?resource_id=7e477adb-d7ab-4d4b-a198-dc4c6dc634c9\",\n",
    "            \"colnames\" : [], #//*** Column names\n",
    "            #//*** Additional Column name attributes. Probably not needed. But ingesting anyway.\n",
    "            #//*** Key - colname, value is attributes\n",
    "            \"attrib\" : {}\n",
    "        },#//*** end Ethnic\n",
    "        \"cases\" : {\n",
    "            \"url\" : \"/api/3/action/datastore_search?resource_id=926fd08f-cc91-4828-af38-bd45de97f8c3\",\n",
    "            \"colnames\" : [], #//*** Column names\n",
    "            #//*** Additional Column name attributes. Probably not needed. But ingesting anyway.\n",
    "            #//*** Key - colname, value is attributes\n",
    "            \"attrib\" : {}, \n",
    "        }#//*** end Cases\n",
    "            \n",
    "        \n",
    "        \n",
    "    }, #//*** CLOSE api\n",
    "    \"weburl\" : \"https://covidtracking.com/data/state/california/cases\",\n",
    "\n",
    "} #//***CLOSE g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Get the whole database 100 records at a time.\n",
    "#//*** This request gets the first 100 records. Future calls are handled in a loop\n",
    "#response = requests.get(g['api']['url']+g[\"api\"][\"ethnic\"][\"url\"])\n",
    "\n",
    "#print(response)\n",
    "#print(cases_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Build a data frame returning all values from a California Data Source API\n",
    "def build_df_from_CA_API(url):\n",
    "    \n",
    "    #//*** Build the attributes for the API. This includes column names and column attributes which includes column\n",
    "    #//*** Type and other details\n",
    "\n",
    "    #//*** Request the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    #//*** Check for valid response\n",
    "    if response.ok == False:\n",
    "        #//*** Trouble with API, so some error handling here.\n",
    "        print(\"Trouble fetching API data\")\n",
    "        print(response)\n",
    "    else:\n",
    "        #//*** Valid Response\n",
    "        #//*** Convert response.content to a dictionary using JSON\n",
    "        rawOBJ=json.loads(response.content)\n",
    "        \n",
    "        #//*** Peek at the results\n",
    "        #print(rawOBJ[\"result\"].keys())\n",
    "        #for key, value in rawOBJ.ites():\n",
    "        #    print(f\"{key} : {value}\")\n",
    "        \n",
    "        #//*** Initialize list of column names\n",
    "        colnames = []\n",
    "        \n",
    "        #//*** Attrib_dict contains the attributes of each column\n",
    "        #//*** key = Column name\n",
    "        #//*** value = dictionary of attributes\n",
    "        attrib_dict = {}\n",
    "        \n",
    "        #//*** Parse the [results][fields] key for data\n",
    "        rawFields = rawOBJ[\"result\"]['fields']\n",
    "        \n",
    "        #//*** Loop through the rawfields dictionary.\n",
    "        #//*** each LoopOBJ contains a column name and column attributes\n",
    "        for loopOBJ in rawFields:\n",
    "            \n",
    "            #//*** Build temporary attributes for each loop instance\n",
    "            loopAttrib = {}\n",
    "\n",
    "            #//*** All Columns have an info field except _id.\n",
    "            if 'info' in loopOBJ.keys():\n",
    "                loopAttrib = loopOBJ[\"info\"]\n",
    "\n",
    "            #//*** Add Type to loopAttrib\n",
    "            loopAttrib['type'] = loopOBJ['type']\n",
    "\n",
    "            #//*** The column name is the ID field. Append the id field to the colnames list\n",
    "            colnames.append(loopOBJ['id'])\n",
    "\n",
    "            #//*** Assign the attributes dictionary based on column name\n",
    "            attrib_dict[ loopOBJ['id'] ] = loopAttrib\n",
    "        \n",
    "        \"\"\"\n",
    "        #//*** Display column names and attributes\n",
    "        print(f\"Column Names: {colnames}\")\n",
    "        \n",
    "        print(\"Attrib_dict\")\n",
    "        for x in colnames:\n",
    "            print(attrib_dict[x])\n",
    "        \"\"\"\n",
    "        \n",
    "        #//*************************************\n",
    "        #//*** Process the row an column data \n",
    "        #//*************************************\n",
    "\n",
    "        #//*** Build dictionary to hold raw data (rd)\n",
    "        rd = {}\n",
    "\n",
    "        #//*** Use each column as a key, create and empty list for each column\n",
    "        for x in colnames:\n",
    "            rd[x] = []\n",
    "\n",
    "        #//################################################################################################\n",
    "        #//*** While rawOBJ['success'] is true. Which implies we've successfully retrieved and API request\n",
    "        #//*** And is our loop mechanism to keep requesting records in 100 record incremenets.\n",
    "        #//################################################################################################\n",
    "        while rawOBJ[\"success\"]:\n",
    "\n",
    "            #//*** Get Records as a List for each entry\n",
    "            rawRecords = rawOBJ['result'][\"records\"]\n",
    "\n",
    "            #//*** Print a visual note for each loop iteraction / API call\n",
    "            print(f\"Processing {len(rawRecords)}\")\n",
    "\n",
    "            #//*** Parse Each Record.\n",
    "            for record in rawRecords:\n",
    "                #//*** Each Record is an object.\n",
    "                #//*** Each key is a column name.\n",
    "                #//*** Loop through the Column names and append the value to the column stored in rd\n",
    "\n",
    "                #//*** This is the sauce to that converts the object values into lists based on columns\n",
    "                #//*** It's kind of cool that the sausage is essentially made with two lines of code\n",
    "                #//*** The rest is just setup and control code.\n",
    "                for col in colnames:\n",
    "                    #//*** Assign each element to the appropriate column\n",
    "                    rd[col].append(record[col])\n",
    "\n",
    "            #//################################\n",
    "            #//*** Check if loop needs to end.\n",
    "            #//################################\n",
    "            #//*** If the number of records returned is less than the limit, we are done\n",
    "            if len(rawOBJ['result']['records']) < rawOBJ['result']['limit']:\n",
    "                print(\"Quitting Loop\")\n",
    "                break\n",
    "\n",
    "            #//*** Check if there are more records to grab\n",
    "            #//*** next contains the URL of the next request\n",
    "            #//*** The API is limited to 100 records per API request.\n",
    "            if 'next' in rawOBJ['result']['_links'].keys():\n",
    "                ##//***API CODE HERE\n",
    "                nextCall = rawOBJ['result']['_links']['next']\n",
    "\n",
    "                #//*** Add the Next value to the base API call\n",
    "                response = requests.get(g['api']['url']+nextCall)\n",
    "                rawOBJ=json.loads(response.content)\n",
    "                if rawOBJ[\"success\"] == False:\n",
    "                    #//*** Break if Success returns False\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                #//*** Quit Here\n",
    "                break\n",
    "        ########################################################\n",
    "        #//*** END while rawOBJ['success'] == True\n",
    "        #//*** Data successfully gathered to the rd dictionary\n",
    "        ########################################################\n",
    "\n",
    "        #//*** Build the dataframe\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        #//*** Create a column based on the values gathered in rd[column name]\n",
    "        for col in colnames:\n",
    "            df[col] = rd[col]\n",
    "\n",
    "        #//*** return the dataframe, column names, attribute dictionary\n",
    "        return df,colnames,attrib_dict\n",
    "\n",
    "#//*** END build_df_from_CA_API\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trouble fetching API data\n",
      "<Response [404]>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a2127db0a926>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0methnic_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'api'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"api\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ethnic\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"url\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mcovid_ethnic_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_df_from_CA_API\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0methnic_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m#covid_ethnic_df, g['api']['ethnic']['colnames'], g['api']['ethnic']['attrib'] = buil_df_from_CA_API(ethnic_url)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "#//*** Build ethnic_df from the API\n",
    "#//*** This is broken out as a function to keep the code cleaner\n",
    "####################################################################################################\n",
    "\n",
    "covid_ethnic_df = pd.DataFrame\n",
    "\n",
    "ethnic_url = g['api']['url']+g[\"api\"][\"ethnic\"][\"url\"]\n",
    "covid_ethnic_df = build_df_from_CA_API(ethnic_url)[0]\n",
    "#covid_ethnic_df, g['api']['ethnic']['colnames'], g['api']['ethnic']['attrib'] = buil_df_from_CA_API(ethnic_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "#//*** Build the covid_cases_df from the API. These are the county COVID numbers by date\n",
    "#//*** This is broken out as a function to keep the code cleaner\n",
    "####################################################################################################\n",
    "cases_url = g['api']['url']+g[\"api\"][\"cases\"][\"url\"]\n",
    "covid_cases_df = build_df_from_CA_API(cases_url)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_cases_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(covid_ethnic_df.columns)\n",
    "print(covid_ethnic_df['race_ethnicity'].unique())\n",
    "print(covid_ethnic_df['_id'].unique())\n",
    "print(covid_ethnic_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(covid_ethnic_df[covid_ethnic_df['race_ethnicity'] == 'Multi-Race'].iloc[0]['percent_ca_population'])\n",
    "print(covid_ethnic_df[covid_ethnic_df['race_ethnicity'] == 'Multiracial'].iloc[0]['percent_ca_population'])\n",
    "print(covid_ethnic_df[covid_ethnic_df['race_ethnicity'] == 'Other'].iloc[0]['percent_ca_population'])\n",
    "print(covid_ethnic_df[covid_ethnic_df['race_ethnicity'] == 'Other'].iloc[0]['deaths'])\n",
    "print(covid_ethnic_df[covid_ethnic_df['race_ethnicity'] == 'Other'].tail(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Process Website using Beautiful Soup\n",
    "response = requests.get(g['weburl'])\n",
    "\n",
    "if response.ok == True:\n",
    "    #//*** Make soup...Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('table')\n",
    "\n",
    "#//*** Get the Table Headers. These will be our data frame Columns.\n",
    "ths = table.find_all(\"th\")\n",
    "\n",
    "#//*** initialize a list to hold the column names\n",
    "colnames = []\n",
    "\n",
    "#//*** Columnnames are the first value contained in contents\n",
    "for th in ths:\n",
    "    if th.contents[0] == 'Cases (confirmed plus probable)':\n",
    "        colnames.append('confirmed')    \n",
    "    else: \n",
    "        colnames.append(th.contents[0])\n",
    "\n",
    "print(colnames)\n",
    "\n",
    "#//**********************************\n",
    "#//*** Initialize tableDict.\n",
    "#//**********************************\n",
    "#//*** tableDict is a dictionary container to hold row data.\n",
    "#//*** The tableDict will hold each of the row lists. The keys will be each colname\n",
    "tableDict = {}\n",
    "\n",
    "#//*** Initialize tableDict\n",
    "for name in colnames:\n",
    "    tableDict[name] = []\n",
    "\n",
    "#//***********************************************\n",
    "#//*** Process each tablerow\n",
    "#//*** The sausage is primarily made here\n",
    "#//***********************************************\n",
    "\n",
    "#//*** Get a BS list of table rows \n",
    "trs = table.find_all(\"tr\")\n",
    "\n",
    "#//*** For each table row in tablerows\n",
    "for tr in trs:\n",
    "    #//*** Skip the table header\n",
    "    if len(tr.find_all(\"th\")) == 0:\n",
    "        #//*** Loop through the colnames Index\n",
    "        #//*** The gets the key value to store the TD data\n",
    "        #//*** Get a TD with a corresponding index value and extract the text\n",
    "        for x in range(0,len(colnames)):\n",
    "            #//*** Append the text to the appropriate colname list.\n",
    "            #//*** Using index values keeps everthing aligned.\n",
    "            #print(tr.find_all('td')[x].find_all('span')[1].contents)\n",
    "            tableDict[colnames[x]].append(tr.find_all('td')[x].find_all('span')[1].contents[0])\n",
    "\n",
    "#print(tableDict)\n",
    "\n",
    "#//*** Remove the Probable Cases Column. These are all N/A\n",
    "colnames.remove('Probable Cases')\n",
    "colnames.remove('Confirmed cases')\n",
    "print(colnames)\n",
    "\n",
    "#//*** Build the initial dataframe\n",
    "covid_project_df = pd.DataFrame()\n",
    "\n",
    "#//*** Convert the Date Object to a datetime object\n",
    "tableDict['Date'] = [datetime.datetime.strptime(tableDict['Date'][x],'%B %d, %Y') for x in range(0,len(tableDict['Date'])) ]\n",
    "print(tableDict.keys())\n",
    "for x in range(0,len(tableDict['confirmed'])):\n",
    "    tableDict['confirmed'][x] = tableDict['confirmed'][x].replace(\",\",\"\")\n",
    "    tableDict['New cases'][x] = tableDict['New cases'][x].replace(\",\",\"\")\n",
    "\n",
    "#//*** Loop through each column name in colnames.\n",
    "#//*** Each col is a key in tableDict. Add each key / list to the dataframe\n",
    "for col in colnames:\n",
    "    if col in ['confirmed','New cases']:\n",
    "        covid_project_df[col] = pd.Series(tableDict[col])\n",
    "    else:\n",
    "        covid_project_df[col] = pd.Series(tableDict[col])\n",
    "\n",
    "covid_project_df.rename(columns = {\"New cases\":\"confirmed_new\",\"Date\" : \"date\"},inplace=True)\n",
    "print(covid_project_df.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Process Flat File: California Ethnicity demographics - cc-est2019-alldata-06.csv\n",
    "raw_ethnic_pop_df = pd.read_csv(\"cc-est2019-alldata-06.csv\")\n",
    "\n",
    "#//*** Data includes values for last twelve years. We only want data for the last year.\n",
    "\n",
    "#//*** Rebuild raw_ethnic_pop_df using only the last year (most recent) data\n",
    "raw_ethnic_pop_df = raw_ethnic_pop_df[raw_ethnic_pop_df['YEAR']==raw_ethnic_pop_df['YEAR'].max()]\n",
    "\n",
    "#//*** Ethnic data is broken down by age. At this stage we will only use the totals of all ages\n",
    "#//*** Only use AGEGRP == 0\n",
    "raw_ethnic_pop_df = raw_ethnic_pop_df[raw_ethnic_pop_df['AGEGRP']==raw_ethnic_pop_df['AGEGRP'].min()]\n",
    "\n",
    "\n",
    "raw_ethnic_pop_df.head(20)\n",
    "\n",
    "#//*** More processing below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Import data from the community resillance estimate\n",
    "raw_res_df = pd.read_csv('cre-2018-a11.csv')\n",
    "\n",
    "#//*** Only collect California Fields.\n",
    "#//*** State Code is 6. Reference the value stored in the raw_ethnic_pop_df to keep everything linked dynamically.\n",
    "#//*** You never know when data sources get moved around. At least the federal definitions of things should remain\n",
    "#//*** constant if they ever have a reason for change.\n",
    "\n",
    "#//*** California Only results\n",
    "raw_res_df = raw_res_df[raw_res_df['state'] == raw_ethnic_pop_df['STATE'].iloc[0]]\n",
    "\n",
    "#//*** Risk Factor Groups are presummed by county in tract 0\n",
    "#//*** Gather all values of tract 0\n",
    "raw_res_df = raw_res_df[raw_res_df['tract'] == 0 ]\n",
    "\n",
    "#//*** This generates a table reflecting three sets of risk factors by county, 0, 1-2, 3 or more risk factors.\n",
    "#//*** Each of these rows needs to be converted to a column that can be added as an attribute for each county in \n",
    "#//*** pop_attrib_df\n",
    "\n",
    "#//*** Use rf (risk factor) as a container for lists that will be turned back into Series.\n",
    "#//*** Adding to existing Series and DataFrames is expensive (whatever that means). \n",
    "#//*** Therefore data should be built into lists and converted back into dataframes once it's all assembled. \n",
    "#//*** Keeping the county fibs value (rf_fibs) since it's the key to keeping the data linked\n",
    "rf = {\"rf_fibs\" : [],\n",
    "      \"0rf_num\" : [],\n",
    "      \"0rf_rate\" : [],\n",
    "      \"0rf_err\" : [],\n",
    "      \"1-2rf_num\" : [],\n",
    "      \"1-2rf_rate\" : [],\n",
    "      \"1-2rf_err\" : [],\n",
    "      \"3plrf_num\" : [],\n",
    "      \"3plrf_rate\" : [],\n",
    "      \"3plrf_err\" : [],\n",
    "}\n",
    "\n",
    "#################################################################################################\n",
    "#//*** rf_key_dict associates the rfgrp value with it's corresponding dictionary list.\n",
    "#//*** The goal is to hardcode these tables, then handle the work with a generic loop thing\n",
    "#//*** If we need to adjust data collection at later time we should only have to adjust these\n",
    "#//*** Structures\n",
    "#################################################################################################\n",
    "rf_key_dict = {\n",
    "    \"0RF\" : {\"prednum\" : \"0rf_num\", \"predrt\" : \"0rf_rate\", \"predrt_moe\" : \"0rf_err\"},\n",
    "    \"1-2RF\" : {\"prednum\" : \"1-2rf_num\", \"predrt\" : \"1-2rf_rate\", \"predrt_moe\" : \"1-2rf_err\"},\n",
    "    \"3PLRF\" : {\"prednum\" : \"3plrf_num\", \"predrt\" : \"3plrf_rate\", \"predrt_moe\" : \"3plrf_err\"}\n",
    "} \n",
    "\n",
    "####################################################################################################\n",
    "#//*** My preference is to get a list of unique counties extract the data with a loop of counties. \n",
    "#//*** Let's do right by DSC530 and use the groupby command\n",
    "############################################################################\n",
    "\n",
    "#//*** BEGIN - raw_res_df.groupby('county')\n",
    "for county_tuple in raw_res_df.groupby('county'):\n",
    "    #//*** The county is the first value of the group tuple, since it's the groupby field.\n",
    "    rf[\"rf_fibs\"].append(county_tuple[0])\n",
    "    \n",
    "    #//*** Get the resulting dataframe containing just the county values\n",
    "    loop_df = county_tuple[1]\n",
    "    \n",
    "    ####################################################################################################\n",
    "    #//*** Loop through the 3 different Risk Factor Groups, by using the key values in rf_key_dict\n",
    "    ####################################################################################################\n",
    "    \n",
    "    #//*** BEGIN - rf_key in rf_key_dict.keys():\n",
    "    for rf_key in rf_key_dict.keys():\n",
    "        \n",
    "        #//*** Loop through the sub dictionary to associate the column/row value with the correct list\n",
    "        #//*** if rf.\n",
    "        #//*** rf_key - Risk Factor group 0rf, 1-2rf, 3plrf. There is one of each value per row\n",
    "        #//*** column - Data frame Column\n",
    "        #//*** key_dict_list - is the key value in rf. Each item is stored in a list.\n",
    "        \n",
    "        #//*** BEGIN - column, key_dict_list in rf_key_dict[rf_key].items():\n",
    "        for column, key_dict_list in rf_key_dict[rf_key].items():\n",
    "            #print(f\"{rf_key} - {column} : {key_dict_list}\")\n",
    "            \n",
    "            #//*** There's a lot going on here, let's spread it out to be more readable\n",
    "            #//*** Grab each risk factor grop value (rfgrp) by column (prednum,predrt,predrt_moe) \n",
    "            loop2_value = loop_df[loop_df['rfgrp'] == rf_key][column].iloc[0]\n",
    "            \n",
    "            #//*** Assign the loop2_value to the appropriate list which is defined by the value: key_dict_list\n",
    "            rf[key_dict_list].append(loop2_value)\n",
    "        \n",
    "        #//*** END - column, key_dict_list in rf_key_dict[rf_key].items():    \n",
    "        \n",
    "    #//*** END - rf_key in rf_key_dict.keys():\n",
    "    \n",
    "#//*** END - raw_res_df.groupby('county')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Build a dataframe to check our work\n",
    "risk_factors_df = pd.DataFrame()\n",
    "\n",
    "#//*** Add columns to the DataFrame by looping through rf\n",
    "for key,value in rf.items():\n",
    "        risk_factors_df[key] = pd.Series(value)\n",
    "\n",
    "#//*** Reindex to county fibs designation\n",
    "risk_factors_df = risk_factors_df.set_index('rf_fibs')\n",
    "\n",
    "#//*** Print the dataframes and check the columns match\n",
    "print(risk_factors_df.head(10))\n",
    "print(raw_res_df.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Convert Applicable federal based census codes to California Census Codes.\n",
    "#//*** Description of Federal Column Values\n",
    "#//*** https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2010-2019/cc-est2019-alldata.pdf\n",
    "\n",
    "#//*** Notably, Federal census regards Hispanic as an ethnicity not a race. For Example: People can be Hispanic White,\n",
    "#//*** Hispanic Black, or Hispanic Asian.\n",
    "#//*** California treats all hispanics as Latino\n",
    "#//*** Latino = H_MALE, H_FEMALE Hispanic\n",
    "#//*** White - NHWA_MALE, NHWA_FEMALE (Not Hispanic White)\n",
    "#//*** Asian - NHAA_MALE, NHAA_FEMALE (Not Hispanic Asian) \n",
    "#//*** Black - NHBA_MALE, NHBA_FEMALE (Not Hispanic Black) \n",
    "\n",
    "#//*** Amer Indian - NHIA_MALE, NHIA_FEMALE (Not Hispanic, American Indian) \n",
    "\n",
    "#//*** Hawaiian - NHNA_MALE, NHNA_FEMALE (Not Hispanic, Hawaiian) \n",
    "\n",
    "#//*** California has the following columns: Multiracial, Other, Multirace. I could not find a good definition of these\n",
    "#//*** These represent less than 5% of the population. Small but not too small to be ignored. These will combined into\n",
    "#//*** Single attribute Other and combined with NHTOM_MALE, NHTOM_FEMALE - Not Hispanic Two or more races\n",
    "\n",
    "#//*** Build a new data frame to hold the sanitized values.\n",
    "pop_attrib_df = pd.DataFrame()\n",
    "\n",
    "#//*** The County Fibs code is shared between the federal census data and the Community Resilliance Estimate\n",
    "pop_attrib_df['cty_fibs'] = raw_ethnic_pop_df['COUNTY']\n",
    "\n",
    "#//*** County Name will be the Common attribute to link to the timeseries Data.\n",
    "#//*** Standardize the County name. Remove County from the column name \n",
    "pop_attrib_df['county'] = raw_ethnic_pop_df['CTYNAME'].str.replace(\" County\",\"\")\n",
    "pop_attrib_df['population'] = raw_ethnic_pop_df['TOT_POP']\n",
    "\n",
    "clean_cols = { 'Latino' : ['H_MALE', 'H_FEMALE'], \n",
    "              'White' : ['NHWA_MALE', 'NHWA_FEMALE'],\n",
    "              'Asian' : ['NHAA_MALE', 'NHAA_FEMALE'],\n",
    "              'Black' : ['NHBA_MALE', 'NHBA_FEMALE'],\n",
    "              'American Indian or Alaska Native' : ['NHIA_MALE','NHIA_FEMALE'],\n",
    "              'Hawaiian' : ['NHNA_MALE', 'NHNA_FEMALE'],\n",
    "              'Multiracial' : ['NHTOM_MALE', 'NHTOM_FEMALE']\n",
    "            \n",
    "            }\n",
    "\n",
    "#//*** Combine male and female columns and store to column with same name as California Data\n",
    "#//*** Loop through the clean_cols dictionary, key is California name, value is Federal columns to combine\n",
    "#//*** These are the easy 1:1 columns\n",
    "#//*** Hawaiian and Other will need adjustment in the Califnornia Side of the Dataset.\n",
    "\n",
    "\n",
    "#//*** California Column name = Federal category male + Federal Category female\n",
    "for ca_name,fed_names in clean_cols.items():\n",
    "    pop_attrib_df[ca_name] = raw_ethnic_pop_df[fed_names[0]] + raw_ethnic_pop_df[fed_names[1]] \n",
    "\n",
    "#              'Native Hawaiian or Pacific Islander' :\n",
    "#              'Native Hawaiian and other Pacific Islander'\n",
    "#            'Other'\n",
    "\n",
    "#//*** Assign the index to the county fibs number\n",
    "pop_attrib_df = pop_attrib_df.set_index('cty_fibs')\n",
    "\n",
    "#//*** Join risk factors with pop attrib\n",
    "pop_attrib_df = pop_attrib_df.join(risk_factors_df)\n",
    "\n",
    "#//*** We've successfully combined ethnic/racial population data with estimated COVID risk factors.\n",
    "#//*** I should run some correlations just for fun. I suspect there is something interesting to find\n",
    "\n",
    "print(pop_attrib_df.head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Export all Data frames to CSVs\n",
    "\n",
    "#//*** Export API DataFrames to File\n",
    "covid_ethnic_df.to_csv(\"z_covid_ethnic_df.csv\")\n",
    "covid_cases_df.to_csv(\"z_covid_cases_df.csv\")\n",
    "\n",
    "#//*** Export pop_attrib_df to CSV\n",
    "pop_attrib_df.to_csv(\"z_pop_attrib_df.csv\")\n",
    "\n",
    "#//*** Export Covid Tracking Project Dataframe to CSV\n",
    "covid_project_df.to_csv(\"z_covid_project_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Columns to remove from imported CSVs. We should be able to kill these on import if we were cool.\n",
    "#//*** But we're not, so we'll use an expedient column delete list.\n",
    "\n",
    "del_cols = ['Unnamed: 0', '_id']\n",
    "#//*** Load datframes from file, because we mess them up\n",
    "covid_ethnic_df = pd.read_csv(\"z_covid_ethnic_df.csv\")\n",
    "covid_cases_df = pd.read_csv(\"z_covid_cases_df.csv\")\n",
    "pop_attrib_df = pd.read_csv(\"z_pop_attrib_df.csv\")\n",
    "covid_project_df = pd.read_csv(\"z_covid_project_df.csv\")\n",
    "\n",
    "#//***********************************************************************************\n",
    "#//*** Remove excess columns from read_csv\n",
    "#//*** Use the loop in case we need to delete columns that are not exclusive to all\n",
    "#//***********************************************************************************\n",
    "for x in del_cols:\n",
    "    if x in covid_cases_df.columns:\n",
    "        covid_cases_df.drop([x], axis=1, inplace=True)\n",
    "\n",
    "    if x in covid_ethnic_df.columns:\n",
    "        covid_ethnic_df.drop([x], axis=1, inplace=True)\n",
    "    \n",
    "    if x in pop_attrib_df.columns:\n",
    "        pop_attrib_df.drop([x], axis=1, inplace=True)\n",
    "    \n",
    "    if x in covid_project_df.columns:\n",
    "        covid_project_df.drop([x], axis=1, inplace=True)\n",
    "        \n",
    "#print(covid_cases_df.head())\n",
    "#print(covid_ethnic_df.head())\n",
    "#print(pop_attrib_df.head())\n",
    "#print(covid_project_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = covid_ethnic_df['race_ethnicity'].unique()\n",
    "\n",
    "for x in races:\n",
    "    print(f\"{x} {covid_ethnic_df[ covid_ethnic_df['race_ethnicity'] == x ]['race_ethnicity'].count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Sort Time Series by date and reset index\n",
    "covid_cases_df = covid_cases_df.sort_values(by='date')\n",
    "covid_ethnic_df = covid_ethnic_df.sort_values(by='date')\n",
    "\n",
    "#//*** Reset the index\n",
    "#covid_ethnic_df.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#//*** Get first Ethnic_df date\n",
    "ethnic_start_date = covid_ethnic_df['date'].iloc[0]\n",
    "print(f\"Ethinic State: {ethnic_start_date}\")\n",
    "\n",
    "#//*************************************************************************************************\n",
    "#//*** Get the iloc (index #) of the first covid_case_df entry to match the date in covid_ethic_df\n",
    "#//*** Compound code\n",
    "#//*** 1. Get the entries where the date matches ethnic start date\n",
    "#//*** 2. Get the first value from the list\n",
    "#//*** 3. Get the Index (name) of that entry\n",
    "#//*** 4. Get the iloc value of the name entry. This is the value to slice from covid_cases_df\n",
    "#//*************************************************************************************************\n",
    "#//*** I hate these, but I see the appeal\n",
    "#//*************************************************************************************************\n",
    "covid_start_iloc = covid_cases_df.index.get_loc(covid_cases_df[ covid_cases_df['date'] == ethnic_start_date].iloc[0].name)\n",
    "\n",
    "#print(covid_cases_df.iloc[covid_start_iloc])\n",
    "#//*** Merge Time Series covid_ethnic_df - covid_cases_df\n",
    "\n",
    "\n",
    "#//*** Start the Bg Table DF with a subset of\n",
    "bt_df = covid_cases_df.iloc[covid_start_iloc:]\n",
    "print\n",
    "print(covid_ethnic_df.head())\n",
    "print(bt_df.head())\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = covid_ethnic_df['race_ethnicity'].unique()\n",
    "\n",
    "for x in races:\n",
    "    print(f\"{x} {covid_ethnic_df[ covid_ethnic_df['race_ethnicity'] == x ]['race_ethnicity'].count()}\")\n",
    "    \n",
    "#//*** Reokace Native hawaiian and/or Paciic islander with Hawaiian\n",
    "#//*** It makes sense to combine these categories. Calling them all Hawaiian is a tad insensitive. I should change it.\n",
    "\n",
    "#covid_ethnic_df['race_ethnicity']=covid_ethnic_df['race_ethnicity'].str.replace('Native Hawaiian or Pacific Islander','Hawaiian')\n",
    "#covid_ethnic_df['race_ethnicity']=covid_ethnic_df['race_ethnicity'].str.replace('Native Hawaiian and other Pacific Islander','Hawaiian')\n",
    "#covid_ethnic_df['race_ethnicity']=covid_ethnic_df['race_ethnicity'].str.replace('Multi-Race','Multiracial' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "races = covid_ethnic_df['race_ethnicity'].unique()\n",
    "\n",
    "for x in races:\n",
    "    print(f\"{x} {covid_ethnic_df[ covid_ethnic_df['race_ethnicity'] == x ]['race_ethnicity'].count()}\")\n",
    "\n",
    "#//*** Convert daily ethnic covid numbers to attributes\n",
    "ethnic_dates = covid_ethnic_df.groupby('date')\n",
    "\n",
    "for date_df in ethnic_dates:\n",
    "   # print(date_df[1].stack())\n",
    "    print(date_df[1])\n",
    "    break\n",
    "\n",
    "bt_dates = bt_df.groupby('date')\n",
    "\n",
    "for date_df in bt_dates:\n",
    "    print(date_df[1].sort_values('county'))\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['Native Hawaiian or Pacific Islander','Native Hawaiian and other Pacific Islander']\n",
    "#'Hawaiian'\n",
    "\n",
    "print(f\"{covid_ethnic_df=}\".split('=')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//**** Dump all data to a database\n",
    "#//*** Build a new database\n",
    "#//*** Start from scratch each run. Therefore delete any previous version\n",
    "#//*** Databases DON'T run this way. This is for expedience. The alternative is to create a living database and \n",
    "#//*** Update the tables as needed. That's a bit \n",
    "db_filename = 'covid_data.sqldb'\n",
    "\n",
    "#//*** Delete the previous db instance if it exists.\n",
    "if os.path.exists(db_filename):\n",
    "    os.remove(db_filename)\n",
    "    \n",
    "#//*** Start a database instance\n",
    "con = sqlite3.connect(db_filename)\n",
    "\n",
    "#//*** Send each Dataframe to the database.\n",
    "#//*** We'll use a string list to name the dataframes, then convert the string name to the loop_df (loop dataframe)\n",
    "#//*** Which will do the individual database loading process\n",
    "for df_name in ['covid_ethnic_df','covid_cases_df','pop_attrib_df','covid_project_df']:\n",
    "\n",
    "    #//******************************************    \n",
    "    #//*** Build loop_df based on string name\n",
    "    #//******************************************\n",
    "    if df_name == 'covid_ethnic_df':\n",
    "        loop_df = covid_ethnic_df\n",
    "    elif df_name == 'covid_cases_df':\n",
    "        loop_df = covid_cases_df\n",
    "    elif df_name == 'pop_attrib_df':\n",
    "        loop_df = pop_attrib_df\n",
    "    elif df_name == 'covid_project_df':\n",
    "        loop_df = covid_project_df\n",
    "    else:\n",
    "        #//*** Display and error message for items missed. \n",
    "        print(f\"Failed to process: {df_name}\")\n",
    "        continue\n",
    "    loop_df.to_sql(df_name, con=con)\n",
    "\n",
    "\n",
    "#//*** Close and Exit the Database. For \n",
    "con.close()\n",
    "con.__exit__\n",
    "\n",
    "#print(covid_ethnic_df.dtypes)\n",
    "\n",
    "#for x in range(0,len(covid_ethnic_df.columns)):\n",
    "#    print(f\"{covid_ethnic_df.columns[x]} {covid_ethnic_df.dtypes[x]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
