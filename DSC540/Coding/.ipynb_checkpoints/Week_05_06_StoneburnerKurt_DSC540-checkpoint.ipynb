{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stoneburner, Kurt\n",
    "- ## DSC 540 - Week 05/06\n",
    "- ## Chapter 5, Activity7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this activity you are given the Wikipedia page where we have the GDP of all countries listed and you are asked to create three data frames from the three sources mentioned in the page ( link - https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal) )\n",
    "\n",
    "You will have to -\n",
    "\n",
    "- Open the page in a separate chrome/firefox tab and use something like `inspect element` tool to see the source HTML and understand the structure\n",
    "- Read the page using bs4\n",
    "- Find the table structure you will need to deal with (how many tables are there)\n",
    "- Find the right table using bs4\n",
    "- Separate the Source Names and their corresponding data\n",
    "- Get the source names from the list of sources you have created\n",
    "- Seperate the header and data from the data that you separated before. For the first source only. And then create a DataFrame using that\n",
    "- Repeat the last task for the other two data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //****************************************************************************************\n",
    "# //*** Set Working Directory to thinkstats folder.\n",
    "# //*** This pseudo-relative path call should work on all Stoneburner localized projects. \n",
    "# //****************************************************************************************\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "#//*** Going to use the requests library, since it's the same library used for API calls\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resource:\n",
    "- https://generalistprogrammer.com/python/python-web-scraping-tutorial-with-beautifulsoup-and-requests/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Use Requests to get the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Verify Response is ok. This *should* be analogous to checking if the response code is 200\n",
    "if response.ok == True: \n",
    "    #//*** Make soup...Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "else:\n",
    "    print(\"Problem with the URL Request\")\n",
    "    \n",
    "#//*** The tables all have the class name wikitable\n",
    "#//*** Discard the first table, which is the container for the other three.\n",
    "tables = soup.find_all('table', class_='wikitable')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Use the tableCounter to keep track of which table we are working in.\n",
    "#//*** If we were super cool, we'd tie something in to using the first row of the maintable to keep track of \n",
    "#//*** Of which table/dataframe is which. But since we are looking for very specific things, it doesn't make\n",
    "#//*** sense to invest in a more robust structure, since any slight change of source will break the whole scrape.\n",
    "\n",
    "#//*** Personally, I'd skip Beautiful Soup and just use Regex. Mostly, because regex is universal and be applied\n",
    "#//*** to other scenarios as well as other languages. It is is very useful across many tasks. I've also written \n",
    "#//*** an HTML parser in javascript using HTML. It was for a project where news talent reads scripts off of an iPad\n",
    "#//*** instead of paper scripts.\n",
    "\n",
    "#//*** The book uses a very pythonic single line method to generate a multi dimensional array. I'm not a fan of \n",
    "#//*** that in general. I prefer more readable verbose code.\n",
    "\n",
    "#//*** We'll deviate from the assignment a bit by handling all three tables in a loop.\n",
    "\n",
    "#//*** tableCounter helps keep track of which of the three tables we are working in. This is required\n",
    "#//*** when determining which dataframe to build\n",
    "tableCounter = 0\n",
    "\n",
    "#//*** Parse Each table\n",
    "for table in tables:\n",
    "    tableCounter+=1\n",
    "    \n",
    "    #//************************************\n",
    "    #//*** Build Table Headers\n",
    "    #//************************************\n",
    "    #//*** Get the Table Headers. These will be our data frame Columns.\n",
    "    ths = table.find_all(\"th\")\n",
    "    #//*** initialize a list to hold the column names\n",
    "    colnames = []\n",
    "    \n",
    "    #//*** Columnnames are the first value contained in contents\n",
    "    for th in ths:\n",
    "        colnames.append(th.contents[0])\n",
    "    \n",
    "    #//**********************************\n",
    "    #//*** Initialize tableDict.\n",
    "    #//**********************************\n",
    "    #//*** tableDict is a dictionary container to hold row data.\n",
    "    #//*** The tableDict will hold each of the row lists. The keys will be each colname\n",
    "    tableDict = {}\n",
    "    \n",
    "    #//*** Initialize tableDict\n",
    "    for name in colnames:\n",
    "        tableDict[name] = []\n",
    "    \n",
    "    #//***********************************************\n",
    "    #//*** Process each tablerow\n",
    "    #//*** The sausage is primarily made here\n",
    "    #//***********************************************\n",
    "    \n",
    "    #//*** Get a BS list of table rows\n",
    "    trs = table.find_all(\"tr\")\n",
    "    \n",
    "    #//*** For each table row in tablerows\n",
    "    for tr in trs:\n",
    "        #//*** Skip the table header\n",
    "        if len(tr.find_all(\"th\")) == 0:\n",
    "            #//*** Loop through the colnames Index\n",
    "            #//*** The gets the key value to store the TD data\n",
    "            #//*** Get a TD with a corresponding index value and extract the text\n",
    "            for x in range(0,len(colnames)):\n",
    "                #//*** Append the text to the appropriate colname list.\n",
    "                #//*** Using index values keeps everthing aligned.\n",
    "                tableDict[colnames[x]].append(tr.find_all('td')[x].text.replace(\"\\n\",\"\"))\n",
    "    \n",
    "    #//**************************************************************************************\n",
    "    #//*** Table is fully parsed into the tableDict\n",
    "    #//*** Remove the first element of each list. It contains the World Summary numbers\n",
    "    #//**************************************************************************************\n",
    "    for key in tableDict.keys():\n",
    "        tableDict[key].pop(0)\n",
    "        \n",
    "    #//*********************************************************\n",
    "    #//*** Convert tableDict into a df.\n",
    "    #//*** the individual df is determined by the tableCounter\n",
    "    #//*********************************************************\n",
    "    if tableCounter == 1:\n",
    "        #//*** Create the IMF Dataframe\n",
    "        imf_df = pd.DataFrame()\n",
    "        \n",
    "        #//*** Add each Column to dataframe\n",
    "        for x in colnames:\n",
    "            imf_df[x] = tableDict[x]\n",
    "    \n",
    "    elif tableCounter == 2:\n",
    "        #//*** Create the IMF Dataframe\n",
    "        worldbank_df = pd.DataFrame()\n",
    "        \n",
    "        #//*** Add each Column to dataframe\n",
    "        for x in colnames:\n",
    "            worldbank_df[x] = tableDict[x]\n",
    "        \n",
    "    elif tableCounter == 3:\n",
    "        #//*** Create the IMF Dataframe\n",
    "        un_df = pd.DataFrame()\n",
    "        \n",
    "        #//*** Add each Column to dataframe\n",
    "        for x in colnames:\n",
    "            un_df[x] = tableDict[x]\n",
    "    \n",
    "#//*********************************************************\n",
    "#//*** END table in tables\n",
    "#//*********************************************************\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#############\n",
      "IMF\n",
      "#############\n",
      "  Rank Country/Territory         GDP\n",
      "0    1     United States  20,807,269\n",
      "1    2   China[n 2][n 3]  14,860,775\n",
      "2    3             Japan   4,910,580\n",
      "3    4           Germany   3,780,553\n",
      "4    5    United Kingdom   2,638,296\n",
      "\n",
      "#############\n",
      "World Bank\n",
      "#############\n",
      "  Rank Country/Territory         GDP\n",
      "0    1     United States  21,427,700\n",
      "1    2        China[n 9]  14,342,903\n",
      "2    3             Japan   5,081,770\n",
      "3    4           Germany   3,845,630\n",
      "4    5             India   2,875,142\n",
      "\n",
      "#############\n",
      "UN\n",
      "#############\n",
      "  Rank Country/Territory         GDP\n",
      "0    1     United States  21,433,226\n",
      "1    2        China[n 9]  14,342,933\n",
      "2    3             Japan   5,082,465\n",
      "3    4           Germany   3,861,123\n",
      "4    5             India   2,891,582\n"
     ]
    }
   ],
   "source": [
    "        print(\"\\n#############\")\n",
    "        print(\"IMF\")\n",
    "        print(\"#############\")\n",
    "        print(imf_df.head())\n",
    "\n",
    "        print(\"\\n#############\")\n",
    "        print(\"World Bank\")\n",
    "        print(\"#############\")\n",
    "        print(worldbank_df.head())\n",
    "        \n",
    "        print(\"\\n#############\")\n",
    "        print(\"UN\")\n",
    "        print(\"#############\")\n",
    "        print(un_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6, Activity 08\n",
    "\n",
    "In this activity we do the following\n",
    "\n",
    "* Create a data frame from a given CSV\n",
    "* Check for duplicates in the columns that matter\n",
    "* Check for NaN in the columns that matter\n",
    "* Apply our domain knowledge to single out and remove outliers\n",
    "* Generate nice print statements as reports for differents steps\n",
    "\n",
    "The data set is a 1000 row data set which represnets the traffic on a certain page of a website. The Names, email, and IP are faked out in order to keep the privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data (the file name is - visit_data.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code bellow this comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task - 1 (Are there duplicates?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code bellow this comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task - 2 (do any essential column contain NaN?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code bellow this comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task - 3 (Get rid of the outliers)\n",
    "\n",
    "Consider what are the essential columns if you are preparing this dataset for a model building exercise where the target is to predict number of visits given a user name, email, IP address, Gender etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code bellow this comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task - 4 (Report the size difference)\n",
    "\n",
    "The `shape` method of a data frame gives you a tuple which represents (row, column) of the data frame, in this task you have to compare and report the number of rows before and after getting rid of the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code bellow this comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task - 5 (Box plot visit to further check any Outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code bellow this comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Insert data into a SQL Lite database – create a table with the following data (Hint: Python for Data Analysis page 191):\n",
    "\n",
    "a. Name, Address, City, State, Zip, Phone Number\n",
    "\n",
    "b. Add at least 10 rows of data and submit your code with a query generating your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //*** CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
