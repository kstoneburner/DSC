{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 9.1 ##\n",
    "### By Kurt Stoneburner ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sparkbyexamples.com/pyspark-tutorial/\n",
    "\n",
    "watermarks are discussed here:\n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sparkbyexamples.com/pyspark/pyspark-exception-java-gateway-process-exited-before-sending-the-driver-its-port-number/\n",
    "\n",
    "https://quabr.com/58723314/pyspark-failed-to-find-data-source-kafka\n",
    "\n",
    "https://search.maven.org/search?q=g:org.apache.spark%20AND%20a:spark-streaming-kafka-0-8-assembly_2.11\n",
    "\n",
    "https://www.rittmanmead.com/blog/2017/01/getting-started-with-spark-streaming-with-python-and-kafka/\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/06/setting-up-real-time-structured-streaming-with-spark-and-kafka-on-windows-os/\n",
    "\n",
    "https://spark.apache.org/docs/latest//api/python/reference/api/pyspark.SparkConf.html\n",
    "\n",
    "https://stackoverflow.com/questions/54227744/pyspark-2-x-programmatically-adding-maven-jar-coordinates-to-spark?__cpo=aHR0cHM6Ly9zdGFja292ZXJmbG93LmNvbQ\n",
    "\n",
    "https://duckduckgo.com/?t=ffab&q=SparkSession+readstream+Failed+to+find+data+source%3A+kafka&ia=web\n",
    "\n",
    "\n",
    "reintsall pyspark: https://sparkbyexamples.com/pyspark-tutorial/#pyspark-installation\n",
    "\n",
    "conda install -c conda-forge pyspark\n",
    "\n",
    "https://kontext.tech/column/spark/298/get-the-current-spark-context-settingsconfigurations\n",
    "\n",
    "Find Spark add_packages resolved the local Spark Issue.\n",
    "https://github.com/minrk/findspark/pull/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from kafka import KafkaProducer, KafkaAdminClient\n",
    "from kafka.admin.new_topic import NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import window, from_json, col\n",
    "from pyspark.sql.types import StringType, TimestampType, DoubleType, StructField, StructType\n",
    "from pyspark.sql.functions import udf\n",
    "from IPython.display import clear_output\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "checkpoint_dir = current_dir.joinpath('checkpoints')\n",
    "locations_checkpoint_dir = checkpoint_dir.joinpath('locations')\n",
    "accelerations_checkpoint_dir = checkpoint_dir.joinpath('accelerations')\n",
    "\n",
    "if locations_checkpoint_dir.exists():\n",
    "    shutil.rmtree(locations_checkpoint_dir)\n",
    "    \n",
    "if accelerations_checkpoint_dir.exists():\n",
    "    shutil.rmtree(accelerations_checkpoint_dir)\n",
    "\n",
    "locations_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "accelerations_checkpoint_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Parameters \n",
    "\n",
    "> **TODO:** Change the configuration prameters to the appropriate values for your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap_servers': ['kafka.kafka.svc.cluster.local:9092'],\n",
       " 'first_name': 'Kurt',\n",
       " 'last_name': 'Stoneburner',\n",
       " 'client_id': 'StoneburnerKurt',\n",
       " 'topic_prefix': 'StoneburnerKurt',\n",
       " 'locations_topic': 'StoneburnerKurt-locations',\n",
       " 'accelerations_topic': 'StoneburnerKurt-accelerations',\n",
       " 'simple_topic': 'StoneburnerKurt-simple'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dict(\n",
    "    bootstrap_servers=['kafka.kafka.svc.cluster.local:9092'],\n",
    "    first_name='Kurt',\n",
    "    last_name='Stoneburner'\n",
    ")\n",
    "\n",
    "config['client_id'] = '{}{}'.format(\n",
    "    config['last_name'], \n",
    "    config['first_name']\n",
    ")\n",
    "config['topic_prefix'] = '{}{}'.format(\n",
    "    config['last_name'], \n",
    "    config['first_name']\n",
    ")\n",
    "\n",
    "config['locations_topic'] = '{}-locations'.format(config['topic_prefix'])\n",
    "config['accelerations_topic'] = '{}-accelerations'.format(config['topic_prefix'])\n",
    "config['simple_topic'] = '{}-simple'.format(config['topic_prefix'])\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Topic Utility Function\n",
    "\n",
    "The `create_kafka_topic` helps create a Kafka topic based on your configuration settings.  For instance, if your first name is *John* and your last name is *Doe*, `create_kafka_topic('locations')` will create a topic with the name `DoeJohn-locations`.  The function will not create the topic if it already exists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic \"StoneburnerKurt-locations\" already exists\n",
      "Topic \"StoneburnerKurt-accelerations\" already exists\n"
     ]
    }
   ],
   "source": [
    "def create_kafka_topic(topic_name, config=config, num_partitions=1, replication_factor=1):\n",
    "    bootstrap_servers = config['bootstrap_servers']\n",
    "    client_id = config['client_id']\n",
    "    topic_prefix = config['topic_prefix']\n",
    "    name = '{}-{}'.format(topic_prefix, topic_name)\n",
    "    \n",
    "    admin_client = KafkaAdminClient(\n",
    "        bootstrap_servers=bootstrap_servers, \n",
    "        client_id=client_id\n",
    "    )\n",
    "    \n",
    "    topic = NewTopic(\n",
    "        name=name,\n",
    "        num_partitions=num_partitions,\n",
    "        replication_factor=replication_factor\n",
    "    )\n",
    "\n",
    "    topic_list = [topic]\n",
    "    try:\n",
    "        admin_client.create_topics(new_topics=topic_list)\n",
    "        print('Created topic \"{}\"'.format(name))\n",
    "    except TopicAlreadyExistsError as e:\n",
    "        print('Topic \"{}\" already exists'.format(name))\n",
    "for topic in ['locations','accelerations']:\n",
    "    create_kafka_topic(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#//*** Close Spark if already running. Guarantees Spark is loaded with current settings. \n",
    "#//*** Prevents some Ipython/Notebook related issues.\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Assignment09\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "df_locations = spark\\\n",
    "    .readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", config['bootstrap_servers'][0])\\\n",
    "    .option(\"subscribe\", config['locations_topic'])\\\n",
    "    .load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Create a data frame called `df_accelerations` that reads from the accelerations topic you published to in assignment 8.  In order to read data from this topic, make sure that you are running the notebook you created in assignment 8 that publishes acceleration and location data to the `LastnameFirstname-simple` topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accelerations = spark\\\n",
    "    .readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", config['bootstrap_servers'][0])\\\n",
    "    .option(\"subscribe\", config['accelerations_topic'])\\\n",
    "    .load()    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Create two streaming queries, `ds_locations` and `ds_accelerations` that publish to the `LastnameFirstname-simple` topic. See http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#starting-streaming-queries and http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_locations = df_locations.writeStream \\\n",
    ".format(\"kafka\") \\\n",
    ".option(\"checkpointLocation\", locations_checkpoint_dir) \\\n",
    ".option(\"kafka.bootstrap.servers\",  config['bootstrap_servers'][0]) \\\n",
    ".option(\"topic\", config['simple_topic']) \\\n",
    ".start()\n",
    "\n",
    "\n",
    "ds_accelerations = df_locations.writeStream \\\n",
    ".format(\"kafka\") \\\n",
    ".option(\"checkpointLocation\", locations_checkpoint_dir) \\\n",
    ".option(\"kafka.bootstrap.servers\",  config['bootstrap_servers'][0]) \\\n",
    ".option(\"topic\", config['simple_topic']) \\\n",
    ".start()\n",
    "try:\n",
    "    ds_locations.awaitTermination()\n",
    "    ds_accelerations.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"STOPPING STREAMING DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
