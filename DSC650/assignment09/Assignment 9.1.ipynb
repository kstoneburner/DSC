{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "\n",
    "# setup arguments\n",
    "submit_args = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = submit_args\n",
    "\n",
    "if 'PYSPARK_SUBMIT_ARGS' not in os.environ:\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = submit_args\n",
    "else:\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] += submit_args\n",
    "\n",
    "# initialize spark\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"juypter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\"\n",
    "\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "submit_args = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0'\n",
    "submit_args = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.0,org.apache.spark:spark-streaming-kafka-0-10_2.12:3.1.0'\n",
    "submit_args = '--packages org.apache.spark:spark-sql_2.13,org.apache.spark:spark-streaming-kafka-0-8_2.11'\n",
    "\n",
    "submit_args = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.0,org.apache.spark:spark-streaming-kafka-0-10_2.12:3.1.0'\n",
    "submit_args = '--master local[3] pyspark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.2.0,org.apache.spark:spark-streaming-kafka-0-10_2.13:3.2.0'\n",
    "#submit_args = '--packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.2.0,org.apache.spark:spark-streaming-kafka-0-10_2.13:3.2.0'\n",
    "#submit_args = '--master local[3] pyspark-shell'\n",
    "\n",
    "submit_args = \"--master local --packages org.apache.spark:spark-streaming-kafka-0-10_2.13:3.2.1 pyspark-shell\"\n",
    "submit_args = \"--master local --packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.1 pyspark-shell\"\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = submit_args\n",
    "\n",
    "\n",
    "\n",
    "os.environ['PYSPARK_PYTHON']=\"python\"\n",
    "#os.environ['PYSPARK_PYTHON']=\"./environment/bin/python\"\n",
    "\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']=\"juypter\"\n",
    "#os.environ['PYSPARK_DRIVER_PYTHON']=\"python\"\n",
    "# initialize spark\n",
    "#import pyspark\n",
    "#import findspark\n",
    "#findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from kafka import KafkaProducer, KafkaAdminClient\n",
    "from kafka.admin.new_topic import NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import window, from_json, col\n",
    "from pyspark.sql.types import StringType, TimestampType, DoubleType, StructField, StructType\n",
    "from pyspark.sql.functions import udf\n",
    "from IPython.display import clear_output\n",
    "import findspark\n",
    "#findspark.init()\n",
    "\n",
    "\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "checkpoint_dir = current_dir.joinpath('checkpoints')\n",
    "locations_checkpoint_dir = checkpoint_dir.joinpath('locations')\n",
    "accelerations_checkpoint_dir = checkpoint_dir.joinpath('accelerations')\n",
    "\n",
    "if locations_checkpoint_dir.exists():\n",
    "    shutil.rmtree(locations_checkpoint_dir)\n",
    "    \n",
    "if accelerations_checkpoint_dir.exists():\n",
    "    shutil.rmtree(accelerations_checkpoint_dir)\n",
    "\n",
    "locations_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "accelerations_checkpoint_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outboarding this...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "refresh_kafka = True\n",
    "import time\n",
    "#//*** Get Working Directory\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "\n",
    "#//*** Go up Two folders\n",
    "kafka_dir = current_dir.parents[2].joinpath(\"kafka_31\")\n",
    "tmp_dir = kafka_dir.joinpath(\"tmp\")\n",
    "log_dir = kafka_dir.joinpath(\"logs\")\n",
    "print(kafka_dir)\n",
    "\n",
    "\n",
    "res = os.system(str(kafka_dir)+\"\\stop_kafka-server-stop.bat\")\n",
    "print(\"Stop Running Kafka Server: \", res)\n",
    "\n",
    "\n",
    "res = os.system(str(kafka_dir)+\"\\stop_zookeeper-server-stop.bat\")\n",
    "print(\"Stop Running Zookeeper Server: \", res)\n",
    "print(\"Waiting For everything to close gracefully.....\")\n",
    "time.sleep(5)\n",
    "\n",
    "if refresh_kafka:\n",
    "    #//*** Delete Kafka Log Files\n",
    "    for root, dirs, files in os.walk(log_dir, topdown=False):\n",
    "        #//*** Delete all the log files\n",
    "        for file in files:\n",
    "            os.remove(Path(root).joinpath(file))\n",
    "        \n",
    "\n",
    "    #//*** Delete Kafka Database\n",
    "    for root, dirs, files in os.walk(tmp_dir, topdown=False):\n",
    "        #//*** Delete all the log files\n",
    "        for file in files:\n",
    "            os.remove(Path(root).joinpath(file))\n",
    "\n",
    "        for folder in dirs:\n",
    "            os.rmdir(Path(root).joinpath(folder))\n",
    "\n",
    "time.sleep(1)\n",
    "print(\"Starting Zookeeper: \")\n",
    "os.system(f\"start {str(kafka_dir)}\\start_Kafka_zookeeper.bat\")\n",
    "\n",
    "time.sleep(1)\n",
    "print(\"Starting Kafka Server: \")\n",
    "print(f\"start {str(kafka_dir)}\\start_Kafka_server.bat\")\n",
    "os.system(f\"start {str(kafka_dir)}\\start_Kafka_server.bat\")\n",
    "\"\"\"\n",
    "print(\"Outboarding this...\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Parameters \n",
    "\n",
    "> **TODO:** Change the configuration prameters to the appropriate values for your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap_servers': ['127.0.0.1:9092'],\n",
       " 'first_name': 'ition',\n",
       " 'last_name': 'Admin',\n",
       " 'client_id': 'Adminition',\n",
       " 'topic_prefix': 'Adminition',\n",
       " 'locations_topic': 'Adminition-locations',\n",
       " 'accelerations_topic': 'Adminition-accelerations',\n",
       " 'simple_topic': 'Adminition-simple'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dict(\n",
    "    bootstrap_servers=['127.0.0.1:9092'],\n",
    "    first_name='ition',\n",
    "    last_name='Admin'\n",
    ")\n",
    "\n",
    "config['client_id'] = '{}{}'.format(\n",
    "    config['last_name'], \n",
    "    config['first_name']\n",
    ")\n",
    "config['topic_prefix'] = '{}{}'.format(\n",
    "    config['last_name'], \n",
    "    config['first_name']\n",
    ")\n",
    "\n",
    "config['locations_topic'] = '{}-locations'.format(config['topic_prefix'])\n",
    "config['accelerations_topic'] = '{}-accelerations'.format(config['topic_prefix'])\n",
    "config['simple_topic'] = '{}-simple'.format(config['topic_prefix'])\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Topic Utility Function\n",
    "\n",
    "The `create_kafka_topic` helps create a Kafka topic based on your configuration settings.  For instance, if your first name is *John* and your last name is *Doe*, `create_kafka_topic('locations')` will create a topic with the name `DoeJohn-locations`.  The function will not create the topic if it already exists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic \"Adminition-locations\" already exists\n",
      "Topic \"Adminition-accelerations\" already exists\n"
     ]
    }
   ],
   "source": [
    "def create_kafka_topic(topic_name, config=config, num_partitions=1, replication_factor=1):\n",
    "    bootstrap_servers = config['bootstrap_servers']\n",
    "    client_id = config['client_id']\n",
    "    topic_prefix = config['topic_prefix']\n",
    "    name = '{}-{}'.format(topic_prefix, topic_name)\n",
    "    \n",
    "    admin_client = KafkaAdminClient(\n",
    "        bootstrap_servers=bootstrap_servers, \n",
    "        client_id=client_id\n",
    "    )\n",
    "    \n",
    "    topic = NewTopic(\n",
    "        name=name,\n",
    "        num_partitions=num_partitions,\n",
    "        replication_factor=replication_factor\n",
    "    )\n",
    "\n",
    "    topic_list = [topic]\n",
    "    try:\n",
    "        admin_client.create_topics(new_topics=topic_list)\n",
    "        print('Created topic \"{}\"'.format(name))\n",
    "    except TopicAlreadyExistsError as e:\n",
    "        print('Topic \"{}\" already exists'.format(name))\n",
    "for topic in ['locations','accelerations']:\n",
    "    create_kafka_topic(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sparkbyexamples.com/pyspark/pyspark-exception-java-gateway-process-exited-before-sending-the-driver-its-port-number/\n",
    "\n",
    "https://quabr.com/58723314/pyspark-failed-to-find-data-source-kafka\n",
    "\n",
    "https://search.maven.org/search?q=g:org.apache.spark%20AND%20a:spark-streaming-kafka-0-8-assembly_2.11\n",
    "\n",
    "https://www.rittmanmead.com/blog/2017/01/getting-started-with-spark-streaming-with-python-and-kafka/\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/06/setting-up-real-time-structured-streaming-with-spark-and-kafka-on-windows-os/\n",
    "\n",
    "https://spark.apache.org/docs/latest//api/python/reference/api/pyspark.SparkConf.html\n",
    "\n",
    "https://stackoverflow.com/questions/54227744/pyspark-2-x-programmatically-adding-maven-jar-coordinates-to-spark?__cpo=aHR0cHM6Ly9zdGFja292ZXJmbG93LmNvbQ\n",
    "\n",
    "https://duckduckgo.com/?t=ffab&q=SparkSession+readstream+Failed+to+find+data+source%3A+kafka&ia=web\n",
    "\n",
    "\n",
    "reintsall pyspark: https://sparkbyexamples.com/pyspark-tutorial/#pyspark-installation\n",
    "\n",
    "conda install -c conda-forge pyspark\n",
    "\n",
    "https://kontext.tech/column/spark/298/get-the-current-spark-context-settingsconfigurations\n",
    "\n",
    "Find Spark add_packages resolved the local Spark Issue.\n",
    "https://github.com/minrk/findspark/pull/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    ssc.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "\"spark-streaming-kafka-0-10_2.12\"\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "conf = SparkConf()\n",
    "conf.set('spark.jars.packages','org.apache.spark:spark-streaming_2.10:2.2.3',)\n",
    "conf.getAll()\n",
    "\n",
    "sc = SparkContext(conf=conf) \n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "   \n",
    "#dir(sc)\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    ssc.stop()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from pyspark.sql import SparkSession\\nSparkSession.getActiveSession()\\n\\nconfigurations = spark.sparkContext.getConf().getAll()\\nfor conf in configurations:\\n    print(conf)\\n    '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from pyspark.sql import SparkSession\n",
    "SparkSession.getActiveSession()\n",
    "\n",
    "configurations = spark.sparkContext.getConf().getAll()\n",
    "for conf in configurations:\n",
    "    print(conf)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del findspark\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import findspark\n",
    "findspark.add_packages(\"--packages org.apache.spark:spark-streaming_2.10:2.2.3,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0\")\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\apps\\spark-3.2.1-bin-hadoop3.2\n",
      "C:\\Users\\family\\anaconda3\\envs\\DSC650\\lib\\site-packages\\pyspark\n",
      "['127.0.0.1:9092']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Terminated with exception: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "#submit_args = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0'\n",
    "#submit_args = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.0,org.apache.spark:spark-streaming-kafka-0-10_2.12:3.1.0'\n",
    "#submit_args = '--master local[3] pyspark-shell --packages org.apache.spark:spark-sql_2.13,org.apache.spark:spark-streaming-kafka-0-8_2.11'\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = submit_args\n",
    "\n",
    "#pkg='org.apache.spark:spark-streaming-kafka_2.10:3.2.1'\n",
    "#pkg = \"org.apache.spark:spark-streaming-kafka-0-8-assembly_2.11:2.4.8\"\n",
    "#pkg= 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0'\n",
    "#pkg= \"--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2,org.apache.kafka:kafka-clients:2.8.1\"\n",
    "#pkg = \"--packages org.apache.spark:spark-streaming_2.10:2.2.3\"\n",
    "\n",
    "#//*** Most Recent\n",
    "\n",
    "#//*** command line: pyspark --packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0\n",
    "pkg = \"org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0\"\n",
    "\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = pkg\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 pyspark-shell'\n",
    "#os.environ['hadoop_home'] = \"C:\\\\hadoop\\\\hadoop-3.3.1\"\n",
    "os.environ['hadoop_home'] = \"C:\\\\apps\\\\spark-3.2.1-bin-hadoop3.2\"\n",
    "#os.environ['spark_home'] = \"C:\\\\apps\\\\spark-3.2.1-bin-hadoop3.2\"\n",
    "os.environ['path'] += \"C:\\\\apps\\\\spark-3.2.1-bin-hadoop3.2\\\\bin\"\n",
    "print(os.environ['hadoop_home'])\n",
    "print(os.environ['spark_home'])\n",
    "\n",
    "#//*** Close Spark if already running. Guarantees Spark is loaded with current settings. \n",
    "#//*** Prevents some Ipython/Notebook related issues.\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "        .appName(\"Assignment09\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "#.config('spark.jars.packages', pkg)\\\n",
    "print(config['bootstrap_servers'])\n",
    "df_locations = spark.readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", config['locations_topic'])\\\n",
    "    .option(\"startingOffsets\", \"earliest\")\\\n",
    "    .option(\"includeHeaders\", \"true\")\\\n",
    "    .load()\n",
    "\n",
    "df = df_locations \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"console\") \\\n",
    "  .start()\n",
    "\n",
    "time.sleep(3)\n",
    "df.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['127.0.0.1:9092']\n"
     ]
    }
   ],
   "source": [
    "#.config('spark.jars.packages', pkg)\\\n",
    "print(config['bootstrap_servers'])\n",
    "df_locations = spark.readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", config['locations_topic'])\\\n",
    "    .option(\"startingOffsets\", \"earliest\")\\\n",
    "    .option(\"includeHeaders\", \"true\")\\\n",
    "    .load()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n=== Streaming Query ===\nIdentifier: [id = 63838e7a-4816-46ba-bd39-9fa5e7c9e835, runId = c50d93cd-aa61-491f-b21a-c9c7f2d677d4]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource ConsoleWriter[numRows=20, truncate=true]\n+- Project [cast(key#38 as string) AS key#54, cast(value#39 as string) AS value#55]\n   +- StreamingDataSourceV2Relation [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44, headers#45], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@5067a66d, KafkaV2[Subscribe[Adminition-locations]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10456/4205203952.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_locations\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m   \u001b[1;33m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CAST(key AS STRING)\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"CAST(value AS STRING)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m   \u001b[1;33m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DSC650\\lib\\site-packages\\pyspark\\sql\\streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DSC650\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DSC650\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStreamingQueryException\u001b[0m: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n=== Streaming Query ===\nIdentifier: [id = 63838e7a-4816-46ba-bd39-9fa5e7c9e835, runId = c50d93cd-aa61-491f-b21a-c9c7f2d677d4]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource ConsoleWriter[numRows=20, truncate=true]\n+- Project [cast(key#38 as string) AS key#54, cast(value#39 as string) AS value#55]\n   +- StreamingDataSourceV2Relation [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44, headers#45], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@5067a66d, KafkaV2[Subscribe[Adminition-locations]]\n"
     ]
    }
   ],
   "source": [
    "#def foreach_batch_function(df, epoch_id):\n",
    "#    print(epoch_id,df)\n",
    "#df_locations.writeStream.foreachBatch(foreach_batch_function).start()  \n",
    "\n",
    "\n",
    "df = df_locations \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"console\") \\\n",
    "  .start().awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df))\n",
    "dir(df)\n",
    "print(df.exception())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_locations.selectExpr(\"CAST(key AS STRING)\",\"CAST(value AS STRING)\", \"timestamp\")\n",
    "\n",
    "#df.writeStream.format(\"console\").outputMode(\"Update\").start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sparkbyexamples.com/pyspark-tutorial/\n",
    "\n",
    "watermarks are discussed here:\n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_locations.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "#df = df_locations.selectExpr(\"CAST(key AS STRING)\",\"CAST(value AS STRING)\", \"timestamp\")\n",
    "#df.printSchema()\n",
    "#df.isStreaming\n",
    "#type(df)\n",
    "#print(df.show())\n",
    "#stream_detail_df.printSchema()\n",
    "#stream_detail_df.writeStream.format(\"console\").outputMode(\"Complete\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Create a data frame called `df_accelerations` that reads from the accelerations topic you published to in assignment 8.  In order to read data from this topic, make sure that you are running the notebook you created in assignment 8 that publishes acceleration and location data to the `LastnameFirstname-simple` topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accelerations = spark.readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", config['accelerations_topic'])\\\n",
    "    .option(\"startingOffsets\", \"earliest\")\\\n",
    "    .option(\"includeHeaders\", \"true\")\\\n",
    "    .load()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Create two streaming queries, `ds_locations` and `ds_accelerations` that publish to the `LastnameFirstname-simple` topic. See http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#starting-streaming-queries and http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_locations = ''\n",
    "\n",
    "ds_accelerations = ''\n",
    "\n",
    "try:\n",
    "    ds_locations.awaitTermination()\n",
    "    ds_accelerations.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"STOPPING STREAMING DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
