{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 9.2\n",
    "### By Kurt Stoneburner ###\n",
    "\n",
    "Pyspark Time/Session Windows:\n",
    "https://towardsdatascience.com/spark-3-2-session-windowing-feature-for-streaming-data-e404d92e267"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "\n",
    "from kafka import KafkaProducer, KafkaAdminClient\n",
    "from kafka.admin.new_topic import NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import window, from_json, col\n",
    "from pyspark.sql.types import StringType, TimestampType, DoubleType, StructField, StructType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "checkpoint_dir = current_dir.joinpath('checkpoints')\n",
    "locations_windowed_checkpoint_dir = checkpoint_dir.joinpath('locations-windowed')\n",
    "\n",
    "if locations_windowed_checkpoint_dir.exists():\n",
    "    shutil.rmtree(locations_windowed_checkpoint_dir)\n",
    "\n",
    "locations_windowed_checkpoint_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Parameters \n",
    "\n",
    "> **TODO:** Change the configuration prameters to the appropriate values for your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap_servers': ['kafka.kafka.svc.cluster.local:9092'],\n",
       " 'first_name': 'Kurt',\n",
       " 'last_name': 'Stoneburner',\n",
       " 'client_id': 'StoneburnerKurt',\n",
       " 'topic_prefix': 'StoneburnerKurt',\n",
       " 'locations_topic': 'StoneburnerKurt-locations',\n",
       " 'accelerations_topic': 'StoneburnerKurt-accelerations',\n",
       " 'windowed_topic': 'StoneburnerKurt-windowed'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dict(\n",
    "    bootstrap_servers=['kafka.kafka.svc.cluster.local:9092'],\n",
    "    first_name='Kurt',\n",
    "    last_name='Stoneburner'\n",
    ")\n",
    "\n",
    "config['client_id'] = '{}{}'.format(\n",
    "    config['last_name'], \n",
    "    config['first_name']\n",
    ")\n",
    "config['topic_prefix'] = '{}{}'.format(\n",
    "    config['last_name'], \n",
    "    config['first_name']\n",
    ")\n",
    "\n",
    "config['locations_topic'] = '{}-locations'.format(config['topic_prefix'])\n",
    "config['accelerations_topic'] = '{}-accelerations'.format(config['topic_prefix'])\n",
    "config['windowed_topic'] = '{}-windowed'.format(config['topic_prefix'])\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Topic Utility Function\n",
    "\n",
    "The `create_kafka_topic` helps create a Kafka topic based on your configuration settings.  For instance, if your first name is *John* and your last name is *Doe*, `create_kafka_topic('locations')` will create a topic with the name `DoeJohn-locations`.  The function will not create the topic if it already exists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic \"StoneburnerKurt-windowed\" already exists\n"
     ]
    }
   ],
   "source": [
    "def create_kafka_topic(topic_name, config=config, num_partitions=1, replication_factor=1):\n",
    "    bootstrap_servers = config['bootstrap_servers']\n",
    "    client_id = config['client_id']\n",
    "    topic_prefix = config['topic_prefix']\n",
    "    name = '{}-{}'.format(topic_prefix, topic_name)\n",
    "    \n",
    "    admin_client = KafkaAdminClient(\n",
    "        bootstrap_servers=bootstrap_servers, \n",
    "        client_id=client_id\n",
    "    )\n",
    "    \n",
    "    topic = NewTopic(\n",
    "        name=name,\n",
    "        num_partitions=num_partitions,\n",
    "        replication_factor=replication_factor\n",
    "    )\n",
    "\n",
    "    topic_list = [topic]\n",
    "    try:\n",
    "        admin_client.create_topics(new_topics=topic_list)\n",
    "        print('Created topic \"{}\"'.format(name))\n",
    "    except TopicAlreadyExistsError as e:\n",
    "        print('Topic \"{}\" already exists'.format(name))\n",
    "\n",
    "create_kafka_topic('windowed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** This code is identical to the code used in 9.1 to publish acceleration and location data to the `LastnameFirstname-simple` topic. You will need to add in the code you used to create the `df_accelerations` dataframe. In order to read data from this topic, make sure that you are running the notebook you created in assignment 8 that publishes acceleration and location data to the LastnameFirstname-simple topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Assignment09\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_locations = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", config['bootstrap_servers'][0]) \\\n",
    "  .option(\"subscribe\", config['locations_topic']) \\\n",
    "  .load()\n",
    "\n",
    "df_accelerations = spark\\\n",
    "    .readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", config['bootstrap_servers'][0])\\\n",
    "    .option(\"subscribe\", config['accelerations_topic'])\\\n",
    "    .load()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines a Spark schema for location and acceleration data as well as a user-defined function (UDF) for parsing the location and acceleration JSON data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_schema = StructType([\n",
    "    StructField('offset', DoubleType(), nullable=True),\n",
    "    StructField('id', StringType(), nullable=True),\n",
    "    StructField('ride_id', StringType(), nullable=True),\n",
    "    StructField('uuid', StringType(), nullable=True),\n",
    "    StructField('course', DoubleType(), nullable=True),\n",
    "    StructField('latitude', DoubleType(), nullable=True),\n",
    "    StructField('longitude', DoubleType(), nullable=True),\n",
    "    StructField('geohash', StringType(), nullable=True),\n",
    "    StructField('speed', DoubleType(), nullable=True),\n",
    "    StructField('accuracy', DoubleType(), nullable=True),\n",
    "])\n",
    "\n",
    "acceleration_schema = StructType([\n",
    "    StructField('offset', DoubleType(), nullable=True),\n",
    "    StructField('id', StringType(), nullable=True),\n",
    "    StructField('ride_id', StringType(), nullable=True),\n",
    "    StructField('uuid', StringType(), nullable=True),\n",
    "    StructField('x', DoubleType(), nullable=True),\n",
    "    StructField('y', DoubleType(), nullable=True),\n",
    "    StructField('z', DoubleType(), nullable=True),\n",
    "])\n",
    "\n",
    "udf_parse_acceleration = udf(lambda x: json.loads(x.decode('utf-8')), acceleration_schema)\n",
    "udf_parse_location = udf(lambda x: json.loads(x.decode('utf-8')), location_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#window-operations-on-event-time for details on how to implement windowed operations. \n",
    "\n",
    "The following code selects the `timestamp` column from the `df_locations` dataframe that reads from the `LastnameFirstname-locations` topic and parses the binary value using the `udf_parse_location` UDF and defines the result to the `json_value` column.\n",
    "\n",
    "```python\n",
    "df_locations \\\n",
    "  .select(\n",
    "    col('timestamp'), \n",
    "    udf_parse_location(df_locations['value']).alias('json_value')\n",
    "  )\n",
    "```\n",
    "\n",
    "From here, you can select data from the `json_value` column using the `select` method. For instance, if you saved the results of the previous code snippet to `df_locations_parsed` you could select columns from the `json_value` field and assign them aliases using the following code. \n",
    "\n",
    "```python\n",
    "df_locations_parsed.select(\n",
    "    col('timestamp'), \n",
    "    col('json_value.ride_id').alias('ride_id'),\n",
    "    col('json_value.uuid').alias('uuid'),\n",
    "    col('json_value.speed').alias('speed')\n",
    "  )\n",
    "```\n",
    "\n",
    "Next, you will want to add a watermark and group by `ride_id` and `speed` using a window duration of *30 seconds* and a slide duration of *15 seconds*. Use the `withWatermark` method in conjunction with the `groupBy` method. The [Spark streaming documentation](http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#window-operations-on-event-time) should provide examples of how to do this. \n",
    "\n",
    "Next use the `mean` aggregation method to compute the average values and rename the column `avg(speed)` to `value` and the column `ride_id` to `key`. The reason you are renaming these values is that the PySpark Kafka API expects `key` and `value` as inputs. In a production example, you would setup serialization that would handle these details for you. \n",
    "\n",
    "When you are finished, you should have a streaming query with `key` and `value` as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- json_value: struct (nullable = true)\n",
      " |    |-- offset: double (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- ride_id: string (nullable = true)\n",
      " |    |-- uuid: string (nullable = true)\n",
      " |    |-- course: double (nullable = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      " |    |-- geohash: string (nullable = true)\n",
      " |    |-- speed: double (nullable = true)\n",
      " |    |-- accuracy: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_locations_parsed = df_locations \\\n",
    "  .select(\n",
    "    col('timestamp'), \n",
    "    udf_parse_location(df_locations['value']).alias('json_value')\n",
    "  )\n",
    "\n",
    "df_locations_parsed.select(\n",
    "    col('timestamp'), \n",
    "    col('json_value.ride_id').alias('ride_id'),\n",
    "    col('json_value.uuid').alias('uuid'),\n",
    "    col('json_value.speed').alias('speed')\n",
    "  )\n",
    "\n",
    "df_locations_parsed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Group the data by session window and userId, and compute the count of each group\\n\\n    sessionizedCounts = events     .withWatermark(\"timestamp\", \"10 minutes\")     .groupBy(\\n        session_window(events.timestamp, \"5 minutes\"),\\n        events.userId)     .count()\\n\\nslidingWindows = windowing_df.withWatermark(\"timeReceived\", \"10 minutes\")\\n.groupBy(\"eventId\", window(\"timeReceived\", \"10 minutes\", \"5 minutes\")).count()slidingWindows.show(truncate = False)\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Group the data by session window and userId, and compute the count of each group\n",
    "\n",
    "    sessionizedCounts = events \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        session_window(events.timestamp, \"5 minutes\"),\n",
    "        events.userId) \\\n",
    "    .count()\n",
    "\n",
    "slidingWindows = windowing_df.withWatermark(\"timeReceived\", \"10 minutes\")\n",
    ".groupBy(\"eventId\", window(\"timeReceived\", \"10 minutes\", \"5 minutes\")).count()slidingWindows.show(truncate = False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowedSpeeds = df_locations_parsed \\\n",
    "    .withWatermark(\"timestamp\", \"10 seconds\") \\\n",
    "    .groupBy(\"json_value.ride_id\", window(\"timestamp\", \"10 seconds\", \"5 seconds\"),'json_value.speed')\\\n",
    "    .mean('json_value.speed') \\\n",
    "    .withColumnRenamed(\"ride_id\",\"key\")\\\n",
    "    .withColumnRenamed(\"avg(json_value.speed AS `speed`)\",\"value\") \\\n",
    "    .select(col('key'),col('value'))\n",
    "\n",
    "windowedSpeeds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowedSpeeds = df_locations_parsed \\\n",
    "    .withWatermark(\"timestamp\", \"10 seconds\") \\\n",
    "    .groupBy(\"json_value.ride_id\", window(\"timestamp\", \"10 seconds\", \"5 seconds\"),'json_value.speed')\\\n",
    "    .mean('json_value.speed') \\\n",
    "    .withColumnRenamed(\"ride_id\",\"key\")\\\n",
    "    .withColumnRenamed(\"avg(json_value.speed AS `speed`)\",\"value\") \\\n",
    "    .select(col('key'),col('value'))\n",
    "\n",
    "windowedSpeeds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowedSpeeds = df_locations_parsed \\\n",
    "    .withWatermark(\"timestamp\", \"10 seconds\") \\\n",
    "    .groupBy(window(\"timestamp\", \"10 seconds\", \"5 seconds\"),\"json_value.ride_id\",'json_value.speed') \\\n",
    "    .count() \\\n",
    "    .select(col('ride_id').alias(\"key\"),col('count').alias(\"value\"))\n",
    "\n",
    "\n",
    "windowedSpeeds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwindowedSpeeds = df_locations_parsed     .withWatermark(\\'timestamp\\', \"30 seconds\")     .groupBy(\\n          window(\"timestamp\", \"30 seconds\", \"15 seconds\"),\\n          \"json_value.ride_id\")     .mean(\"json_value.speed\") \\n    \\nwindowedSpeeds.printSchema()\\nwindowedSpeeds.select(windowedSpeeds.columns[2])\\n\\n#.select(col(\"json_value.ride_id\").alias(\"key\"), col(\"avg(speed)\").alias(\"value\"))\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "windowedSpeeds = df_locations_parsed \\\n",
    "    .withWatermark('timestamp', \"30 seconds\") \\\n",
    "    .groupBy(\n",
    "          window(\"timestamp\", \"30 seconds\", \"15 seconds\"),\n",
    "          \"json_value.ride_id\") \\\n",
    "    .mean(\"json_value.speed\") \n",
    "    \n",
    "windowedSpeeds.printSchema()\n",
    "windowedSpeeds.select(windowedSpeeds.columns[2])\n",
    "\n",
    "#.select(col(\"json_value.ride_id\").alias(\"key\"), col(\"avg(speed)\").alias(\"value\"))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "\n",
    "    # Transform and write batchDF\n",
    "    #print(df.printSchema())\n",
    "    try:\n",
    "        clear_output(wait=True)\n",
    "        print(df.select(df.columns).show())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    \n",
    "ds_locations = df_locations_parsed.writeStream.foreachBatch(foreach_batch_function).start()   \n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    ds_locations.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"STOPPING STREAMING DATA\")\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowedSpeeds = df_locations_parsed \\\n",
    "    .withWatermark(\"timestamp\", \"15 seconds\") \\\n",
    "    .groupBy(  \n",
    "        window(\"timestamp\", \"30 seconds\", \"15 seconds\"), \n",
    "        \"json_value.ride_id\",\n",
    "        'json_value.speed',\n",
    "        'timestamp'\n",
    "    )\\\n",
    "    .mean('json_value.speed')\\\n",
    "    .select(col('ride_id').alias('key'), col('speed').alias('value') )\n",
    "\n",
    "\n",
    "windowedSpeeds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous Jupyter cells, you should have created the `windowedSpeeds` streaming query.  Next, you will need to write that to the `LastnameFirstname-windowed` topic. If you created the `windowsSpeeds` streaming query correctly, the following should publish the results to the `LastnameFirstname-windowed` topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "Writing job aborted.\n=== Streaming Query ===\nIdentifier: [id = 60f34d1c-fc52-4f84-aeec-cdbf62465592, runId = 65cb0fa9-3da1-45a4-ba24-a59c88991847]\nCurrent Committed Offsets: {KafkaV2[Subscribe[StoneburnerKurt-locations]]: {\"StoneburnerKurt-locations\":{\"0\":11104}}}\nCurrent Available Offsets: {KafkaV2[Subscribe[StoneburnerKurt-locations]]: {\"StoneburnerKurt-locations\":{\"0\":11105}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource org.apache.spark.sql.kafka010.KafkaStreamingWrite@6b89d9cc\n+- Project [cast(key#144 as string) AS key#636, cast(value#145 as string) AS value#637]\n   +- Project [ride_id#136 AS key#144, speed#137 AS value#145]\n      +- Aggregate [window#138-T15000ms, json_value#43.ride_id, json_value#43.speed, timestamp#12-T15000ms], [window#138-T15000ms AS window#128-T15000ms, json_value#43.ride_id AS ride_id#136, json_value#43.speed AS speed#137, timestamp#12-T15000ms, avg(json_value#43.speed) AS avg(json_value.speed AS `speed`)#133]\n         +- Filter ((timestamp#12-T15000ms >= window#138-T15000ms.start) AND (timestamp#12-T15000ms < window#138-T15000ms.end))\n            +- Expand [ArrayBuffer(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) END + cast(0 as bigint)) - cast(2 as bigint)) * 15000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) END + cast(0 as bigint)) - cast(2 as bigint)) * 15000000) + 0) + 30000000), LongType, TimestampType)), timestamp#12-T15000ms, json_value#43), ArrayBuffer(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) END + cast(1 as bigint)) - cast(2 as bigint)) * 15000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) END + cast(1 as bigint)) - cast(2 as bigint)) * 15000000) + 0) + 30000000), LongType, TimestampType)), timestamp#12-T15000ms, json_value#43)], [window#138-T15000ms, timestamp#12-T15000ms, json_value#43]\n               +- EventTimeWatermark timestamp#12: timestamp, 15 seconds\n                  +- Project [timestamp#12, <lambda>(value#8) AS json_value#43]\n                     +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@31a12753, KafkaV2[Subscribe[StoneburnerKurt-locations]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-0e36252d7665>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mds_locations_windowed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"STOPPING STREAMING DATA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: Writing job aborted.\n=== Streaming Query ===\nIdentifier: [id = 60f34d1c-fc52-4f84-aeec-cdbf62465592, runId = 65cb0fa9-3da1-45a4-ba24-a59c88991847]\nCurrent Committed Offsets: {KafkaV2[Subscribe[StoneburnerKurt-locations]]: {\"StoneburnerKurt-locations\":{\"0\":11104}}}\nCurrent Available Offsets: {KafkaV2[Subscribe[StoneburnerKurt-locations]]: {\"StoneburnerKurt-locations\":{\"0\":11105}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource org.apache.spark.sql.kafka010.KafkaStreamingWrite@6b89d9cc\n+- Project [cast(key#144 as string) AS key#636, cast(value#145 as string) AS value#637]\n   +- Project [ride_id#136 AS key#144, speed#137 AS value#145]\n      +- Aggregate [window#138-T15000ms, json_value#43.ride_id, json_value#43.speed, timestamp#12-T15000ms], [window#138-T15000ms AS window#128-T15000ms, json_value#43.ride_id AS ride_id#136, json_value#43.speed AS speed#137, timestamp#12-T15000ms, avg(json_value#43.speed) AS avg(json_value.speed AS `speed`)#133]\n         +- Filter ((timestamp#12-T15000ms >= window#138-T15000ms.start) AND (timestamp#12-T15000ms < window#138-T15000ms.end))\n            +- Expand [ArrayBuffer(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) END + cast(0 as bigint)) - cast(2 as bigint)) * 15000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) END + cast(0 as bigint)) - cast(2 as bigint)) * 15000000) + 0) + 30000000), LongType, TimestampType)), timestamp#12-T15000ms, json_value#43), ArrayBuffer(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) END + cast(1 as bigint)) - cast(2 as bigint)) * 15000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#12-T15000ms, TimestampType, LongType) - 0) as double) / cast(15000000 as double))) END + cast(1 as bigint)) - cast(2 as bigint)) * 15000000) + 0) + 30000000), LongType, TimestampType)), timestamp#12-T15000ms, json_value#43)], [window#138-T15000ms, timestamp#12-T15000ms, json_value#43]\n               +- EventTimeWatermark timestamp#12: timestamp, 15 seconds\n                  +- Project [timestamp#12, <lambda>(value#8) AS json_value#43]\n                     +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@31a12753, KafkaV2[Subscribe[StoneburnerKurt-locations]]\n"
     ]
    }
   ],
   "source": [
    "ds_locations_windowed = windowedSpeeds \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka.kafka.svc.cluster.local:9092\") \\\n",
    "  .option(\"topic\", config['windowed_topic']) \\\n",
    "  .option(\"checkpointLocation\", str(locations_windowed_checkpoint_dir)) \\\n",
    ".start()\n",
    "\n",
    "try:\n",
    "    ds_locations_windowed.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"STOPPING STREAMING DATA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
