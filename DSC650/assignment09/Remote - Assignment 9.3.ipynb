{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 9.3 ##\n",
    "### By Kurt Stoneburner ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from kafka import KafkaProducer, KafkaAdminClient\n",
    "from kafka.admin.new_topic import NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import window, from_json, col, expr, to_json, struct, when\n",
    "from pyspark.sql.types import StringType, TimestampType, DoubleType, StructField, StructType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "checkpoint_dir = current_dir.joinpath('checkpoints')\n",
    "joined_checkpoint_dir = checkpoint_dir.joinpath('joined')\n",
    "locations_checkpoint_dir = checkpoint_dir.joinpath('locations')\n",
    "accelerations_checkpoint_dir = checkpoint_dir.joinpath('accelerations')\n",
    "\n",
    "if joined_checkpoint_dir.exists():\n",
    "    shutil.rmtree(joined_checkpoint_dir)\n",
    "\n",
    "joined_checkpoint_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Parameters \n",
    "\n",
    "> **TODO:** Change the configuration prameters to the appropriate values for your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap_servers': ['kafka.kafka.svc.cluster.local:9092'],\n",
       " 'first_name': 'Kurt',\n",
       " 'last_name': 'Stoneburner',\n",
       " 'client_id': 'StoneburnerKurt',\n",
       " 'topic_prefix': 'StoneburnerKurt',\n",
       " 'locations_topic': 'StoneburnerKurt-locations',\n",
       " 'accelerations_topic': 'StoneburnerKurt-accelerations',\n",
       " 'joined_topic': 'StoneburnerKurt-joined'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dict(\n",
    "    bootstrap_servers=['kafka.kafka.svc.cluster.local:9092'],\n",
    "    first_name='Kurt',\n",
    "    last_name='Stoneburner'\n",
    ")\n",
    "\n",
    "config['client_id'] = '{}{}'.format(\n",
    "    config['last_name'], \n",
    "    config['first_name']\n",
    ")\n",
    "config['topic_prefix'] = '{}{}'.format(\n",
    "    config['last_name'], \n",
    "    config['first_name']\n",
    ")\n",
    "\n",
    "config['locations_topic'] = '{}-locations'.format(config['topic_prefix'])\n",
    "config['accelerations_topic'] = '{}-accelerations'.format(config['topic_prefix'])\n",
    "config['joined_topic'] = '{}-joined'.format(config['topic_prefix'])\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Topic Utility Function\n",
    "\n",
    "The `create_kafka_topic` helps create a Kafka topic based on your configuration settings.  For instance, if your first name is *John* and your last name is *Doe*, `create_kafka_topic('locations')` will create a topic with the name `DoeJohn-locations`.  The function will not create the topic if it already exists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic \"StoneburnerKurt-joined\" already exists\n"
     ]
    }
   ],
   "source": [
    "def create_kafka_topic(topic_name, config=config, num_partitions=1, replication_factor=1):\n",
    "    bootstrap_servers = config['bootstrap_servers']\n",
    "    client_id = config['client_id']\n",
    "    topic_prefix = config['topic_prefix']\n",
    "    name = '{}-{}'.format(topic_prefix, topic_name)\n",
    "    \n",
    "    admin_client = KafkaAdminClient(\n",
    "        bootstrap_servers=bootstrap_servers, \n",
    "        client_id=client_id\n",
    "    )\n",
    "    \n",
    "    topic = NewTopic(\n",
    "        name=name,\n",
    "        num_partitions=num_partitions,\n",
    "        replication_factor=replication_factor\n",
    "    )\n",
    "\n",
    "    topic_list = [topic]\n",
    "    try:\n",
    "        admin_client.create_topics(new_topics=topic_list)\n",
    "        print('Created topic \"{}\"'.format(name))\n",
    "    except TopicAlreadyExistsError as e:\n",
    "        print('Topic \"{}\" already exists'.format(name))\n",
    "\n",
    "create_kafka_topic('joined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** This code is identical to the code used in 9.1 to publish acceleration and location data to the `LastnameFirstname-simple` topic. You will need to add in the code you used to create the `df_accelerations` dataframe. In order to read data from this topic, make sure that you are running the notebook you created in assignment 8 that publishes acceleration and location data to the LastnameFirstname-simple topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Assignment09\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_locations = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka.kafka.svc.cluster.local:9092\") \\\n",
    "  .option(\"subscribe\", config['locations_topic']) \\\n",
    "  .load()\n",
    "\n",
    "## TODO: Add code to create the df_accelerations dataframe\n",
    "df_accelerations = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka.kafka.svc.cluster.local:9092\") \\\n",
    "  .option(\"subscribe\", config['accelerations_topic']) \\\n",
    "  .load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines a Spark schema for location and acceleration data as well as a user-defined function (UDF) for parsing the location and acceleration JSON data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_schema = StructType([\n",
    "    StructField('offset', DoubleType(), nullable=True),\n",
    "    StructField('id', StringType(), nullable=True),\n",
    "    StructField('ride_id', StringType(), nullable=True),\n",
    "    StructField('uuid', StringType(), nullable=True),\n",
    "    StructField('course', DoubleType(), nullable=True),\n",
    "    StructField('latitude', DoubleType(), nullable=True),\n",
    "    StructField('longitude', DoubleType(), nullable=True),\n",
    "    StructField('geohash', StringType(), nullable=True),\n",
    "    StructField('speed', DoubleType(), nullable=True),\n",
    "    StructField('accuracy', DoubleType(), nullable=True),\n",
    "])\n",
    "\n",
    "acceleration_schema = StructType([\n",
    "    StructField('offset', DoubleType(), nullable=True),\n",
    "    StructField('id', StringType(), nullable=True),\n",
    "    StructField('ride_id', StringType(), nullable=True),\n",
    "    StructField('uuid', StringType(), nullable=True),\n",
    "    StructField('x', DoubleType(), nullable=True),\n",
    "    StructField('y', DoubleType(), nullable=True),\n",
    "    StructField('z', DoubleType(), nullable=True),\n",
    "])\n",
    "\n",
    "udf_parse_acceleration = udf(lambda x: json.loads(x.decode('utf-8')), acceleration_schema)\n",
    "udf_parse_location = udf(lambda x: json.loads(x.decode('utf-8')), location_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**  \n",
    "\n",
    "* Complete the code to create the `accelerationsWithWatermark` dataframe. \n",
    "  * Select the `timestamp` field with the alias `acceleration_timestamp`\n",
    "  * Use the `udf_parse_acceleration` UDF to parse the JSON values\n",
    "  * Select the `ride_id` as `acceleration_ride_id`\n",
    "  * Select the `x`, `y`, and `z` columns\n",
    "  * Use the same watermark timespan used in the `locationsWithWatermark` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "locationsWithWatermark = df_locations \\\n",
    "  .select(\n",
    "    col('timestamp').alias('location_timestamp'), \n",
    "    udf_parse_location(df_locations['value']).alias('json_value')\n",
    "   ) \\\n",
    "  .select(\n",
    "    col('location_timestamp'), \n",
    "    col('json_value.ride_id').alias('location_ride_id'),\n",
    "    col('json_value.speed').alias('speed'),\n",
    "    col('json_value.latitude').alias('latitude'),\n",
    "    col('json_value.longitude').alias('longitude'),\n",
    "    col('json_value.geohash').alias('geohash'),\n",
    "    col('json_value.accuracy').alias('accuracy')\n",
    "  ) \\\n",
    " .withWatermark('location_timestamp', \"2 seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_accelerations.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- accelerations_timestamp: timestamp (nullable = true)\n",
      " |-- acceleration_ride_id: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accelerationsWithWatermark = df_accelerations \\\n",
    "  .select(\n",
    "    col('timestamp').alias('accelerations_timestamp'), \n",
    "    udf_parse_acceleration(df_accelerations['value']).alias('json_value')\n",
    "   )  \\\n",
    "   .select(\n",
    "        col('accelerations_timestamp'),\n",
    "        col('json_value.ride_id').alias('acceleration_ride_id'),\n",
    "        col('json_value.x').alias('x'),\n",
    "        col('json_value.y').alias('y'),\n",
    "        col('json_value.z').alias('z'),\n",
    ")\\\n",
    ".withWatermark('accelerations_timestamp', \"2 seconds\")\n",
    "accelerationsWithWatermark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**  \n",
    "\n",
    "* Complete the code to create the `df_joined` dataframe. See http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#stream-stream-joins for additional information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ride_id: string (nullable = true)\n",
      " |-- location_timestamp: timestamp (nullable = true)\n",
      " |-- speed: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- geohash: string (nullable = true)\n",
      " |-- accuracy: double (nullable = true)\n",
      " |-- accelerations_timestamp: timestamp (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined = ''\n",
    "df_joined = locationsWithWatermark \\\n",
    ".join(accelerationsWithWatermark, \n",
    "      expr(\"\"\"\n",
    "      location_ride_id = acceleration_ride_id\n",
    "      \"\"\"\n",
    "          )) \\\n",
    ".select(\n",
    "    col('location_ride_id').alias('ride_id'),\n",
    "    col('location_timestamp'),\n",
    "    col('speed'),\n",
    "    col('latitude'),\n",
    "    col('longitude'),\n",
    "    col('geohash'),\n",
    "    col('accuracy'),\n",
    "    col('accelerations_timestamp'),\n",
    "    col('x'),\n",
    "    col('y'),\n",
    "    col('z'),\n",
    "   \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "df_joined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_joined = locationsWithWatermark \\\n",
    "#.join(accelerationsWithWatermark)\n",
    "#df_joined.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you correctly created the `df_joined` dataframe, you should be able to use the following code to create a streaming query that outputs results to the `LastnameFirstname-joined` topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "Writing job aborted.\n=== Streaming Query ===\nIdentifier: [id = 46058cd5-ae85-417e-bebc-70a3138b6ea4, runId = d25b0093-fa9f-40c3-8638-34efab131f8a]\nCurrent Committed Offsets: {KafkaV2[Subscribe[StoneburnerKurt-locations]]: {\"StoneburnerKurt-locations\":{\"0\":10705}},KafkaV2[Subscribe[StoneburnerKurt-accelerations]]: {\"StoneburnerKurt-accelerations\":{\"0\":10602}}}\nCurrent Available Offsets: {KafkaV2[Subscribe[StoneburnerKurt-locations]]: {\"StoneburnerKurt-locations\":{\"0\":10706}},KafkaV2[Subscribe[StoneburnerKurt-accelerations]]: {\"StoneburnerKurt-accelerations\":{\"0\":10602}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource org.apache.spark.sql.kafka010.KafkaStreamingWrite@5f1eb19a\n+- Project [cast(key#145 as string) AS key#159, cast(value#132 as string) AS value#160]\n   +- Project [ride_id#120, location_timestamp#42-T2000ms, speed#48, latitude#49, longitude#50, geohash#51, accuracy#52, accelerations_timestamp#66-T2000ms, x#72, y#73, z#74, value#132, ride_id#120 AS key#145]\n      +- Project [ride_id#120, location_timestamp#42-T2000ms, speed#48, latitude#49, longitude#50, geohash#51, accuracy#52, accelerations_timestamp#66-T2000ms, x#72, y#73, z#74, to_json(struct(ride_id, ride_id#120, location_timestamp, location_timestamp#42-T2000ms, speed, speed#48, latitude, latitude#49, longitude, longitude#50, geohash, geohash#51, accuracy, accuracy#52, accelerations_timestamp, accelerations_timestamp#66-T2000ms, x, x#72, y, y#73, z, z#74), Some(Etc/UTC)) AS value#132]\n         +- Project [location_ride_id#47 AS ride_id#120, location_timestamp#42-T2000ms, speed#48, latitude#49, longitude#50, geohash#51, accuracy#52, accelerations_timestamp#66-T2000ms, x#72, y#73, z#74]\n            +- Join Inner, (location_ride_id#47 = acceleration_ride_id#71)\n               :- EventTimeWatermark location_timestamp#42: timestamp, 2 seconds\n               :  +- Project [location_timestamp#42, json_value#44.ride_id AS location_ride_id#47, json_value#44.speed AS speed#48, json_value#44.latitude AS latitude#49, json_value#44.longitude AS longitude#50, json_value#44.geohash AS geohash#51, json_value#44.accuracy AS accuracy#52]\n               :     +- Project [timestamp#12 AS location_timestamp#42, <lambda>(value#8) AS json_value#44]\n               :        +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@5788f11c, KafkaV2[Subscribe[StoneburnerKurt-locations]]\n               +- EventTimeWatermark accelerations_timestamp#66: timestamp, 2 seconds\n                  +- Project [accelerations_timestamp#66, json_value#68.ride_id AS acceleration_ride_id#71, json_value#68.x AS x#72, json_value#68.y AS y#73, json_value#68.z AS z#74]\n                     +- Project [timestamp#33 AS accelerations_timestamp#66, <lambda>(value#29) AS json_value#68]\n                        +- StreamingDataSourceV2Relation [key#28, value#29, topic#30, partition#31, offset#32L, timestamp#33, timestampType#34], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@791cfe1e, KafkaV2[Subscribe[StoneburnerKurt-accelerations]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-493b02e2e50e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mds_joined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"STOPPING STREAMING DATA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: Writing job aborted.\n=== Streaming Query ===\nIdentifier: [id = 46058cd5-ae85-417e-bebc-70a3138b6ea4, runId = d25b0093-fa9f-40c3-8638-34efab131f8a]\nCurrent Committed Offsets: {KafkaV2[Subscribe[StoneburnerKurt-locations]]: {\"StoneburnerKurt-locations\":{\"0\":10705}},KafkaV2[Subscribe[StoneburnerKurt-accelerations]]: {\"StoneburnerKurt-accelerations\":{\"0\":10602}}}\nCurrent Available Offsets: {KafkaV2[Subscribe[StoneburnerKurt-locations]]: {\"StoneburnerKurt-locations\":{\"0\":10706}},KafkaV2[Subscribe[StoneburnerKurt-accelerations]]: {\"StoneburnerKurt-accelerations\":{\"0\":10602}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource org.apache.spark.sql.kafka010.KafkaStreamingWrite@5f1eb19a\n+- Project [cast(key#145 as string) AS key#159, cast(value#132 as string) AS value#160]\n   +- Project [ride_id#120, location_timestamp#42-T2000ms, speed#48, latitude#49, longitude#50, geohash#51, accuracy#52, accelerations_timestamp#66-T2000ms, x#72, y#73, z#74, value#132, ride_id#120 AS key#145]\n      +- Project [ride_id#120, location_timestamp#42-T2000ms, speed#48, latitude#49, longitude#50, geohash#51, accuracy#52, accelerations_timestamp#66-T2000ms, x#72, y#73, z#74, to_json(struct(ride_id, ride_id#120, location_timestamp, location_timestamp#42-T2000ms, speed, speed#48, latitude, latitude#49, longitude, longitude#50, geohash, geohash#51, accuracy, accuracy#52, accelerations_timestamp, accelerations_timestamp#66-T2000ms, x, x#72, y, y#73, z, z#74), Some(Etc/UTC)) AS value#132]\n         +- Project [location_ride_id#47 AS ride_id#120, location_timestamp#42-T2000ms, speed#48, latitude#49, longitude#50, geohash#51, accuracy#52, accelerations_timestamp#66-T2000ms, x#72, y#73, z#74]\n            +- Join Inner, (location_ride_id#47 = acceleration_ride_id#71)\n               :- EventTimeWatermark location_timestamp#42: timestamp, 2 seconds\n               :  +- Project [location_timestamp#42, json_value#44.ride_id AS location_ride_id#47, json_value#44.speed AS speed#48, json_value#44.latitude AS latitude#49, json_value#44.longitude AS longitude#50, json_value#44.geohash AS geohash#51, json_value#44.accuracy AS accuracy#52]\n               :     +- Project [timestamp#12 AS location_timestamp#42, <lambda>(value#8) AS json_value#44]\n               :        +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@5788f11c, KafkaV2[Subscribe[StoneburnerKurt-locations]]\n               +- EventTimeWatermark accelerations_timestamp#66: timestamp, 2 seconds\n                  +- Project [accelerations_timestamp#66, json_value#68.ride_id AS acceleration_ride_id#71, json_value#68.x AS x#72, json_value#68.y AS y#73, json_value#68.z AS z#74]\n                     +- Project [timestamp#33 AS accelerations_timestamp#66, <lambda>(value#29) AS json_value#68]\n                        +- StreamingDataSourceV2Relation [key#28, value#29, topic#30, partition#31, offset#32L, timestamp#33, timestampType#34], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@791cfe1e, KafkaV2[Subscribe[StoneburnerKurt-accelerations]]\n"
     ]
    }
   ],
   "source": [
    "ds_joined = df_joined \\\n",
    "  .withColumn(\n",
    "    'value', \n",
    "    to_json(\n",
    "        struct(\n",
    "            'ride_id', 'location_timestamp', 'speed', \n",
    "            'latitude', 'longitude', 'geohash', 'accuracy', \n",
    "            'accelerations_timestamp', 'x', 'y', 'z'\n",
    "        )\n",
    "    )\n",
    "    ).withColumn(\n",
    "     'key', col('ride_id')\n",
    "    ) \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka.kafka.svc.cluster.local:9092\") \\\n",
    "  .option(\"topic\", config['joined_topic']) \\\n",
    "  .option(\"checkpointLocation\", str(joined_checkpoint_dir)) \\\n",
    "  .start()\n",
    "\n",
    "try:\n",
    "    ds_joined.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"STOPPING STREAMING DATA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
