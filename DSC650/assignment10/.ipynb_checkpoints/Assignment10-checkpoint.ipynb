{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stoneburner, Kurt\n",
    "- ## DSC 650 - Assignment 10\n",
    "\n",
    "\n",
    "Links to Deep Learning Sample Code:\n",
    "- https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part01_introduction.ipynb\n",
    "\n",
    "- https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part02_sequence-models.ipynb\n",
    "\n",
    "- https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part03_transformer.ipynb\n",
    "\n",
    "- https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part04_sequence-to-sequence-learning.ipynb\n",
    "\n",
    "ngram reference:\n",
    "- https://www.analyticsvidhya.com/blog/2021/09/what-are-n-grams-and-how-to-implement-them-in-python/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "#//*** Reusing Code from assignment 04\n",
    "from chardet.universaldetector import UniversalDetector\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n"
     ]
    }
   ],
   "source": [
    "#//*** Get Working Directory\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "\n",
    "#//*** Go up Two folders\n",
    "project_dir = current_dir.parents[2]\n",
    "\n",
    "#//*** IMDB Data Path\n",
    "imdb_path = project_dir.joinpath(\"dsc650/data/external/imdb/aclImdb\")\n",
    "\n",
    "file_path = imdb_path.joinpath(\"train/pos\")\n",
    "\n",
    "#//*** Grab the first positive review text for testing\n",
    "file_path = file_path.joinpath(os.listdir(file_path)[0])\n",
    "\n",
    "with open(file_path,'r') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "print(sample_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Randomly assign 20% of the training Data and copy to a validation folder\n",
    "import os, pathlib, shutil, random\n",
    "\n",
    "val_dir = imdb_path.joinpath(\"val\")\n",
    "train_dir = imdb_path.joinpath(\"train\")\n",
    "test_dir = imdb_path.joinpath(\"test\")\n",
    "\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    #//*** Skip if val folder exists (Delete Folder to resample)\n",
    "    if os.path.exists(val_dir.joinpath(category)):\n",
    "        break\n",
    "    \n",
    "    os.makedirs(val_dir.joinpath(category))\n",
    "    files = os.listdir(train_dir.joinpath(category))\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load IMDB Dataset #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Raw Validation Set\n",
      "Loading Raw Train Data\n",
      "Dropping File: 7714_1.txt due to decoding issues\n",
      "Dropping File: 11351_9.txt due to decoding issues\n",
      "Dropping File: 8263_9.txt due to decoding issues\n",
      "Loading Raw Test Data\n",
      "Dropping File: 4414_1.txt due to decoding issues\n",
      "Dropping File: 6973_1.txt due to decoding issues\n",
      "Dropping File: 2464_10.txt due to decoding issues\n",
      "Dropping File: 5281_10.txt due to decoding issues\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#//*** Use Universal Detector to determine file encoding.\n",
    "#//*** Borrowed from Assignment04\n",
    "def read_file_with_encoding(filepath):\n",
    "\n",
    "    detector = UniversalDetector()\n",
    "    \n",
    "    try:\n",
    "        with open(filepath) as f:\n",
    "            return f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        detector.reset()\n",
    "        with open(filepath, 'rb') as f:\n",
    "            for line in f.readlines():\n",
    "                detector.feed(line)\n",
    "                if detector.done:\n",
    "                    break\n",
    "        detector.close()\n",
    "        encoding = detector.result['encoding']\n",
    "        with open(filepath, encoding=encoding) as f:\n",
    "            return f.read()\n",
    "\n",
    "#//*** Borrowed from Assignment04\n",
    "def parse_html_payload(payload):\n",
    "    \"\"\"\n",
    "    This function uses Beautiful Soup to read HTML data\n",
    "    and return the text.  If the payload is plain text, then\n",
    "    Beautiful Soup will return the original content\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(payload, 'html.parser')\n",
    "    return str(soup.get_text()).encode('utf-8').decode('utf-8')\n",
    "\n",
    "def load_dataset(dir_path):\n",
    "    \n",
    "    text = []\n",
    "    targets = []\n",
    "    \n",
    "    #//*** Crawl the neg and pos folders\n",
    "    for category in (\"neg\", \"pos\"):\n",
    "        files = os.listdir(dir_path.joinpath(category))\n",
    "        \n",
    "        #//*** Loop through each file in the folder\n",
    "        for file in files:\n",
    "            try:\n",
    "                #//*** Add processed file to text\n",
    "                text.append(\n",
    "                    #//*** Strip HTML Tags\n",
    "                    parse_html_payload(\n",
    "                        #//*** Read File from disk. Function uses Universal Detector to determine file encoding\n",
    "                        read_file_with_encoding(\n",
    "                            dir_path.joinpath(category).joinpath(file))))\n",
    "\n",
    "                #//*** Append Target Value\n",
    "                if category == 'neg':\n",
    "                    targets.append(0)\n",
    "                else:\n",
    "                    targets.append(1)\n",
    "            except:\n",
    "                print(f\"Dropping File: {file} due to decoding issues\")\n",
    "    return text,targets\n",
    "print(\"Loading Raw Validation Set\")\n",
    "raw_val_text, val_targets = load_dataset(val_dir)\n",
    "\n",
    "print(\"Loading Raw Train Data\")\n",
    "raw_train_text, train_targets = load_dataset(train_dir)\n",
    "\n",
    "print(\"Loading Raw Test Data\")\n",
    "raw_test_text, test_targets = load_dataset(test_dir)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10.1 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text: (First 500 Chars)\n",
      "Airport '77 starts as a brand new luxury 747 plane is loaded up with valuable paintings & such belonging to rich businessman Philip Stevens (James Stewart) who is flying them & a bunch of VIP's to his estate in preparation of it being opened to the public as a museum, also on board is Stevens daughter Julie (Kathleen Quinlan) & her son. The luxury jetliner takes off as planned but mid-air the plane is hi-jacked by the co-pilot Chambers (Robert Foxworth) & his two accomplice's Banker (Monte Markh\n",
      "====\n",
      "This film lacked something I couldn't put my finger on at first: charisma on the part of the leading actress. This inevitably translated to lack of chemistry when she shared the screen with her leading man. Even the romantic scenes came across as being merely the actors at play. It could very well have been the director who miscalculated what he needed from the actors. I just don't know.But could it have been the screenplay? Just exactly who was the chef in love with? He seemed more enamored of \n",
      "====\n",
      "Sorry everyone,,, I know this is supposed to be an \"art\" film,, but wow, they should have handed out guns at the screening so people could blow their brains out and not watch. Although the scene design and photographic direction was excellent, this story is too painful to watch. The absence of a sound track was brutal. The loooonnnnng shots were too long. How long can you watch two people just sitting there and talking? Especially when the dialogue is two people complaining. I really had a hard \n",
      "====\n",
      "Vijay Krishna Acharya's 'Tashan' is a over-hyped, stylized, product. Sure its a one of the most stylish films, but when it comes to content, even the masses will reject this one. Why? The films script is as amateur as a 2 year old baby. Script is king, without a good script even the greatest director of all-time cannot do anything. Tashan is produced by the most successful production banner 'Yash Raj Films' and Mega Stars appearing in it. But nothing on earth can save you if you script is bland.\n",
      "====\n",
      "The worst movie I have seen since Tera Jadoo Chal Gaya. There is no story, no humor, no nothing! The action sequences seem more like a series of haphazard Akshay Kumar Thumbs-Up advertisements stitched together. Heavily influenced from The Matrix and Kung-Fu Hustle but very poorly executed.I did not go a lot of expectations, but watching this movie is an exasperating experience which makes you wonder \"What were these guys thinking??!!\".The only thing you might remember after watching it is an an\n",
      "====\n",
      "\n",
      "\n",
      "Tokens: (First 100 tokens)\n",
      "['airport', '77', 'starts', 'as', 'a', 'brand', 'new', 'luxury', '747', 'plane', 'is', 'loaded', 'up', 'with', 'valuable', 'paintings', 'such', 'belonging', 'to', 'rich', 'businessman', 'philip', 'stevens', 'james', 'stewart', 'who', 'is', 'flying', 'them', 'a', 'bunch', 'of', 'vips', 'to', 'his', 'estate', 'in', 'preparation', 'of', 'it', 'being', 'opened', 'to', 'the', 'public', 'as', 'a', 'museum', 'also', 'on', 'board', 'is', 'stevens', 'daughter', 'julie', 'kathleen', 'quinlan', 'her', 'son', 'the', 'luxury', 'jetliner', 'takes', 'off', 'as', 'planned', 'but', 'midair', 'the', 'plane', 'is', 'hijacked', 'by', 'the', 'copilot', 'chambers', 'robert', 'foxworth', 'his', 'two', 'accomplices', 'banker', 'monte', 'markham', 'wilson', 'michael', 'pataki', 'who', 'knock', 'the', 'passengers', 'crew', 'out', 'with', 'sleeping', 'gas', 'they', 'plan', 'to', 'steal']\n",
      "====\n",
      "['this', 'film', 'lacked', 'something', 'i', 'couldnt', 'put', 'my', 'finger', 'on', 'at', 'first', 'charisma', 'on', 'the', 'part', 'of', 'the', 'leading', 'actress', 'this', 'inevitably', 'translated', 'to', 'lack', 'of', 'chemistry', 'when', 'she', 'shared', 'the', 'screen', 'with', 'her', 'leading', 'man', 'even', 'the', 'romantic', 'scenes', 'came', 'across', 'as', 'being', 'merely', 'the', 'actors', 'at', 'play', 'it', 'could', 'very', 'well', 'have', 'been', 'the', 'director', 'who', 'miscalculated', 'what', 'he', 'needed', 'from', 'the', 'actors', 'i', 'just', 'dont', 'knowbut', 'could', 'it', 'have', 'been', 'the', 'screenplay', 'just', 'exactly', 'who', 'was', 'the', 'chef', 'in', 'love', 'with', 'he', 'seemed', 'more', 'enamored', 'of', 'his', 'culinary', 'skills', 'and', 'restaurant', 'and', 'ultimately', 'of', 'himself', 'and', 'his']\n",
      "====\n",
      "['sorry', 'everyone', 'i', 'know', 'this', 'is', 'supposed', 'to', 'be', 'an', 'art', 'film', 'but', 'wow', 'they', 'should', 'have', 'handed', 'out', 'guns', 'at', 'the', 'screening', 'so', 'people', 'could', 'blow', 'their', 'brains', 'out', 'and', 'not', 'watch', 'although', 'the', 'scene', 'design', 'and', 'photographic', 'direction', 'was', 'excellent', 'this', 'story', 'is', 'too', 'painful', 'to', 'watch', 'the', 'absence', 'of', 'a', 'sound', 'track', 'was', 'brutal', 'the', 'loooonnnnng', 'shots', 'were', 'too', 'long', 'how', 'long', 'can', 'you', 'watch', 'two', 'people', 'just', 'sitting', 'there', 'and', 'talking', 'especially', 'when', 'the', 'dialogue', 'is', 'two', 'people', 'complaining', 'i', 'really', 'had', 'a', 'hard', 'time', 'just', 'getting', 'through', 'this', 'film', 'the', 'performances', 'were', 'excellent', 'but', 'how']\n",
      "====\n",
      "['vijay', 'krishna', 'acharyas', 'tashan', 'is', 'a', 'overhyped', 'stylized', 'product', 'sure', 'its', 'a', 'one', 'of', 'the', 'most', 'stylish', 'films', 'but', 'when', 'it', 'comes', 'to', 'content', 'even', 'the', 'masses', 'will', 'reject', 'this', 'one', 'why', 'the', 'films', 'script', 'is', 'as', 'amateur', 'as', 'a', '2', 'year', 'old', 'baby', 'script', 'is', 'king', 'without', 'a', 'good', 'script', 'even', 'the', 'greatest', 'director', 'of', 'alltime', 'cannot', 'do', 'anything', 'tashan', 'is', 'produced', 'by', 'the', 'most', 'successful', 'production', 'banner', 'yash', 'raj', 'films', 'and', 'mega', 'stars', 'appearing', 'in', 'it', 'but', 'nothing', 'on', 'earth', 'can', 'save', 'you', 'if', 'you', 'script', 'is', 'bland', 'thumbs', 'down', 'performances', 'anil', 'kapoor', 'is', 'a', 'veteran', 'actor', 'but']\n",
      "====\n",
      "['the', 'worst', 'movie', 'i', 'have', 'seen', 'since', 'tera', 'jadoo', 'chal', 'gaya', 'there', 'is', 'no', 'story', 'no', 'humor', 'no', 'nothing', 'the', 'action', 'sequences', 'seem', 'more', 'like', 'a', 'series', 'of', 'haphazard', 'akshay', 'kumar', 'thumbsup', 'advertisements', 'stitched', 'together', 'heavily', 'influenced', 'from', 'the', 'matrix', 'and', 'kungfu', 'hustle', 'but', 'very', 'poorly', 'executedi', 'did', 'not', 'go', 'a', 'lot', 'of', 'expectations', 'but', 'watching', 'this', 'movie', 'is', 'an', 'exasperating', 'experience', 'which', 'makes', 'you', 'wonder', 'what', 'were', 'these', 'guys', 'thinkingthe', 'only', 'thing', 'you', 'might', 'remember', 'after', 'watching', 'it', 'is', 'an', 'anorexic', 'kareena', 'in', 'a', 'bikinithe', 'reason', 'why', 'i', 'did', 'not', 'give', 'a', 'rating', 'of', '1', 'is', 'that', 'every', 'time']\n",
      "====\n",
      "\n",
      "\n",
      "ngrams: (First 50 tokens)\n",
      "['airport 77', '77 starts', 'starts as', 'as a', 'a brand', 'brand new', 'new luxury', 'luxury 747', '747 plane', 'plane is', 'is loaded', 'loaded up', 'up with', 'with valuable', 'valuable paintings', 'paintings such', 'such belonging', 'belonging to', 'to rich', 'rich businessman', 'businessman philip', 'philip stevens', 'stevens james', 'james stewart', 'stewart who', 'who is', 'is flying', 'flying them', 'them a', 'a bunch', 'bunch of', 'of vips', 'vips to', 'to his', 'his estate', 'estate in', 'in preparation', 'preparation of', 'of it', 'it being', 'being opened', 'opened to', 'to the', 'the public', 'public as', 'as a', 'a museum', 'museum also', 'also on', 'on board', 'board is', 'is stevens', 'stevens daughter', 'daughter julie', 'julie kathleen', 'kathleen quinlan', 'quinlan her', 'her son', 'son the', 'the luxury', 'luxury jetliner', 'jetliner takes', 'takes off', 'off as', 'as planned', 'planned but', 'but midair', 'midair the', 'the plane', 'plane is', 'is hijacked', 'hijacked by', 'by the', 'the copilot', 'copilot chambers', 'chambers robert', 'robert foxworth', 'foxworth his', 'his two', 'two accomplices', 'accomplices banker', 'banker monte', 'monte markham', 'markham wilson', 'wilson michael', 'michael pataki', 'pataki who', 'who knock', 'knock the', 'the passengers', 'passengers crew', 'crew out', 'out with', 'with sleeping', 'sleeping gas', 'gas they', 'they plan', 'plan to', 'to steal', 'steal the']\n",
      "====\n",
      "['this film', 'film lacked', 'lacked something', 'something i', 'i couldnt', 'couldnt put', 'put my', 'my finger', 'finger on', 'on at', 'at first', 'first charisma', 'charisma on', 'on the', 'the part', 'part of', 'of the', 'the leading', 'leading actress', 'actress this', 'this inevitably', 'inevitably translated', 'translated to', 'to lack', 'lack of', 'of chemistry', 'chemistry when', 'when she', 'she shared', 'shared the', 'the screen', 'screen with', 'with her', 'her leading', 'leading man', 'man even', 'even the', 'the romantic', 'romantic scenes', 'scenes came', 'came across', 'across as', 'as being', 'being merely', 'merely the', 'the actors', 'actors at', 'at play', 'play it', 'it could', 'could very', 'very well', 'well have', 'have been', 'been the', 'the director', 'director who', 'who miscalculated', 'miscalculated what', 'what he', 'he needed', 'needed from', 'from the', 'the actors', 'actors i', 'i just', 'just dont', 'dont knowbut', 'knowbut could', 'could it', 'it have', 'have been', 'been the', 'the screenplay', 'screenplay just', 'just exactly', 'exactly who', 'who was', 'was the', 'the chef', 'chef in', 'in love', 'love with', 'with he', 'he seemed', 'seemed more', 'more enamored', 'enamored of', 'of his', 'his culinary', 'culinary skills', 'skills and', 'and restaurant', 'restaurant and', 'and ultimately', 'ultimately of', 'of himself', 'himself and', 'and his', 'his youthful']\n",
      "====\n",
      "['sorry everyone', 'everyone i', 'i know', 'know this', 'this is', 'is supposed', 'supposed to', 'to be', 'be an', 'an art', 'art film', 'film but', 'but wow', 'wow they', 'they should', 'should have', 'have handed', 'handed out', 'out guns', 'guns at', 'at the', 'the screening', 'screening so', 'so people', 'people could', 'could blow', 'blow their', 'their brains', 'brains out', 'out and', 'and not', 'not watch', 'watch although', 'although the', 'the scene', 'scene design', 'design and', 'and photographic', 'photographic direction', 'direction was', 'was excellent', 'excellent this', 'this story', 'story is', 'is too', 'too painful', 'painful to', 'to watch', 'watch the', 'the absence', 'absence of', 'of a', 'a sound', 'sound track', 'track was', 'was brutal', 'brutal the', 'the loooonnnnng', 'loooonnnnng shots', 'shots were', 'were too', 'too long', 'long how', 'how long', 'long can', 'can you', 'you watch', 'watch two', 'two people', 'people just', 'just sitting', 'sitting there', 'there and', 'and talking', 'talking especially', 'especially when', 'when the', 'the dialogue', 'dialogue is', 'is two', 'two people', 'people complaining', 'complaining i', 'i really', 'really had', 'had a', 'a hard', 'hard time', 'time just', 'just getting', 'getting through', 'through this', 'this film', 'film the', 'the performances', 'performances were', 'were excellent', 'excellent but', 'but how', 'how much']\n",
      "====\n",
      "['vijay krishna', 'krishna acharyas', 'acharyas tashan', 'tashan is', 'is a', 'a overhyped', 'overhyped stylized', 'stylized product', 'product sure', 'sure its', 'its a', 'a one', 'one of', 'of the', 'the most', 'most stylish', 'stylish films', 'films but', 'but when', 'when it', 'it comes', 'comes to', 'to content', 'content even', 'even the', 'the masses', 'masses will', 'will reject', 'reject this', 'this one', 'one why', 'why the', 'the films', 'films script', 'script is', 'is as', 'as amateur', 'amateur as', 'as a', 'a 2', '2 year', 'year old', 'old baby', 'baby script', 'script is', 'is king', 'king without', 'without a', 'a good', 'good script', 'script even', 'even the', 'the greatest', 'greatest director', 'director of', 'of alltime', 'alltime cannot', 'cannot do', 'do anything', 'anything tashan', 'tashan is', 'is produced', 'produced by', 'by the', 'the most', 'most successful', 'successful production', 'production banner', 'banner yash', 'yash raj', 'raj films', 'films and', 'and mega', 'mega stars', 'stars appearing', 'appearing in', 'in it', 'it but', 'but nothing', 'nothing on', 'on earth', 'earth can', 'can save', 'save you', 'you if', 'if you', 'you script', 'script is', 'is bland', 'bland thumbs', 'thumbs down', 'down performances', 'performances anil', 'anil kapoor', 'kapoor is', 'is a', 'a veteran', 'veteran actor', 'actor but', 'but how']\n",
      "====\n",
      "['the worst', 'worst movie', 'movie i', 'i have', 'have seen', 'seen since', 'since tera', 'tera jadoo', 'jadoo chal', 'chal gaya', 'gaya there', 'there is', 'is no', 'no story', 'story no', 'no humor', 'humor no', 'no nothing', 'nothing the', 'the action', 'action sequences', 'sequences seem', 'seem more', 'more like', 'like a', 'a series', 'series of', 'of haphazard', 'haphazard akshay', 'akshay kumar', 'kumar thumbsup', 'thumbsup advertisements', 'advertisements stitched', 'stitched together', 'together heavily', 'heavily influenced', 'influenced from', 'from the', 'the matrix', 'matrix and', 'and kungfu', 'kungfu hustle', 'hustle but', 'but very', 'very poorly', 'poorly executedi', 'executedi did', 'did not', 'not go', 'go a', 'a lot', 'lot of', 'of expectations', 'expectations but', 'but watching', 'watching this', 'this movie', 'movie is', 'is an', 'an exasperating', 'exasperating experience', 'experience which', 'which makes', 'makes you', 'you wonder', 'wonder what', 'what were', 'were these', 'these guys', 'guys thinkingthe', 'thinkingthe only', 'only thing', 'thing you', 'you might', 'might remember', 'remember after', 'after watching', 'watching it', 'it is', 'is an', 'an anorexic', 'anorexic kareena', 'kareena in', 'in a', 'a bikinithe', 'bikinithe reason', 'reason why', 'why i', 'i did', 'did not', 'not give', 'give a', 'a rating', 'rating of', 'of 1', '1 is', 'is that', 'that every', 'every time', 'time i']\n",
      "====\n",
      "\n",
      "\n",
      "Small one hot encoded Sample:\n",
      "[[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "\n",
      "Encoded Vocabulary\n",
      "{'of the': 0, 'is a': 1, 'airport 77': 2, 'in the': 3, 'have been': 4, 'as a': 5, 'to the': 6, 'the plane': 7, 'by the': 8, 'the three': 9, 'i have': 10, 'have seen': 11, 'this one': 12, 'with a': 13, 'when the': 14, 'this is': 15, 'time oscar': 16, 'oscar winner': 17, 'the most': 18, 'even the': 19, 'script is': 20, 'plane is': 21, 'james stewart': 22, 'oil rig': 23, 'where it': 24, 'for the': 25, '77 is': 26, 'it is': 27, 'three airport': 28, 'airport films': 29, 'so far': 30, 'far i': 31, 'the 747': 32, 'disaster flick': 33, 'as the': 34, 'its a': 35, 'could have': 36, 'lack of': 37, 'or tension': 38, 'while the': 39, 'that much': 40, 'as i': 41, 'should have': 42, 'george kennedy': 43, 'kennedy as': 44, 'couple of': 45, 'of airport': 46, 'with the': 47, 'i reckon': 48, 'a little': 49, 'the action': 50, 'it was': 51, 'was a': 52, 'in this': 53, 'one time': 54, 'i liked': 55, 'this film': 56, 'the actors': 57, 'been the': 58, 'from the': 59, 'in love': 60, 'love with': 61, 'this movie': 62, 'at the': 63, 'can you': 64, 'two people': 65, 'film the': 66, 'but how': 67, 'only thing': 68, 'i think': 69, 'tashan is': 70, 'the films': 71, 'akshay kumar': 72, 'the worst': 73, 'did not': 74, 'is an': 75, '77 starts': 76, 'starts as': 77, 'a brand': 78, 'brand new': 79, 'new luxury': 80, 'luxury 747': 81, '747 plane': 82, 'is loaded': 83, 'loaded up': 84, 'up with': 85, 'with valuable': 86, 'valuable paintings': 87, 'paintings such': 88, 'such belonging': 89, 'belonging to': 90, 'to rich': 91, 'rich businessman': 92, 'businessman philip': 93, 'philip stevens': 94, 'stevens james': 95, 'stewart who': 96, 'who is': 97, 'is flying': 98, 'flying them': 99}\n",
      "\n",
      "\n",
      "Decoded Text from vocabulary (limited by max tokens)\n",
      "airport 77 77 starts starts as as a a brand brand new new luxury luxury 747 747 plane plane is is loaded loaded up up with with valuable valuable paintings paintings such such belonging belonging to to rich rich businessman businessman philip philip stevens stevens james james stewart stewart who who is is flying flying them to the as a the plane plane is by the oil rig in the of the the plane where it to the in the of the for the to the airport 77 77 is it is for the of the the three three airport airport films i have have seen so far far i this one of the the three with a oil rig of the the 747 disaster flick of the where it with a when the as the the 747 of the its a could have disaster flick lack of or tension this is is a while the that much the plane as i should have have been when the that much with a george kennedy kennedy as as the couple of in the of airport airport 77 while the george kennedy kennedy as couple of of airport airport 77 with the in the i reckon a little the action a little or tension is a as i i reckon could have have been time oscar oscar winner it was was a in this this one one time time oscar oscar winner james stewart one time time oscar oscar winner 77 is the most of the the three three airport airport films so far far i i liked by the\n",
      "\n",
      "this film of the lack of even the the actors have been been the from the the actors have been been the in love love with in love love with with the in this this movie it was\n",
      "\n",
      "this is should have at the can you two people when the two people this film film the but how can you only thing i liked was a i think this is\n",
      "\n",
      "tashan is is a its a of the the most even the this one the films script is as a script is even the tashan is by the the most script is is a but how akshay kumar is a film the the films at the\n",
      "\n",
      "the worst i have have seen the action akshay kumar from the did not this movie is an only thing it is is an did not i think i have have seen the worst\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#//*** Vectorize a corpus\n",
    "class Vectorizer:\n",
    "    def __init__(self,**kwargs):\n",
    "        self.corpus_tokens = []\n",
    "        self.corpus_ngrams = []\n",
    "\n",
    "        self.max_tokens = None\n",
    "        self.ngram_size = 1\n",
    "        self.tidyup = True\n",
    "        \n",
    "        for key,value in kwargs.items():\n",
    "            if key ==\"max_tokens\":\n",
    "                self.max_tokens = value\n",
    "                \n",
    "            if key == \"ngrams\":\n",
    "                self.ngram_size = value\n",
    "            \n",
    "            if key == \"tidyup\":\n",
    "                self.tidyup = value\n",
    "        \n",
    "        \n",
    "        #//*** One Hot Encoding Dictionaries\n",
    "        #//*** Key = Token Index, Value = Word\n",
    "        self.ngram_index = {}\n",
    "        \n",
    "        #//*** Key = Word, Value = Token Index\n",
    "        self.vocabulary_index = {}\n",
    "        \n",
    "    def tokenize(self,raw_text):\n",
    "        #//*** Initialize Output Tokens\n",
    "        tokens = []\n",
    "\n",
    "        #//*** Split Text into words\n",
    "        for x in re.split(\"\\s\",raw_text):\n",
    "\n",
    "            #//*** Findall Non text characters in each word\n",
    "            non_text = re.findall(\"\\W\",x)\n",
    "\n",
    "            #//*** Remove non_text Characters\n",
    "            for i in non_text:\n",
    "                x = x.replace(i,\"\")\n",
    "\n",
    "            #//*** If X has length, append out\n",
    "            if len(x) > 0:\n",
    "                tokens.append(x.lower())\n",
    "        return tokens\n",
    "\n",
    "    def build_ngrams(self):\n",
    "        if self.ngram_size <= 0:\n",
    "            print(\"Ngram size must be an integer > 0\")\n",
    "            print(\"Quitting!\")\n",
    "            return None\n",
    "        \n",
    "        #//*** Using unigrams, use tokens\n",
    "        if self.ngram_size == 1:\n",
    "            self.corpus_ngrams = self.corpus_tokens\n",
    "            return\n",
    "\n",
    "        self.corpus_ngrams = []\n",
    "        \n",
    "        #//*** Get each token group from corpus_tokens\n",
    "        for token in self.corpus_tokens:\n",
    "            \n",
    "            loop_ngram = []\n",
    "            \n",
    "            #//*** Use an index based range to loop through tokens\n",
    "            for x in range(0,len(token) ):\n",
    "\n",
    "                #//*** Check if index + ngram_size exceeds the length of tokens\n",
    "                if x+self.ngram_size <= len(token):\n",
    "\n",
    "                    result = \"\"\n",
    "\n",
    "                    #//*** Build the ngram\n",
    "                    for y in range(self.ngram_size):\n",
    "                        #print(self.tokens[x+y])\n",
    "                        result += token[x+y] + \" \"\n",
    "\n",
    "                    loop_ngram.append(result[:-1])\n",
    "\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            #//*** Token group ngram is built. Add loop_ngram to corpus_ngram\n",
    "            self.corpus_ngrams.append(loop_ngram)\n",
    "        \n",
    "\n",
    "    def one_hot_encode(self,tokens):\n",
    "        \n",
    "        #//*** Encoded Results\n",
    "        result = []\n",
    "        \n",
    "        #//*** Set the Max array size to the total number of items in self.ngram_index\n",
    "        array_size = len(self.ngram_index.keys())\n",
    "        \n",
    "\n",
    "        #//*** hot encode each ngram\n",
    "        for ngram in tokens:\n",
    "            \n",
    "            #//*** Skip words not in self.vocabulary_index\n",
    "            #//*** These are skipped due to max_tokens limitations\n",
    "            if ngram not in self.vocabulary_index.keys():\n",
    "                continue\n",
    "            \n",
    "            #//*** Generate Array of Zeroes of Vocabulary length \n",
    "            encoded_text = list(np.zeros(array_size,dtype = int))\n",
    "            \n",
    "            #//*** Set Index of Vocabulary Word to 1\n",
    "            encoded_text[ self.vocabulary_index[ngram] ] = 1\n",
    "            \n",
    "            #//*** Add the one-hot-encoded word to encoded text\n",
    "            result.append(encoded_text)\n",
    "        \n",
    "        #//*** END for ngram in tokens:\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def encode(self,corpus):\n",
    "        \n",
    "        if not isinstance(corpus,list) :\n",
    "            print(\"Vectorizer Requires a corpus (list of text):\")\n",
    "            return None\n",
    "        \n",
    "        self.tokens = []\n",
    "        \n",
    "        #//*** Tokenize each text entry in the corpus\n",
    "        for raw_text in corpus:\n",
    "            self.corpus_tokens.append(self.tokenize(raw_text))\n",
    "        \n",
    "        #//*** Build ngrams (Defaults to unigrams)\n",
    "        self.build_ngrams()\n",
    "        \n",
    "        word_freq = {}\n",
    "        \n",
    "        #//*** Build dictionary of unique words\n",
    "        #//*** Loop through each element of the corpus\n",
    "        for element in self.corpus_ngrams:\n",
    "        \n",
    "            #//*** Process each individual ngram\n",
    "            for ngram in element:\n",
    "\n",
    "                #//*** Add unique words to dictionaries\n",
    "                if ngram not in self.ngram_index.values():\n",
    "                    index = len(self.ngram_index.values())\n",
    "                    self.ngram_index[ index ] = ngram\n",
    "                    self.vocabulary_index [ ngram ] = index\n",
    "                    \n",
    "                    #//*** Initialize Word Frequency\n",
    "                    word_freq[ ngram ] = 1\n",
    "                else:\n",
    "                    #//*** Increment Word Frequency\n",
    "                    word_freq[ ngram ] += 1\n",
    "\n",
    "        #//*** END for element in self.corpus_ngrams:\n",
    "        if self.max_tokens != None:\n",
    "            \n",
    "            #//*** Check if token count exceeds max tokens\n",
    "            if self.max_tokens < len(self.ngram_index.items()):\n",
    "                \n",
    "                #//*** Sort the Word Frequency Dictionary. Keep the highest frequency words\n",
    "                word_freq = dict(sorted(word_freq.items(), key=lambda x: x[1], reverse=True))\n",
    "                \n",
    "                \n",
    "                #//*** Get list of keys that are lowest frequency\n",
    "                for key in list(word_freq.keys())[self.max_tokens:]:\n",
    "                    #//*** Delete Low Frequency ngrams\n",
    "                    del word_freq[ key ]\n",
    "                \n",
    "                self.ngram_index = {}\n",
    "                self.vocabulary_index = {}\n",
    "                \n",
    "                #//*** Rebuild ngram_index & vocabulary_index\n",
    "                for ngram in word_freq.keys():\n",
    "                    index = len(self.ngram_index.values())\n",
    "                    self.ngram_index[ index ] = ngram\n",
    "                    self.vocabulary_index [ ngram ] = index        \n",
    "            \n",
    "            #//*** END Trim Low Frequency ngrams\n",
    "        self.word_freq = word_freq\n",
    "        \n",
    "        #//**** List of Encoded Values\n",
    "        encoded = []\n",
    "        \n",
    "        #//*** One hot encode each text element\n",
    "        for element in self.corpus_ngrams:\n",
    "            encoded.append( self.one_hot_encode(element) )\n",
    "            \n",
    "        #//*** TidyUp (Delete) ngrams and Tokens\n",
    "        if self.tidyup:\n",
    "            self.corpus_tokens = []\n",
    "            self.corpus_ngrams = []\n",
    "            \n",
    "        return encoded\n",
    "    \n",
    "    #//*** Convert One-Hot-Encoding to text\n",
    "    def decode(self,corpus):\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        #//*** For Each element in Corpus\n",
    "        for elements in corpus:\n",
    "            \n",
    "            decoded = \"\"\n",
    "            \n",
    "            #//*** For Each ngram (word(s)) in Elements\n",
    "            for ngram in elements:\n",
    "                \n",
    "                \n",
    "                \n",
    "                decoded += self.ngram_index[ ngram.index(1) ] + \" \"\n",
    "                \n",
    "            #//*** END for ngram in elements:\n",
    "            results.append( decoded[:-1])\n",
    "            \n",
    "        #//*** END for elements in corpus:\n",
    "        return results\n",
    "\n",
    "\n",
    "#//*** Test the Vectorizer with some sample data\n",
    "vectorizer = Vectorizer(max_tokens=100,ngrams=2, tidyup=False)\n",
    "\n",
    "temp_vals = vectorizer.encode(raw_val_text[:5])\n",
    "\n",
    "print(\"Sample Text: (First 500 Chars)\")\n",
    "for element in raw_val_text[:5]:\n",
    "    print(element[:500])\n",
    "    print(\"====\")\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"Tokens: (First 100 tokens)\")\n",
    "for token in vectorizer.corpus_tokens:\n",
    "    print(token[:100])\n",
    "    print(\"====\")\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"ngrams: (First 50 tokens)\")\n",
    "for token in vectorizer.corpus_ngrams:\n",
    "    print(token[:100])\n",
    "    print(\"====\")\n",
    "print()\n",
    "print()\n",
    "print(\"Small one hot encoded Sample:\")\n",
    "print(temp_vals[0][:10])\n",
    "print()\n",
    "print()\n",
    "print(\"Encoded Vocabulary\")\n",
    "print(vectorizer.vocabulary_index)\n",
    "print()\n",
    "print()\n",
    "print(\"Decoded Text from vocabulary (limited by max tokens)\")\n",
    "for result in vectorizer.decode(temp_vals):\n",
    "    print(result)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CustomObjectScope',\n",
       " 'GeneratorEnqueuer',\n",
       " 'OrderedEnqueuer',\n",
       " 'Progbar',\n",
       " 'Sequence',\n",
       " 'SequenceEnqueuer',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_sys',\n",
       " 'array_to_img',\n",
       " 'custom_object_scope',\n",
       " 'deserialize_keras_object',\n",
       " 'experimental',\n",
       " 'get_custom_objects',\n",
       " 'get_file',\n",
       " 'get_registered_name',\n",
       " 'get_registered_object',\n",
       " 'get_source_inputs',\n",
       " 'image_dataset_from_directory',\n",
       " 'img_to_array',\n",
       " 'load_img',\n",
       " 'model_to_dot',\n",
       " 'normalize',\n",
       " 'pack_x_y_sample_weight',\n",
       " 'plot_model',\n",
       " 'register_keras_serializable',\n",
       " 'save_img',\n",
       " 'serialize_keras_object',\n",
       " 'text_dataset_from_directory',\n",
       " 'timeseries_dataset_from_array',\n",
       " 'to_categorical',\n",
       " 'unpack_x_y_sample_weight']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(keras.utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Airport '77 starts as a brand new luxury 747 plane is loaded up with valuable paintings & such belonging to rich businessman Philip Stevens (James Stewart) who is flying them & a bunch of VIP's to his estate in preparation of it being opened to the public as a museum, also on board is Stevens daughter Julie (Kathleen Quinlan) & her son. The luxury jetliner takes off as planned but mid-air the plane is hi-jacked by the co-pilot Chambers (Robert Foxworth) & his two accomplice's Banker (Monte Markham) & Wilson (Michael Pataki) who knock the passengers & crew out with sleeping gas, they plan to steal the valuable cargo & land on a disused plane strip on an isolated island but while making his descent Chambers almost hits an oil rig in the Ocean & loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the Bermuda Triangle. With air in short supply, water leaking in & having flown over 200 miles off course the problems mount for the survivor's as they await help with time fast running out...Also known under the slightly different tile Airport 1977 this second sequel to the smash-hit disaster thriller Airport (1970) was directed by Jerry Jameson & while once again like it's predecessors I can't say Airport '77 is any sort of forgotten classic it is entertaining although not necessarily for the right reasons. Out of the three Airport films I have seen so far I actually liked this one the best, just. It has my favourite plot of the three with a nice mid-air hi-jacking & then the crashing (didn't he see the oil rig?) & sinking of the 747 (maybe the makers were trying to cross the original Airport with another popular disaster flick of the period The Poseidon Adventure (1972)) & submerged is where it stays until the end with a stark dilemma facing those trapped inside, either suffocate when the air runs out or drown as the 747 floods or if any of the doors are opened & it's a decent idea that could have made for a great little disaster flick but bad unsympathetic character's, dull dialogue, lethargic set-pieces & a real lack of danger or suspense or tension means this is a missed opportunity. While the rather sluggish plot keeps one entertained for 108 odd minutes not that much happens after the plane sinks & there's not as much urgency as I thought there should have been. Even when the Navy become involved things don't pick up that much with a few shots of huge ships & helicopters flying about but there's just something lacking here. George Kennedy as the jinxed airline worker Joe Patroni is back but only gets a couple of scenes & barely even says anything preferring to just look worried in the background.The home video & theatrical version of Airport '77 run 108 minutes while the US TV versions add an extra hour of footage including a new opening credits sequence, many more scenes with George Kennedy as Patroni, flashbacks to flesh out character's, longer rescue scenes & the discovery or another couple of dead bodies including the navigator. While I would like to see this extra footage I am not sure I could sit through a near three hour cut of Airport '77. As expected the film has dated badly with horrible fashions & interior design choices, I will say no more other than the toy plane model effects aren't great either. Along with the other two Airport sequels this takes pride of place in the Razzie Award's Hall of Shame although I can think of lots of worse films than this so I reckon that's a little harsh. The action scenes are a little dull unfortunately, the pace is slow & not much excitement or tension is generated which is a shame as I reckon this could have been a pretty good film if made properly.The production values are alright if nothing spectacular. The acting isn't great, two time Oscar winner Jack Lemmon has said since it was a mistake to star in this, one time Oscar winner James Stewart looks old & frail, also one time Oscar winner Lee Grant looks drunk while Sir Christopher Lee is given little to do & there are plenty of other familiar faces to look out for too.Airport '77 is the most disaster orientated of the three Airport films so far & I liked the ideas behind it even if they were a bit silly, the production & bland direction doesn't help though & a film about a sunken plane just shouldn't be this boring or lethargic. Followed by The Concorde ... Airport '79 (1979).\", \"This film lacked something I couldn't put my finger on at first: charisma on the part of the leading actress. This inevitably translated to lack of chemistry when she shared the screen with her leading man. Even the romantic scenes came across as being merely the actors at play. It could very well have been the director who miscalculated what he needed from the actors. I just don't know.But could it have been the screenplay? Just exactly who was the chef in love with? He seemed more enamored of his culinary skills and restaurant, and ultimately of himself and his youthful exploits, than of anybody or anything else. He never convinced me he was in love with the princess.I was disappointed in this movie. But, don't forget it was nominated for an Oscar, so judge for yourself.\", 'Sorry everyone,,, I know this is supposed to be an \"art\" film,, but wow, they should have handed out guns at the screening so people could blow their brains out and not watch. Although the scene design and photographic direction was excellent, this story is too painful to watch. The absence of a sound track was brutal. The loooonnnnng shots were too long. How long can you watch two people just sitting there and talking? Especially when the dialogue is two people complaining. I really had a hard time just getting through this film. The performances were excellent, but how much of that dark, sombre, uninspired, stuff can you take? The only thing i liked was Maureen Stapleton and her red dress and dancing scene. Otherwise this was a ripoff of Bergman. And i\\'m no fan f his either. I think anyone who says they enjoyed 1 1/2 hours of this is,, well, lying.', \"Vijay Krishna Acharya's 'Tashan' is a over-hyped, stylized, product. Sure its a one of the most stylish films, but when it comes to content, even the masses will reject this one. Why? The films script is as amateur as a 2 year old baby. Script is king, without a good script even the greatest director of all-time cannot do anything. Tashan is produced by the most successful production banner 'Yash Raj Films' and Mega Stars appearing in it. But nothing on earth can save you if you script is bland. Thumbs down! Performances: Anil Kapoor, is a veteran actor. But how could he okay a role like this? Akshay Kumar is great actor, in fact he's the sole saving grace. Kareena Kapoor has never looked so hot. She looks stunning and leaves you, all stand up. Saif Ali Khan doesn't get his due in here. Sanjay Mishra, Manoj Phawa and Yashpal Sharma are wasted.'Tashan' is a boring film. The films failure at the box office, should you keep away.\", 'The worst movie I have seen since Tera Jadoo Chal Gaya. There is no story, no humor, no nothing! The action sequences seem more like a series of haphazard Akshay Kumar Thumbs-Up advertisements stitched together. Heavily influenced from The Matrix and Kung-Fu Hustle but very poorly executed.I did not go a lot of expectations, but watching this movie is an exasperating experience which makes you wonder \"What were these guys thinking??!!\".The only thing you might remember after watching it is an anorexic Kareena in a bikini.The reason why I did not give a rating of \\'1\\' is that every time I think I have seen the worst, Bollywood proves me wrong.', 'Shame on Yash Raj films and Aditya Chopra who seems to have lost their intelligence over the years and providing steady fare of tripe in this piece of cinematic crap thats not even worth You Tube standards. I was gritting my teeth throughout the whole flick start to finish with the schizophrenic direction, plot line that never quite materialized and on the last scene I just felt ashamed that my country and its crorepati film makers can \"THROW AWAY\" crores on such stupidity. Shame on the actors for taking this work and even commenting on it as some piece of work they can own up to. Saif Ali Khan -completely disappointed in your choice of film. Kareen shows enough skin for the puberty stricken and Akshay comes up as the dim-wit. Anil another retard with a pubescent fascination for English. His cronies were commendable in their acting and with the bizarre cinematography scattered in the last 15 minutes, it was enough to pop a blood vessel. DON\\'T WASTe any brain cells, energy or your money to go see this- Go SEE / Rent AMU -with Konkana Sensharma instead- a beautiful piece of independent film thats ever come out of India.Intelligent, poignant and a wonderful story-tale that will touch everyone with intelligent actors and gave me hope that all is not lost in Indian cinema making.', \"Tashan - the title itself explains the nature of the movie.This type of movies are actually made for flop. What a shame that Yash Raj Films produces such movies those are worthless than C-grade movies. Or even some C-grade movies have better and pleasing story than Tashan. The much hyped and over-confidently promoted Tashan poorly bombed at the box-office which it certainly deserved.In my view, this is the worst movie ever made from honourable Yash Raj Films' banner. How come they handled such a heavy project to new Vijay Krishna Acharya who has no actual sense of making action flick? He tried to imitate Sanjay Gadhvi's ways of making like Dhoom but he suffered at last. The action scenes are more like than comics or cartoon movies made for exhausting the audiences.The story also loses in its meaning and substances to tenderly win the audiences' hearts. In most scenes Anil Kapoor reminds me of southern Tamil star Rajnikant in his body languages and wordly expressions. I am not a fan of neither Saif nor Akshay, but the award of Kareena should have finally gone to Saif''s hand instead of Akshay. Just from the starting point I expected of it, but at the end it displeased me with the climax truth. Saif is the main behind the whole adventure, while Akshay joins in the midst. In any movie, the final should be judged with the whole characters of the entire story and the award or say reward should be given to the one who deserves credit. And Tashan loses in this way, and unexpectedly failed to become a hit.Akshay's has nothing new to show off his comedian talent here but still reminds of his previous movies. He seriously need to form a new image to his fans that would impress them again and again. In between Saif did a great job in Race, and now he returned again in his hilarious nature through this movie. But he has fully developed himself in the acting field. And last but not the least about Kareena. She looks really hot with bikini dress of which some complain as she became too lean. But I myself don't think so, instead she became slim. Yes slim!!! it is a good factor for a female to attract the major people (or say, male). Beside them it is nice that Saif's son Ibrahim appears in the beginning & last as young Saif. I hope now he too will lean forward in target of making acting as his career.Those who like this Tashan they are either mentally immatured or still want to go back to childhood, or say want to be admitted in an asylum. Thumbs down to debutante director Vijay Krishna Acharya who mishandled the project offered by Yash Raj Films. In future he should experiment and study the script minimum of 5 years before going into practical directions.Sorry, I don't like to rate good stars to this type of junk movies.\", \"I was very displeased with this move. Everything was terrible from the start. The comedy was unhumorous, the action overdone, the songs unmelodious. Even the storyline was weightless. From a writer who has written successful scripts like Guru and Dhoom, I had high expectations. The actors worked way too hard and did not help the film at all. Of course, Kareena rocked the screen in a bikini but for two seconds. I think Hindi stunt directors should research how action movies are done. They tend to exaggerate way too much. In Chinese films, this style works because that is their signature piece. But, Hindi cinema's signature are the songs. A good action movie should last no more than two hours and cannot look unrealistic. But, in the future, I'm sure these action movies will get much sharper. Also to be noted: Comedy and action films do not mix unless done properly. Good Luck next time.\", \"Such a long awaited movie.. But it has disappointed me and my friends who had gone to see the movie on the first day.. From the trailers it looked like a action movie, but it turned out to be a out & out comedy(a bad comedy). But one thing that deserves appreciation is the acting by these professional actors, they've done their part of the movie very well. Good acting, but i don't think that can save the movie.. India has been shot beautifully. Kerala, Rajasthan, (Ladakh?) were all saturated with color, alright. Nevertheless the way the intrinsic beauty of these places was shot made me want to find out exactly where those places were and when I could go there ;-)Action sequences were shot very shabbily, no one could make out head & tail of the stunts, they've used Akki(akshay kumar) very well but could've been done much much better..Animation is the worst i've seen in recent movies(90's movies had better animation scenes i guess(initial scene where the car is falling off 'flying should be better word' the road into lake).And the movies name has been mentioned nearly every 20 to 30 mins, just to make sure audiences don't forget the movie name i guess..\", \"This is the biggest Flop of 2008. I don know what Director has is his mind of creating such a big disaster. The songs have been added without situations, the story have been stretched to fill the 3 hrs gap and most disgusting are the action stunts performed by the actors it's like everyone are having superpowers they can run in between the bullets are fire and nothing happens to them and one person fighting with 100 people. Only the best performance was by Anil Kapoor man he is all time at his best playing the role of villain with a comic act speaking Hinglish... Akki is also done a good job.... But the movieee just forget it.\"]\n"
     ]
    }
   ],
   "source": [
    "print(raw_val_text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build: Training Data Set\n",
      "Found 70000 files belonging to 3 classes.\n",
      "Build: Validation Data Set\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Build: Test Data Set\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "\n",
    "print(\"Build: Training Data Set\")\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    imdb_path.joinpath(\"train\"), batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(\"Build: Validation Data Set\")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    imdb_path.joinpath(\"val\"), batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(\"Build: Test Data Set\")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    imdb_path.joinpath(\"test\"), batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b\"I saw this about 14 years ago in a stroke of luck ( a local TV station had picked up a print, and my mother, horror buff that she is, decided to tape it), and the film has stuck with me ever since. It's not your typical horror film, and has more of a tragic element which was so very common to films of the genre in this particular era. The dark and dirty imagery only serves to enhance the premise, and the shrine the Hook children build to their mother is downright creepy. The children do a very decent job of portraying children ( something that is increasingly rare these days) and Dirk Bogarde does a fantastic job of portraying their scumbag father. And to boot, we've got a heavy incest theme going on. If you can get a hold of this one, go for it: it's very much of its time, but the opportunity is well worth any trouble.\", shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(2, shape=(), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2188"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#//*** Displaying the shapes and dtypes of the first batch\n",
    "\n",
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break\n",
    "\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for inputs, targets in binary_1gram_val_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //*** CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
