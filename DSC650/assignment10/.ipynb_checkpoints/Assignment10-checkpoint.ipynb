{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stoneburner, Kurt\n",
    "- ## DSC 650 - Assignment 10\n",
    "\n",
    "\n",
    "Links to Deep Learning Sample Code:\n",
    "- https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part01_introduction.ipynb\n",
    "\n",
    "- https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part02_sequence-models.ipynb\n",
    "\n",
    "- https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part03_transformer.ipynb\n",
    "\n",
    "- https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part04_sequence-to-sequence-learning.ipynb\n",
    "\n",
    "ngram reference:\n",
    "- https://www.analyticsvidhya.com/blog/2021/09/what-are-n-grams-and-how-to-implement-them-in-python/\n",
    "\n",
    "Convert Numpy Array to Tensor:\n",
    "- https://www.projectpro.io/recipes/convert-numpy-array-tensor\n",
    "\n",
    "Convert Tensor to Numpy Array:\n",
    "- https://www.delftstack.com/howto/numpy/python-convert-tensor-to-numpy-array/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import datetime \n",
    "\n",
    "#//*** Reusing Code from assignment 04\n",
    "from chardet.universaldetector import UniversalDetector\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n"
     ]
    }
   ],
   "source": [
    "#//*** Get Working Directory\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "\n",
    "#//*** Go up Two folders\n",
    "project_dir = current_dir.parents[2]\n",
    "\n",
    "#//*** IMDB Data Path\n",
    "imdb_path = project_dir.joinpath(\"dsc650/data/external/imdb/aclImdb\")\n",
    "\n",
    "file_path = imdb_path.joinpath(\"train/pos\")\n",
    "\n",
    "#//*** Grab the first positive review text for testing\n",
    "file_path = file_path.joinpath(os.listdir(file_path)[0])\n",
    "\n",
    "with open(file_path,'r') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "print(sample_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Randomly assign 20% of the training Data and copy to a validation folder\n",
    "import os, pathlib, shutil, random\n",
    "\n",
    "val_dir = imdb_path.joinpath(\"val\")\n",
    "train_dir = imdb_path.joinpath(\"train\")\n",
    "test_dir = imdb_path.joinpath(\"test\")\n",
    "\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    #//*** Skip if val folder exists (Delete Folder to resample)\n",
    "    if os.path.exists(val_dir.joinpath(category)):\n",
    "        break\n",
    "    \n",
    "    os.makedirs(val_dir.joinpath(category))\n",
    "    files = os.listdir(train_dir.joinpath(category))\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load IMDB Dataset #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Raw Validation Set\n",
      "Loading Raw Train Data\n",
      "Dropping File: 7714_1.txt due to decoding issues\n",
      "Dropping File: 11351_9.txt due to decoding issues\n",
      "Dropping File: 8263_9.txt due to decoding issues\n",
      "Loading Raw Test Data\n",
      "Dropping File: 4414_1.txt due to decoding issues\n",
      "Dropping File: 6973_1.txt due to decoding issues\n",
      "Dropping File: 2464_10.txt due to decoding issues\n",
      "Dropping File: 5281_10.txt due to decoding issues\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#//*** Use Universal Detector to determine file encoding.\n",
    "#//*** Borrowed from Assignment04\n",
    "def read_file_with_encoding(filepath):\n",
    "\n",
    "    detector = UniversalDetector()\n",
    "    \n",
    "    try:\n",
    "        with open(filepath) as f:\n",
    "            return f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        detector.reset()\n",
    "        with open(filepath, 'rb') as f:\n",
    "            for line in f.readlines():\n",
    "                detector.feed(line)\n",
    "                if detector.done:\n",
    "                    break\n",
    "        detector.close()\n",
    "        encoding = detector.result['encoding']\n",
    "        with open(filepath, encoding=encoding) as f:\n",
    "            return f.read()\n",
    "\n",
    "#//*** Borrowed from Assignment04\n",
    "def parse_html_payload(payload):\n",
    "    \"\"\"\n",
    "    This function uses Beautiful Soup to read HTML data\n",
    "    and return the text.  If the payload is plain text, then\n",
    "    Beautiful Soup will return the original content\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(payload, 'html.parser')\n",
    "    return str(soup.get_text()).encode('utf-8').decode('utf-8')\n",
    "\n",
    "def load_dataset(dir_path):\n",
    "    \n",
    "    text = []\n",
    "    targets = []\n",
    "    \n",
    "    #//*** Crawl the neg and pos folders\n",
    "    for category in (\"neg\", \"pos\"):\n",
    "        files = os.listdir(dir_path.joinpath(category))\n",
    "        \n",
    "        #//*** Loop through each file in the folder\n",
    "        for file in files:\n",
    "            try:\n",
    "                #//*** Add processed file to text\n",
    "                text.append(\n",
    "                    #//*** Strip HTML Tags\n",
    "                    parse_html_payload(\n",
    "                        #//*** Read File from disk. Function uses Universal Detector to determine file encoding\n",
    "                        read_file_with_encoding(\n",
    "                            dir_path.joinpath(category).joinpath(file))))\n",
    "\n",
    "                #//*** Append Target Value\n",
    "                if category == 'neg':\n",
    "                    targets.append(0)\n",
    "                else:\n",
    "                    targets.append(1)\n",
    "            except:\n",
    "                print(f\"Dropping File: {file} due to decoding issues\")\n",
    "    return text,targets\n",
    "print(\"Loading Raw Validation Set\")\n",
    "raw_val_text, val_target = load_dataset(val_dir)\n",
    "\n",
    "print(\"Loading Raw Train Data\")\n",
    "raw_train_text, train_target = load_dataset(train_dir)\n",
    "\n",
    "print(\"Loading Raw Test Data\")\n",
    "raw_test_text, test_target = load_dataset(test_dir)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10.1 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Building ngrams...\n",
      "Building Vocabulary...\n",
      "Sorting Word Frequency...\n",
      "Building Token Dictionary\n",
      "Rebuilding Vocabulary\n",
      "Tokenizing...\n",
      "Building ngrams...\n",
      "One Hot Coding....\n",
      "Encoding Complete: 0:00:00.030889\n",
      "Run Time: 0:00:00.033881\n",
      "Sample Text: (First 500 Chars)\n",
      "Airport '77 starts as a brand new luxury 747 plane is loaded up with valuable paintings & such belonging to rich businessman Philip Stevens (James Stewart) who is flying them & a bunch of VIP's to his estate in preparation of it being opened to the public as a museum, also on board is Stevens daughter Julie (Kathleen Quinlan) & her son. The luxury jetliner takes off as planned but mid-air the plane is hi-jacked by the co-pilot Chambers (Robert Foxworth) & his two accomplice's Banker (Monte Markh\n",
      "====\n",
      "This film lacked something I couldn't put my finger on at first: charisma on the part of the leading actress. This inevitably translated to lack of chemistry when she shared the screen with her leading man. Even the romantic scenes came across as being merely the actors at play. It could very well have been the director who miscalculated what he needed from the actors. I just don't know.But could it have been the screenplay? Just exactly who was the chef in love with? He seemed more enamored of \n",
      "====\n",
      "Sorry everyone,,, I know this is supposed to be an \"art\" film,, but wow, they should have handed out guns at the screening so people could blow their brains out and not watch. Although the scene design and photographic direction was excellent, this story is too painful to watch. The absence of a sound track was brutal. The loooonnnnng shots were too long. How long can you watch two people just sitting there and talking? Especially when the dialogue is two people complaining. I really had a hard \n",
      "====\n",
      "Vijay Krishna Acharya's 'Tashan' is a over-hyped, stylized, product. Sure its a one of the most stylish films, but when it comes to content, even the masses will reject this one. Why? The films script is as amateur as a 2 year old baby. Script is king, without a good script even the greatest director of all-time cannot do anything. Tashan is produced by the most successful production banner 'Yash Raj Films' and Mega Stars appearing in it. But nothing on earth can save you if you script is bland.\n",
      "====\n",
      "The worst movie I have seen since Tera Jadoo Chal Gaya. There is no story, no humor, no nothing! The action sequences seem more like a series of haphazard Akshay Kumar Thumbs-Up advertisements stitched together. Heavily influenced from The Matrix and Kung-Fu Hustle but very poorly executed.I did not go a lot of expectations, but watching this movie is an exasperating experience which makes you wonder \"What were these guys thinking??!!\".The only thing you might remember after watching it is an an\n",
      "====\n",
      "\n",
      "\n",
      "Tokens: (First 100 tokens)\n",
      "['airport', '77', 'starts', 'as', 'a', 'brand', 'new', 'luxury', '747', 'plane', 'is', 'loaded', 'up', 'with', 'valuable', 'paintings', 'such', 'belonging', 'to', 'rich', 'businessman', 'philip', 'stevens', 'james', 'stewart', 'who', 'is', 'flying', 'them', 'a', 'bunch', 'of', 'vips', 'to', 'his', 'estate', 'in', 'preparation', 'of', 'it', 'being', 'opened', 'to', 'the', 'public', 'as', 'a', 'museum', 'also', 'on', 'board', 'is', 'stevens', 'daughter', 'julie', 'kathleen', 'quinlan', 'her', 'son', 'the', 'luxury', 'jetliner', 'takes', 'off', 'as', 'planned', 'but', 'midair', 'the', 'plane', 'is', 'hijacked', 'by', 'the', 'copilot', 'chambers', 'robert', 'foxworth', 'his', 'two', 'accomplices', 'banker', 'monte', 'markham', 'wilson', 'michael', 'pataki', 'who', 'knock', 'the', 'passengers', 'crew', 'out', 'with', 'sleeping', 'gas', 'they', 'plan', 'to', 'steal']\n",
      "====\n",
      "['this', 'film', 'lacked', 'something', 'i', 'couldnt', 'put', 'my', 'finger', 'on', 'at', 'first', 'charisma', 'on', 'the', 'part', 'of', 'the', 'leading', 'actress', 'this', 'inevitably', 'translated', 'to', 'lack', 'of', 'chemistry', 'when', 'she', 'shared', 'the', 'screen', 'with', 'her', 'leading', 'man', 'even', 'the', 'romantic', 'scenes', 'came', 'across', 'as', 'being', 'merely', 'the', 'actors', 'at', 'play', 'it', 'could', 'very', 'well', 'have', 'been', 'the', 'director', 'who', 'miscalculated', 'what', 'he', 'needed', 'from', 'the', 'actors', 'i', 'just', 'dont', 'knowbut', 'could', 'it', 'have', 'been', 'the', 'screenplay', 'just', 'exactly', 'who', 'was', 'the', 'chef', 'in', 'love', 'with', 'he', 'seemed', 'more', 'enamored', 'of', 'his', 'culinary', 'skills', 'and', 'restaurant', 'and', 'ultimately', 'of', 'himself', 'and', 'his']\n",
      "====\n",
      "['sorry', 'everyone', 'i', 'know', 'this', 'is', 'supposed', 'to', 'be', 'an', 'art', 'film', 'but', 'wow', 'they', 'should', 'have', 'handed', 'out', 'guns', 'at', 'the', 'screening', 'so', 'people', 'could', 'blow', 'their', 'brains', 'out', 'and', 'not', 'watch', 'although', 'the', 'scene', 'design', 'and', 'photographic', 'direction', 'was', 'excellent', 'this', 'story', 'is', 'too', 'painful', 'to', 'watch', 'the', 'absence', 'of', 'a', 'sound', 'track', 'was', 'brutal', 'the', 'loooonnnnng', 'shots', 'were', 'too', 'long', 'how', 'long', 'can', 'you', 'watch', 'two', 'people', 'just', 'sitting', 'there', 'and', 'talking', 'especially', 'when', 'the', 'dialogue', 'is', 'two', 'people', 'complaining', 'i', 'really', 'had', 'a', 'hard', 'time', 'just', 'getting', 'through', 'this', 'film', 'the', 'performances', 'were', 'excellent', 'but', 'how']\n",
      "====\n",
      "['vijay', 'krishna', 'acharyas', 'tashan', 'is', 'a', 'overhyped', 'stylized', 'product', 'sure', 'its', 'a', 'one', 'of', 'the', 'most', 'stylish', 'films', 'but', 'when', 'it', 'comes', 'to', 'content', 'even', 'the', 'masses', 'will', 'reject', 'this', 'one', 'why', 'the', 'films', 'script', 'is', 'as', 'amateur', 'as', 'a', '2', 'year', 'old', 'baby', 'script', 'is', 'king', 'without', 'a', 'good', 'script', 'even', 'the', 'greatest', 'director', 'of', 'alltime', 'cannot', 'do', 'anything', 'tashan', 'is', 'produced', 'by', 'the', 'most', 'successful', 'production', 'banner', 'yash', 'raj', 'films', 'and', 'mega', 'stars', 'appearing', 'in', 'it', 'but', 'nothing', 'on', 'earth', 'can', 'save', 'you', 'if', 'you', 'script', 'is', 'bland', 'thumbs', 'down', 'performances', 'anil', 'kapoor', 'is', 'a', 'veteran', 'actor', 'but']\n",
      "====\n",
      "['the', 'worst', 'movie', 'i', 'have', 'seen', 'since', 'tera', 'jadoo', 'chal', 'gaya', 'there', 'is', 'no', 'story', 'no', 'humor', 'no', 'nothing', 'the', 'action', 'sequences', 'seem', 'more', 'like', 'a', 'series', 'of', 'haphazard', 'akshay', 'kumar', 'thumbsup', 'advertisements', 'stitched', 'together', 'heavily', 'influenced', 'from', 'the', 'matrix', 'and', 'kungfu', 'hustle', 'but', 'very', 'poorly', 'executedi', 'did', 'not', 'go', 'a', 'lot', 'of', 'expectations', 'but', 'watching', 'this', 'movie', 'is', 'an', 'exasperating', 'experience', 'which', 'makes', 'you', 'wonder', 'what', 'were', 'these', 'guys', 'thinkingthe', 'only', 'thing', 'you', 'might', 'remember', 'after', 'watching', 'it', 'is', 'an', 'anorexic', 'kareena', 'in', 'a', 'bikinithe', 'reason', 'why', 'i', 'did', 'not', 'give', 'a', 'rating', 'of', '1', 'is', 'that', 'every', 'time']\n",
      "====\n",
      "\n",
      "\n",
      "ngrams: (First 50 tokens)\n",
      "['airport 77', '77 starts', 'starts as', 'as a', 'a brand', 'brand new', 'new luxury', 'luxury 747', '747 plane', 'plane is', 'is loaded', 'loaded up', 'up with', 'with valuable', 'valuable paintings', 'paintings such', 'such belonging', 'belonging to', 'to rich', 'rich businessman', 'businessman philip', 'philip stevens', 'stevens james', 'james stewart', 'stewart who', 'who is', 'is flying', 'flying them', 'them a', 'a bunch', 'bunch of', 'of vips', 'vips to', 'to his', 'his estate', 'estate in', 'in preparation', 'preparation of', 'of it', 'it being', 'being opened', 'opened to', 'to the', 'the public', 'public as', 'as a', 'a museum', 'museum also', 'also on', 'on board', 'board is', 'is stevens', 'stevens daughter', 'daughter julie', 'julie kathleen', 'kathleen quinlan', 'quinlan her', 'her son', 'son the', 'the luxury', 'luxury jetliner', 'jetliner takes', 'takes off', 'off as', 'as planned', 'planned but', 'but midair', 'midair the', 'the plane', 'plane is', 'is hijacked', 'hijacked by', 'by the', 'the copilot', 'copilot chambers', 'chambers robert', 'robert foxworth', 'foxworth his', 'his two', 'two accomplices', 'accomplices banker', 'banker monte', 'monte markham', 'markham wilson', 'wilson michael', 'michael pataki', 'pataki who', 'who knock', 'knock the', 'the passengers', 'passengers crew', 'crew out', 'out with', 'with sleeping', 'sleeping gas', 'gas they', 'they plan', 'plan to', 'to steal', 'steal the']\n",
      "====\n",
      "['this film', 'film lacked', 'lacked something', 'something i', 'i couldnt', 'couldnt put', 'put my', 'my finger', 'finger on', 'on at', 'at first', 'first charisma', 'charisma on', 'on the', 'the part', 'part of', 'of the', 'the leading', 'leading actress', 'actress this', 'this inevitably', 'inevitably translated', 'translated to', 'to lack', 'lack of', 'of chemistry', 'chemistry when', 'when she', 'she shared', 'shared the', 'the screen', 'screen with', 'with her', 'her leading', 'leading man', 'man even', 'even the', 'the romantic', 'romantic scenes', 'scenes came', 'came across', 'across as', 'as being', 'being merely', 'merely the', 'the actors', 'actors at', 'at play', 'play it', 'it could', 'could very', 'very well', 'well have', 'have been', 'been the', 'the director', 'director who', 'who miscalculated', 'miscalculated what', 'what he', 'he needed', 'needed from', 'from the', 'the actors', 'actors i', 'i just', 'just dont', 'dont knowbut', 'knowbut could', 'could it', 'it have', 'have been', 'been the', 'the screenplay', 'screenplay just', 'just exactly', 'exactly who', 'who was', 'was the', 'the chef', 'chef in', 'in love', 'love with', 'with he', 'he seemed', 'seemed more', 'more enamored', 'enamored of', 'of his', 'his culinary', 'culinary skills', 'skills and', 'and restaurant', 'restaurant and', 'and ultimately', 'ultimately of', 'of himself', 'himself and', 'and his', 'his youthful']\n",
      "====\n",
      "['sorry everyone', 'everyone i', 'i know', 'know this', 'this is', 'is supposed', 'supposed to', 'to be', 'be an', 'an art', 'art film', 'film but', 'but wow', 'wow they', 'they should', 'should have', 'have handed', 'handed out', 'out guns', 'guns at', 'at the', 'the screening', 'screening so', 'so people', 'people could', 'could blow', 'blow their', 'their brains', 'brains out', 'out and', 'and not', 'not watch', 'watch although', 'although the', 'the scene', 'scene design', 'design and', 'and photographic', 'photographic direction', 'direction was', 'was excellent', 'excellent this', 'this story', 'story is', 'is too', 'too painful', 'painful to', 'to watch', 'watch the', 'the absence', 'absence of', 'of a', 'a sound', 'sound track', 'track was', 'was brutal', 'brutal the', 'the loooonnnnng', 'loooonnnnng shots', 'shots were', 'were too', 'too long', 'long how', 'how long', 'long can', 'can you', 'you watch', 'watch two', 'two people', 'people just', 'just sitting', 'sitting there', 'there and', 'and talking', 'talking especially', 'especially when', 'when the', 'the dialogue', 'dialogue is', 'is two', 'two people', 'people complaining', 'complaining i', 'i really', 'really had', 'had a', 'a hard', 'hard time', 'time just', 'just getting', 'getting through', 'through this', 'this film', 'film the', 'the performances', 'performances were', 'were excellent', 'excellent but', 'but how', 'how much']\n",
      "====\n",
      "['vijay krishna', 'krishna acharyas', 'acharyas tashan', 'tashan is', 'is a', 'a overhyped', 'overhyped stylized', 'stylized product', 'product sure', 'sure its', 'its a', 'a one', 'one of', 'of the', 'the most', 'most stylish', 'stylish films', 'films but', 'but when', 'when it', 'it comes', 'comes to', 'to content', 'content even', 'even the', 'the masses', 'masses will', 'will reject', 'reject this', 'this one', 'one why', 'why the', 'the films', 'films script', 'script is', 'is as', 'as amateur', 'amateur as', 'as a', 'a 2', '2 year', 'year old', 'old baby', 'baby script', 'script is', 'is king', 'king without', 'without a', 'a good', 'good script', 'script even', 'even the', 'the greatest', 'greatest director', 'director of', 'of alltime', 'alltime cannot', 'cannot do', 'do anything', 'anything tashan', 'tashan is', 'is produced', 'produced by', 'by the', 'the most', 'most successful', 'successful production', 'production banner', 'banner yash', 'yash raj', 'raj films', 'films and', 'and mega', 'mega stars', 'stars appearing', 'appearing in', 'in it', 'it but', 'but nothing', 'nothing on', 'on earth', 'earth can', 'can save', 'save you', 'you if', 'if you', 'you script', 'script is', 'is bland', 'bland thumbs', 'thumbs down', 'down performances', 'performances anil', 'anil kapoor', 'kapoor is', 'is a', 'a veteran', 'veteran actor', 'actor but', 'but how']\n",
      "====\n",
      "['the worst', 'worst movie', 'movie i', 'i have', 'have seen', 'seen since', 'since tera', 'tera jadoo', 'jadoo chal', 'chal gaya', 'gaya there', 'there is', 'is no', 'no story', 'story no', 'no humor', 'humor no', 'no nothing', 'nothing the', 'the action', 'action sequences', 'sequences seem', 'seem more', 'more like', 'like a', 'a series', 'series of', 'of haphazard', 'haphazard akshay', 'akshay kumar', 'kumar thumbsup', 'thumbsup advertisements', 'advertisements stitched', 'stitched together', 'together heavily', 'heavily influenced', 'influenced from', 'from the', 'the matrix', 'matrix and', 'and kungfu', 'kungfu hustle', 'hustle but', 'but very', 'very poorly', 'poorly executedi', 'executedi did', 'did not', 'not go', 'go a', 'a lot', 'lot of', 'of expectations', 'expectations but', 'but watching', 'watching this', 'this movie', 'movie is', 'is an', 'an exasperating', 'exasperating experience', 'experience which', 'which makes', 'makes you', 'you wonder', 'wonder what', 'what were', 'were these', 'these guys', 'guys thinkingthe', 'thinkingthe only', 'only thing', 'thing you', 'you might', 'might remember', 'remember after', 'after watching', 'watching it', 'it is', 'is an', 'an anorexic', 'anorexic kareena', 'kareena in', 'in a', 'a bikinithe', 'bikinithe reason', 'reason why', 'why i', 'i did', 'did not', 'not give', 'give a', 'a rating', 'rating of', 'of 1', '1 is', 'is that', 'that every', 'every time', 'time i']\n",
      "====\n",
      "\n",
      "\n",
      "Small one hot encoded Sample:\n",
      "[<tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x000001B90B235E10>, <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x000001B90B235C50>, <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x000001B90B235E48>, <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x000001B90B235E80>, <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x000001B90B235EB8>]\n",
      "\n",
      "\n",
      "Encoded Vocabulary\n",
      "{'of the': 0, 'is a': 1, 'airport 77': 2, 'in the': 3, 'have been': 4, 'as a': 5, 'to the': 6, 'the plane': 7, 'by the': 8, 'the three': 9, 'i have': 10, 'have seen': 11, 'this one': 12, 'with a': 13, 'when the': 14, 'this is': 15, 'time oscar': 16, 'oscar winner': 17, 'the most': 18, 'even the': 19, 'script is': 20, 'plane is': 21, 'james stewart': 22, 'oil rig': 23, 'where it': 24, 'for the': 25, '77 is': 26, 'it is': 27, 'three airport': 28, 'airport films': 29, 'so far': 30, 'far i': 31, 'the 747': 32, 'disaster flick': 33, 'as the': 34, 'its a': 35, 'could have': 36, 'lack of': 37, 'or tension': 38, 'while the': 39, 'that much': 40, 'as i': 41, 'should have': 42, 'george kennedy': 43, 'kennedy as': 44, 'couple of': 45, 'of airport': 46, 'with the': 47, 'i reckon': 48, 'a little': 49, 'the action': 50, 'it was': 51, 'was a': 52, 'in this': 53, 'one time': 54, 'i liked': 55, 'this film': 56, 'the actors': 57, 'been the': 58, 'from the': 59, 'in love': 60, 'love with': 61, 'this movie': 62, 'at the': 63, 'can you': 64, 'two people': 65, 'film the': 66, 'but how': 67, 'only thing': 68, 'i think': 69, 'tashan is': 70, 'the films': 71, 'akshay kumar': 72, 'the worst': 73, 'did not': 74, 'is an': 75, '77 starts': 76, 'starts as': 77, 'a brand': 78, 'brand new': 79, 'new luxury': 80, 'luxury 747': 81, '747 plane': 82, 'is loaded': 83, 'loaded up': 84, 'up with': 85, 'with valuable': 86, 'valuable paintings': 87, 'paintings such': 88, 'such belonging': 89, 'belonging to': 90, 'to rich': 91, 'rich businessman': 92, 'businessman philip': 93, 'philip stevens': 94, 'stevens james': 95, 'stewart who': 96, 'who is': 97, 'is flying': 98, 'flying them': 99}\n",
      "\n",
      "\n",
      "Decoded Text from vocabulary (limited by max tokens)\n"
     ]
    }
   ],
   "source": [
    "#//*** Vectorize a corpus\n",
    "class Vectorizer:\n",
    "    def __init__(self,**kwargs):\n",
    "        self.corpus_tokens = []\n",
    "        self.corpus_ngrams = []\n",
    "\n",
    "        self.max_tokens = None\n",
    "        self.ngram_size = 1\n",
    "        self.tidyup = True\n",
    "        \n",
    "        for key,value in kwargs.items():\n",
    "            if key ==\"max_tokens\":\n",
    "                self.max_tokens = value\n",
    "                \n",
    "            if key == \"ngrams\":\n",
    "                self.ngram_size = value\n",
    "            \n",
    "            if key == \"tidyup\":\n",
    "                self.tidyup = value\n",
    "        \n",
    "        \n",
    "        #//*** One Hot Encoding Dictionaries\n",
    "        #//*** Key = Token Index, Value = Word\n",
    "        self.ngram_index = {}\n",
    "        \n",
    "        #//*** Key = Word, Value = Token Index\n",
    "        self.vocabulary_index = {}\n",
    "        \n",
    "    def tokenize(self,raw_text):\n",
    "        #//*** Initialize Output Tokens\n",
    "        tokens = []\n",
    "\n",
    "        #//*** Split Text into words\n",
    "        for x in re.split(\"\\s\",raw_text):\n",
    "\n",
    "            #//*** Findall Non text characters in each word\n",
    "            non_text = re.findall(\"\\W\",x)\n",
    "\n",
    "            #//*** Remove non_text Characters\n",
    "            for i in non_text:\n",
    "                x = x.replace(i,\"\")\n",
    "\n",
    "            #//*** If X has length, append out\n",
    "            if len(x) > 0:\n",
    "                tokens.append(x.lower())\n",
    "        return tokens\n",
    "\n",
    "    def build_ngrams(self):\n",
    "        if self.ngram_size <= 0:\n",
    "            print(\"Ngram size must be an integer > 0\")\n",
    "            print(\"Quitting!\")\n",
    "            return None\n",
    "        \n",
    "        #//*** Using unigrams, use tokens\n",
    "        if self.ngram_size == 1:\n",
    "            self.corpus_ngrams = self.corpus_tokens\n",
    "            return\n",
    "\n",
    "        self.corpus_ngrams = []\n",
    "        \n",
    "        #//*** Get each token group from corpus_tokens\n",
    "        for token in self.corpus_tokens:\n",
    "            \n",
    "            loop_ngram = []\n",
    "            \n",
    "            #//*** Use an index based range to loop through tokens\n",
    "            for x in range(0,len(token) ):\n",
    "\n",
    "                #//*** Check if index + ngram_size exceeds the length of tokens\n",
    "                if x+self.ngram_size <= len(token):\n",
    "\n",
    "                    result = \"\"\n",
    "\n",
    "                    #//*** Build the ngram\n",
    "                    for y in range(self.ngram_size):\n",
    "                        #print(self.tokens[x+y])\n",
    "                        result += token[x+y] + \" \"\n",
    "\n",
    "                    loop_ngram.append(result[:-1])\n",
    "\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            #//*** Token group ngram is built. Add loop_ngram to corpus_ngram\n",
    "            self.corpus_ngrams.append(loop_ngram)\n",
    "        \n",
    "\n",
    "    \n",
    "    def build_vocabulary(self,corpus):\n",
    "        if not isinstance(corpus,list) :\n",
    "            print(\"Vectorizer Requires a corpus (list of text):\")\n",
    "            return None \n",
    "        \n",
    "        self.tokens = []\n",
    "        \n",
    "        print(\"Tokenizing...\")\n",
    "        #//*** Tokenize each text entry in the corpus\n",
    "        for raw_text in corpus:\n",
    "            self.corpus_tokens.append(self.tokenize(raw_text))\n",
    "        \n",
    "        print(\"Building ngrams...\")\n",
    "        #//*** Build ngrams (Defaults to unigrams)\n",
    "        self.build_ngrams()\n",
    "        \n",
    "        word_freq = {}\n",
    "        \n",
    "        print(\"Building Vocabulary...\")\n",
    "        #//*** Build dictionary of unique words\n",
    "        #//*** Loop through each element of the corpus\n",
    "        for element in self.corpus_ngrams:\n",
    "        \n",
    "            #//*** Process each individual ngram\n",
    "            for ngram in element:\n",
    "\n",
    "                #//*** Add unique words to dictionaries\n",
    "                if ngram not in self.vocabulary_index.keys():\n",
    "                    index = len(self.ngram_index.values())\n",
    "                    self.ngram_index[ index ] = ngram\n",
    "                    self.vocabulary_index [ ngram ] = index\n",
    "                    \n",
    "                    #//*** Initialize Word Frequency\n",
    "                    word_freq[ ngram ] = 1\n",
    "                else:\n",
    "                    #//*** Increment Word Frequency\n",
    "                    word_freq[ ngram ] += 1\n",
    "\n",
    "        #//*** END for element in self.corpus_ngrams:\n",
    "        if self.max_tokens != None:\n",
    "            \n",
    "            #//*** Check if token count exceeds max tokens\n",
    "            if self.max_tokens < len(self.ngram_index.items()):\n",
    "                \n",
    "                print(\"Sorting Word Frequency...\")\n",
    "                #//*** Sort the Word Frequency Dictionary. Keep the highest frequency words\n",
    "                word_freq = dict(sorted(word_freq.items(), key=lambda x: x[1], reverse=True))\n",
    "                \n",
    "                print(\"Building Token Dictionary\")\n",
    "                #//*** Get list of keys that are lowest frequency\n",
    "                for key in list(word_freq.keys())[self.max_tokens:]:\n",
    "                    #//*** Delete Low Frequency ngrams\n",
    "                    del word_freq[ key ]\n",
    "                \n",
    "                self.ngram_index = {}\n",
    "                self.vocabulary_index = {}\n",
    "                \n",
    "                print(\"Rebuilding Vocabulary\")\n",
    "                #//*** Rebuild ngram_index & vocabulary_index\n",
    "                for ngram in word_freq.keys():\n",
    "                    index = len(self.ngram_index.values())\n",
    "                    self.ngram_index[ index ] = ngram\n",
    "                    self.vocabulary_index [ ngram ] = index        \n",
    "            \n",
    "            #//*** END Trim Low Frequency ngrams\n",
    "        self.word_freq = word_freq\n",
    "\n",
    "    #//*** One Hot encode the corpus.\n",
    "    #//*** Handling the corpus as a whole increases processing speed\n",
    "    #//*** Hot encode to a sparse tensor to for increased encoding speed compared to a dense array\n",
    "    def one_hot_encode(self,corpus):\n",
    "        \n",
    "        #//*** Encoded Results\n",
    "        results = []\n",
    "        \n",
    "        #//*** Set the Max array size to the total number of items in self.ngram_index\n",
    "        array_size = len(self.ngram_index.keys())\n",
    "        \n",
    " \n",
    "        start_time = datetime.datetime.now()\n",
    "        count = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        for element in corpus:\n",
    "            #//*** hot encode each ngram\n",
    "            result = []\n",
    "            for ngram in element:\n",
    "                \n",
    "                #//*** Skip words not in self.vocabulary_index\n",
    "                #//*** These are skipped due to max_tokens limitations\n",
    "                if ngram not in self.vocabulary_index.keys():\n",
    "                    continue\n",
    "\n",
    "                sparse_tensor = tf.SparseTensor(indices=[[0,self.vocabulary_index[ngram]]],values=[1],dense_shape=[1,array_size])\n",
    "                #index = self.vocabulary_index[ngram]\n",
    "                \n",
    "                #base_array = np.zeros(array_size, dtype=int)\n",
    "                \n",
    "                #base_array [index] = 1\n",
    "                \n",
    "                \n",
    "                #//*** Add the one-hot-encoded word to encoded text\n",
    "                result.append(sparse_tensor)\n",
    "                \n",
    "\n",
    "            #//*** END for ngram in tokens:\n",
    "            \n",
    "            result = tf.sparse.concat(axis=1, sp_inputs=result)\n",
    "            #//*** concat Sparse Matrix\n",
    "            results.append( result )\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            #//*** Print a status update every 1000 items\n",
    "            if count % 100 == 0:\n",
    "                print(f\"{count} / {len(corpus)} Encoded: {datetime.datetime.now() - start_time}\")\n",
    "        \n",
    "        #//*** Concat List of Sparse Matrixes into a sparse matrix\n",
    "        #results =  tf.sparse.concat(axis=1, sp_inputs=results)\n",
    "        \n",
    "        print(f\"Encoding Complete: {datetime.datetime.now() - start_time}\")\n",
    "        \n",
    "        return results        \n",
    "    \n",
    "    def encode(self,corpus):\n",
    "        \n",
    "        if not isinstance(corpus,list) :\n",
    "            print(\"Vectorizer Requires a corpus (list of text):\")\n",
    "            return None\n",
    "\n",
    "        self.corpus_tokens = []\n",
    "        self.corpus_ngrams = []\n",
    "        print(\"Tokenizing...\")\n",
    "        #//*** Tokenize each text entry in the corpus\n",
    "        for raw_text in corpus:\n",
    "            self.corpus_tokens.append(self.tokenize(raw_text))\n",
    "        \n",
    "        print(\"Building ngrams...\")\n",
    "        #//*** Build ngrams (Defaults to unigrams)\n",
    "        self.build_ngrams()\n",
    "        \n",
    "        print(\"One Hot Coding....\")\n",
    "        #//*** One hot encode each text element\n",
    "\n",
    "\n",
    "        #//*** One Hot Encode Values. These are actually sparse tensors for speed.\n",
    "        encoded = self.one_hot_encode(self.corpus_ngrams)\n",
    "   \n",
    "        #//*** TidyUp (Delete) ngrams and Tokens\n",
    "        if self.tidyup:\n",
    "            self.corpus_tokens = []\n",
    "            self.corpus_ngrams = []\n",
    "            \n",
    "        return encoded\n",
    "    \n",
    "    #//*** Convert One-Hot-Encoding to text\n",
    "    def decode(self,elements):\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        #//*** For Each element in Corpus\n",
    "            \n",
    "        decoded = \"\"\n",
    "\n",
    "        #//*** For Each ngram (word(s)) in Elements\n",
    "        for ngram in elements:\n",
    "\n",
    "            #//*** Grab Index of 1 from sparse tensor\n",
    "            index = ngram.indices[0].numpy()[1]\n",
    "            \n",
    "            #ngram = list(ngram.numpy())\n",
    "\n",
    "            decoded += self.ngram_index[ index ] + \" \"\n",
    "\n",
    "        #//*** END for ngram in elements:\n",
    "        results.append( decoded[:-1])\n",
    "            \n",
    "        #//*** END for elements in corpus:\n",
    "        return results\n",
    "\n",
    "\n",
    "#//*** Test the Vectorizer with some sample data\n",
    "vectorizer = Vectorizer(max_tokens=100,ngrams=2, tidyup=False)\n",
    "vectorizer.build_vocabulary(raw_val_text[:5])\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "temp_vals = vectorizer.encode(raw_val_text[:5])\n",
    "\n",
    "print(f\"Run Time: {datetime.datetime.now() - start_time}\")\n",
    "\n",
    "\n",
    "print(\"Sample Text: (First 500 Chars)\")\n",
    "for element in raw_val_text[:5]:\n",
    "    print(element[:500])\n",
    "    print(\"====\")\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"Tokens: (First 100 tokens)\")\n",
    "for token in vectorizer.corpus_tokens:\n",
    "    print(token[:100])\n",
    "    print(\"====\")\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"ngrams: (First 50 tokens)\")\n",
    "for token in vectorizer.corpus_ngrams:\n",
    "    print(token[:100])\n",
    "    print(\"====\")\n",
    "print()\n",
    "print()\n",
    "print(\"Small one hot encoded Sample:\")\n",
    "print(temp_vals)\n",
    "print()\n",
    "print()\n",
    "print(\"Encoded Vocabulary\")\n",
    "print(vectorizer.vocabulary_index)\n",
    "print()\n",
    "print()\n",
    "print(\"Decoded Text from vocabulary (limited by max tokens)\")\n",
    "#for result in vectorizer.decode(temp_vals):\n",
    "#    print(result)\n",
    "#    print()\n",
    "\n",
    "del temp_vals\n",
    "\n",
    "del vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Validation Data...\n",
      "Encoding Training Data...\n",
      "Encoding Test Data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#//*** Test the Vectorizer with some sample data\n",
    "#max_tokens = 20000\n",
    "#ngrams = 1\n",
    "#vectorizer = Vectorizer(max_tokens=max_tokens,ngrams=ngrams)\n",
    "\n",
    "#//*** Build Vocabulary based on the training text\n",
    "#vectorizer.build_vocabulary(raw_train_text)\n",
    "\n",
    "#//*** Encode Validation, training and test data\n",
    "\n",
    "print(\"Encoding Validation Data...\")\n",
    "#val_train = vectorizer.encode(raw_val_text)\n",
    "\n",
    "print(\"Encoding Training Data...\")\n",
    "#x_train = vectorizer.encode(raw_train_text)\n",
    "\n",
    "print(\"Encoding Test Data...\")\n",
    "#y_train = vectorizer.encode(raw_test_text)\n",
    "\n",
    "\"\"\"\n",
    "raw_train_text\n",
    "raw_val_text\n",
    "raw_test_text\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "val_train = vectorizer.encode(raw_val_text[:1000])\n",
    "print(f\"Validation set Encoded: {datetime.datetime.now() - start_time}\")\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/first_edition/6.1-using-word-embeddings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "# Number of words to consider as features\n",
    "max_features = 10000\n",
    "# Cut texts after this number of words \n",
    "# (among top max_features most common words)\n",
    "maxlen = 20\n",
    "\n",
    "# Load the data as lists of integers.\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# This turns our lists of integers\n",
    "# into a 2D integer tensor of shape `(samples, maxlen)`\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "625/625 [==============================] - 1s 844us/step - loss: 0.6677 - acc: 0.6240 - val_loss: 0.6164 - val_acc: 0.7022\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 0s 685us/step - loss: 0.5408 - acc: 0.7513 - val_loss: 0.5268 - val_acc: 0.7328\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 0s 687us/step - loss: 0.4612 - acc: 0.7880 - val_loss: 0.5009 - val_acc: 0.7504\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 0s 771us/step - loss: 0.4209 - acc: 0.8098 - val_loss: 0.4939 - val_acc: 0.7550\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 0s 691us/step - loss: 0.3928 - acc: 0.8256 - val_loss: 0.4940 - val_acc: 0.7554\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 0s 690us/step - loss: 0.3697 - acc: 0.8369 - val_loss: 0.4981 - val_acc: 0.7574\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - 0s 682us/step - loss: 0.3491 - acc: 0.8469 - val_loss: 0.5057 - val_acc: 0.7522\n",
      "Epoch 8/20\n",
      "625/625 [==============================] - 0s 683us/step - loss: 0.3307 - acc: 0.8598 - val_loss: 0.5109 - val_acc: 0.7548\n",
      "Epoch 9/20\n",
      "625/625 [==============================] - 0s 678us/step - loss: 0.3130 - acc: 0.8683 - val_loss: 0.5200 - val_acc: 0.7518\n",
      "Epoch 10/20\n",
      "625/625 [==============================] - 0s 686us/step - loss: 0.2964 - acc: 0.8771 - val_loss: 0.5283 - val_acc: 0.7468\n",
      "Epoch 11/20\n",
      "625/625 [==============================] - 0s 700us/step - loss: 0.2803 - acc: 0.8867 - val_loss: 0.5387 - val_acc: 0.7452\n",
      "Epoch 12/20\n",
      "625/625 [==============================] - 0s 669us/step - loss: 0.2648 - acc: 0.8942 - val_loss: 0.5492 - val_acc: 0.7426\n",
      "Epoch 13/20\n",
      "625/625 [==============================] - 0s 691us/step - loss: 0.2498 - acc: 0.9010 - val_loss: 0.5603 - val_acc: 0.7422\n",
      "Epoch 14/20\n",
      "625/625 [==============================] - 0s 697us/step - loss: 0.2356 - acc: 0.9087 - val_loss: 0.5720 - val_acc: 0.7370\n",
      "Epoch 15/20\n",
      "625/625 [==============================] - 0s 674us/step - loss: 0.2223 - acc: 0.9146 - val_loss: 0.5871 - val_acc: 0.7376\n",
      "Epoch 16/20\n",
      "625/625 [==============================] - 0s 690us/step - loss: 0.2099 - acc: 0.9203 - val_loss: 0.6014 - val_acc: 0.7338\n",
      "Epoch 17/20\n",
      "625/625 [==============================] - 0s 677us/step - loss: 0.1980 - acc: 0.9258 - val_loss: 0.6156 - val_acc: 0.7294\n",
      "Epoch 18/20\n",
      "625/625 [==============================] - 0s 683us/step - loss: 0.1867 - acc: 0.9298 - val_loss: 0.6297 - val_acc: 0.7300\n",
      "Epoch 19/20\n",
      "625/625 [==============================] - 0s 686us/step - loss: 0.1759 - acc: 0.9348 - val_loss: 0.6462 - val_acc: 0.7266\n",
      "Epoch 20/20\n",
      "625/625 [==============================] - 0s 689us/step - loss: 0.1657 - acc: 0.9390 - val_loss: 0.6619 - val_acc: 0.7228\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "# We specify the maximum input length to our Embedding layer\n",
    "# so we can later flatten the embedded inputs\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "# After the Embedding layer, \n",
    "# our activations have shape `(samples, maxlen, 8)`.\n",
    "\n",
    "# We flatten the 3D tensor of embeddings \n",
    "# into a 2D tensor of shape `(samples, maxlen * 8)`\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add the classifier on top\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5094160437583923\n",
      "Test accuracy: 0.7583199739456177\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose = 0) \n",
    "\n",
    "print('Test loss:', score[0]) \n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.ParallelMapDataset"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(binary_1gram_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //*** CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
