{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stoneburner, Kurt\n",
    "- ## DSC 650 - Assignment 10\n",
    "\n",
    "\n",
    "Links to Deep Learning Sample Code:\n",
    "- https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part01_introduction.ipynb\n",
    "\n",
    "- https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part02_sequence-models.ipynb\n",
    "\n",
    "- https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part03_transformer.ipynb\n",
    "\n",
    "- https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part04_sequence-to-sequence-learning.ipynb\n",
    "\n",
    "ngram reference:\n",
    "- https://www.analyticsvidhya.com/blog/2021/09/what-are-n-grams-and-how-to-implement-them-in-python/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n"
     ]
    }
   ],
   "source": [
    "#//*** Get Working Directory\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "\n",
    "#//*** Go up Two folders\n",
    "project_dir = current_dir.parents[2]\n",
    "\n",
    "#//*** IMDB Data Path\n",
    "imdb_path = project_dir.joinpath(\"dsc650/data/external/imdb/aclImdb\")\n",
    "\n",
    "file_path = imdb_path.joinpath(\"train/pos\")\n",
    "\n",
    "#//*** Grab the first positive review text for testing\n",
    "file_path = file_path.joinpath(os.listdir(file_path)[0])\n",
    "\n",
    "with open(file_path,'r') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "print(sample_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10.1 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text:\n",
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
      "\n",
      "Tokens:\n",
      "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', 'such', 'as', 'teachers', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead']\n",
      "\n",
      "ngrams:\n",
      "['Bromwell High', 'High is', 'is a', 'a cartoon', 'cartoon comedy', 'comedy It', 'It ran', 'ran at', 'at the', 'the same', 'same time', 'time as', 'as some', 'some other', 'other programs', 'programs about', 'about school', 'school life', 'life such', 'such as', 'as Teachers', 'Teachers My', 'My 35', '35 years', 'years in', 'in the', 'the teaching', 'teaching profession', 'profession lead', 'lead me']\n",
      "\n",
      "Encoded:\n",
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 64, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 0, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 33, 0, 1, 124, 125, 126, 127, 128, 129, 130, 131]]\n",
      "\n",
      "Encoded Vocabulary\n",
      "{'Bromwell High': 0, 'High is': 1, 'is a': 2, 'a cartoon': 3, 'cartoon comedy': 4, 'comedy It': 5, 'It ran': 6, 'ran at': 7, 'at the': 8, 'the same': 9, 'same time': 10, 'time as': 11, 'as some': 12, 'some other': 13, 'other programs': 14, 'programs about': 15, 'about school': 16, 'school life': 17, 'life such': 18, 'such as': 19, 'as Teachers': 20, 'Teachers My': 21, 'My 35': 22, '35 years': 23, 'years in': 24, 'in the': 25, 'the teaching': 26, 'teaching profession': 27, 'profession lead': 28, 'lead me': 29, 'me to': 30, 'to believe': 31, 'believe that': 32, 'that Bromwell': 33, 'Bromwell Highs': 34, 'Highs satire': 35, 'satire is': 36, 'is much': 37, 'much closer': 38, 'closer to': 39, 'to reality': 40, 'reality than': 41, 'than is': 42, 'is Teachers': 43, 'Teachers The': 44, 'The scramble': 45, 'scramble to': 46, 'to survive': 47, 'survive financially': 48, 'financially the': 49, 'the insightful': 50, 'insightful students': 51, 'students who': 52, 'who can': 53, 'can see': 54, 'see right': 55, 'right through': 56, 'through their': 57, 'their pathetic': 58, 'pathetic teachers': 59, 'teachers pomp': 60, 'pomp the': 61, 'the pettiness': 62, 'pettiness of': 63, 'of the': 64, 'the whole': 65, 'whole situation': 66, 'situation all': 67, 'all remind': 68, 'remind me': 69, 'me of': 70, 'the schools': 71, 'schools I': 72, 'I knew': 73, 'knew and': 74, 'and their': 75, 'their students': 76, 'students When': 77, 'When I': 78, 'I saw': 79, 'saw the': 80, 'the episode': 81, 'episode in': 82, 'in which': 83, 'which a': 84, 'a student': 85, 'student repeatedly': 86, 'repeatedly tried': 87, 'tried to': 88, 'to burn': 89, 'burn down': 90, 'down the': 91, 'the school': 92, 'school I': 93, 'I immediately': 94, 'immediately recalled': 95, 'recalled at': 96, 'at High': 97, 'High A': 98, 'A classic': 99, 'classic line': 100, 'line INSPECTOR': 101, 'INSPECTOR Im': 102, 'Im here': 103, 'here to': 104, 'to sack': 105, 'sack one': 106, 'one of': 107, 'of your': 108, 'your teachers': 109, 'teachers STUDENT': 110, 'STUDENT Welcome': 111, 'Welcome to': 112, 'to Bromwell': 113, 'High I': 114, 'I expect': 115, 'expect that': 116, 'that many': 117, 'many adults': 118, 'adults of': 119, 'of my': 120, 'my age': 121, 'age think': 122, 'think that': 123, 'is far': 124, 'far fetched': 125, 'fetched What': 126, 'What a': 127, 'a pity': 128, 'pity that': 129, 'that it': 130, 'it isnt': 131}\n"
     ]
    }
   ],
   "source": [
    "class Vectorizer:\n",
    "    def __init__(self):\n",
    "        self.tokens = []\n",
    "        self.ngrams = []\n",
    "        self.encoded = []\n",
    "        \n",
    "        #//*** One Hot Encoding Dictionaries\n",
    "        #//*** Key = Token Index, Value = Word\n",
    "        self.token_index = {}\n",
    "        \n",
    "        #//*** Key = Word, Value = Token Index\n",
    "        self.vocabulary_index = {}\n",
    "        \n",
    "    def tokenize(self,raw_text):\n",
    "        #//*** Initialize Output Tokens\n",
    "        self.tokens = []\n",
    "\n",
    "        #//*** Split Text into words\n",
    "        for x in re.split(\"\\s\",raw_text):\n",
    "\n",
    "            #//*** Findall Non text characters in each word\n",
    "            non_text = re.findall(\"\\W\",x)\n",
    "\n",
    "            #//*** Remove non_text Characters\n",
    "            for i in non_text:\n",
    "                x = x.replace(i,\"\")\n",
    "\n",
    "            #//*** If X has length, append out\n",
    "            if len(x) > 0:\n",
    "                self.tokens.append(x.lower())\n",
    "\n",
    "    def build_ngrams(self, ngram_size):\n",
    "        if ngram_size <= 0:\n",
    "            print(\"Ngram size must be an integer > 1 and < tokens size\")\n",
    "            print(\"Quitting!\")\n",
    "            return None\n",
    "\n",
    "        if ngram_size >= len(tokens)-1:\n",
    "            print(\"Ngram size must be an integer > 1 and < tokens size\")\n",
    "            print(\"Quitting!\")\n",
    "            return None\n",
    "\n",
    "        self.ngrams = []\n",
    "        \n",
    "        \n",
    "        #//*** Use an index based range to loop through tokens\n",
    "        for x in range(0,len(self.tokens) ):\n",
    "\n",
    "            #//*** Check if index + ngram_size exceeds the length of tokens\n",
    "            if x+ngram_size <= len(self.tokens):\n",
    "\n",
    "                result = \"\"\n",
    "\n",
    "                #//*** Build the ngram\n",
    "                for y in range(ngram_size):\n",
    "                    result += tokens[x+y] + \" \"\n",
    "                \n",
    "                self.ngrams.append(result[:-1])\n",
    "            \n",
    "            else:\n",
    "                break\n",
    "\n",
    "    def one_hot_encode(self):\n",
    "        #//*** If ngrams Not built use tokens, defaulting to unigrams\n",
    "        if len(self.ngrams) == 0:\n",
    "            self.ngrams = self.tokens\n",
    "        \n",
    "\n",
    "        #//*** Hold a list of encoded \n",
    "        encoded_text = []\n",
    "            \n",
    "        for word in self.ngrams:\n",
    "            \n",
    "            #//*** Add word to dictionaries if needed\n",
    "            if word not in self.token_index.values():\n",
    "                index = len(self.token_index.values())\n",
    "                self.token_index[ index ] = word\n",
    "                self.vocabulary_index [ word ] = index\n",
    "\n",
    "            #print(word,self.vocabulary_index[word])\n",
    "            encoded_text.append(self.vocabulary_index[word])\n",
    "        self.encoded.append(encoded_text)\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "vectorizer.tokenize(sample_text)\n",
    "vectorizer.build_ngrams(2)\n",
    "vectorizer.one_hot_encode()\n",
    "#vectorizer.ngrams[:10]\n",
    "\n",
    "print(\"Sample Text:\")\n",
    "print(sample_text)\n",
    "print()\n",
    "print(\"Tokens:\")\n",
    "print(vectorizer.tokens[:30])\n",
    "print()\n",
    "print(\"ngrams:\")\n",
    "print(vectorizer.ngrams[:30])\n",
    "print()\n",
    "print(\"Encoded:\")\n",
    "print(vectorizer.encoded)\n",
    "print()\n",
    "print(\"Encoded Vocabulary\")\n",
    "print(vectorizer.vocabulary_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //*** CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //*** CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //*** CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
