{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stoneburner, Kurt\n",
    "- ## DSC 650 - Week XX\n",
    "\n",
    "https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms\n",
    "\n",
    "https://bansalh944.medium.com/text-generation-using-lstm-b6ced8629b03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# //*** Imports and Load Data\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "#pd.set_option('display.max_rows', 100)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "\n",
    "import email\n",
    "from email.policy import default\n",
    "from email.parser import Parser\n",
    "\n",
    "   \n",
    "\n",
    "from chardet.universaldetector import UniversalDetector\n",
    "from bs4 import BeautifulSoup\n",
    "    \n",
    "#//*** Quiet the BS4 warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "#book = epub.read_epub('./books/Moby-Dick-Herman-Melville.epub')\n",
    "book = epub.read_epub('./books/pg25525.epub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import tensorflow.compat.v1 as tf \n",
    "\n",
    "tf.enable_eager_execution(tf.ConfigProto(log_device_placement=True)) \n",
    "\n",
    "print(tf.add([1.0, 2.0], [3.0, 4.0])) \n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html_payload(payload):\n",
    "    #from bs4 import BeautifulSoup\n",
    "    \n",
    "    #//*** Quiet the BS4 warnings\n",
    "    #import warnings\n",
    "    #warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "    \"\"\"\n",
    "    This function uses Beautiful Soup to read HTML data\n",
    "    and return the text.  If the payload is plain text, then\n",
    "    Beautiful Soup will return the original content\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(payload, 'html.parser')\n",
    "    #print(soup.find_all(\"p\"))\n",
    "    return str(soup.get_text()).encode('utf-8').decode('utf-8')\n",
    "    #return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Before Cleaning:  2592579\n",
      "Length After Cleaning:  2493812\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"\"\n",
    "for x in book.get_items():\n",
    "    if x.get_type() == 9:\n",
    "        raw_text += parse_html_payload(x.get_body_content())\n",
    "\n",
    "print(\"Length Before Cleaning: \", len(raw_text))\n",
    "#//*******************\n",
    "#//*** Light cleaning\n",
    "#//*******************\n",
    "#//*** Manually remove all text before the first original Poe Story\n",
    "raw_text = raw_text[raw_text.find(\"THE UNPARALLELED ADVENTURES OF ONE HANS PFAAL\")+1:]    \n",
    "\n",
    "#//*** Find the end of his collected works, this will remove all the copyright notices and additional comments\n",
    "end_dex = raw_text.find(\"NOTES\\nOf the many verses from time to time ascribed to the pen of Edgar Poe\")\n",
    "raw_text = raw_text[raw_text.find(\"THE UNPARALLELED ADVENTURES OF ONE HANS PFAAL\"):end_dex]\n",
    "\n",
    "\n",
    "#//*** Remove \\xa0 spacing characters\n",
    "while \"\\xa0\" in raw_text:\n",
    "    raw_text = raw_text.replace(\"\\xa0\",\"\")\n",
    "print(\"Length After Cleaning: \", len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  1\n",
      "Loading Regex: import re\n",
      "Loading Datetime: import datetime\n",
      "Tokenizing String...\n",
      "Building ngrams...type string\n",
      "Building Vocabulary...\n",
      "Sorting Word Frequency...\n",
      "Building Token Dictionary\n",
      "Rebuilding Vocabulary\n",
      "218333 218333\n",
      "Total ngram Count: 218333\n",
      "Max Token ngrams for modeling: 167423\n",
      "167420 3 1000\n",
      "(167420, 3, 1000)\n",
      "10000 / 167420 Encoded: 0:00:00.025006\n",
      "20000 / 167420 Encoded: 0:00:00.050012\n",
      "30000 / 167420 Encoded: 0:00:00.075017\n",
      "40000 / 167420 Encoded: 0:00:00.100023\n",
      "50000 / 167420 Encoded: 0:00:00.126029\n",
      "60000 / 167420 Encoded: 0:00:00.158035\n",
      "70000 / 167420 Encoded: 0:00:00.184042\n",
      "80000 / 167420 Encoded: 0:00:00.209047\n",
      "90000 / 167420 Encoded: 0:00:00.234053\n",
      "100000 / 167420 Encoded: 0:00:00.259059\n",
      "110000 / 167420 Encoded: 0:00:00.284065\n",
      "120000 / 167420 Encoded: 0:00:00.308070\n",
      "130000 / 167420 Encoded: 0:00:00.333076\n",
      "140000 / 167420 Encoded: 0:00:00.358081\n",
      "150000 / 167420 Encoded: 0:00:00.384087\n",
      "160000 / 167420 Encoded: 0:00:00.408092\n",
      "Encoding Complete: 0:00:00.434098\n",
      "(167420, 3, 1000)\n",
      "(167420, 1000)\n"
     ]
    }
   ],
   "source": [
    "#//*** Vectorize a corpus\n",
    "class Vectorizer:\n",
    "   \n",
    "    def __init__(self,**kwargs):\n",
    "        self.corpus_tokens = []\n",
    "        self.corpus_ngrams = []\n",
    "\n",
    "        self.max_tokens = None\n",
    "        self.ngram_size = 1\n",
    "        self.tidyup = True\n",
    "        \n",
    "        self.max_element_count = -1\n",
    "        \n",
    "        for key,value in kwargs.items():\n",
    "            if key ==\"max_tokens\":\n",
    "                self.max_tokens = value\n",
    "                \n",
    "            if key == \"ngrams\":\n",
    "                self.ngram_size = value\n",
    "            \n",
    "            if key == \"tidyup\":\n",
    "                self.tidyup = value\n",
    "        \n",
    "        #//*** Load Regex Locally\n",
    "        if 're' not in globals():\n",
    "            print(\"Loading Regex: import re\")\n",
    "            import re\n",
    "        \n",
    "        self.re = re\n",
    "        \n",
    "        if 'datetime' not in globals():\n",
    "            print(\"Loading Datetime: import datetime\")\n",
    "            import datetime\n",
    "        self.datetime = datetime\n",
    "        \"\"\"\n",
    "        if 'tf' not in globals():\n",
    "            print(\"Loading tensorflow: import tensorflow as tf\")\n",
    "            import tensorflow.compat.v1 as tf \n",
    "        import tensorflow.compat.v1 as tf \n",
    "        self.tf = tf\n",
    "        \"\"\"        \n",
    "        \n",
    "        \n",
    "        #//*** One Hot Encoding Dictionaries\n",
    "        #//*** Key = Token Index, Value = Word\n",
    "        self.ngram_index = {}\n",
    "        \n",
    "        #//*** Key = Word, Value = Token Index\n",
    "        self.vocabulary_index = {}\n",
    "        \n",
    "    def tokenize(self,raw_text):\n",
    "        \n",
    "        #//*** Load Regex Locally\n",
    "        re = self.re\n",
    "\n",
    "            \n",
    "        #//*** Initialize Output Tokens\n",
    "        tokens = []\n",
    "\n",
    "        #//*** Split Text into words\n",
    "        for x in re.split(\"\\s\",raw_text):\n",
    "\n",
    "            #//*** Findall Non text characters in each word\n",
    "            non_text = re.findall(\"\\W\",x)\n",
    "\n",
    "            #//*** Remove non_text Characters\n",
    "            for i in non_text:\n",
    "                x = x.replace(i,\"\")\n",
    "\n",
    "            #//*** If X has length, append out\n",
    "            if len(x) > 0:\n",
    "                tokens.append(x.lower())\n",
    "        return tokens\n",
    "    \n",
    "    #//*** Defaults to building ngrams from a list of lists\n",
    "    #//*** input_type = \"string\" will process a single string for ngrams\n",
    "    def build_ngrams(self,input_type=\"list\"):\n",
    "        if self.ngram_size <= 0:\n",
    "            print(\"Ngram size must be an integer > 0\")\n",
    "            print(\"Quitting!\")\n",
    "            return None\n",
    "        \n",
    "        #//*** Using unigrams, use tokens\n",
    "        if self.ngram_size == 1:\n",
    "            self.corpus_ngrams = self.corpus_tokens\n",
    "            return\n",
    "\n",
    "        self.corpus_ngrams = []\n",
    "        \n",
    "        #//*** Get each token group from corpus_tokens\n",
    "        for token in self.corpus_tokens:\n",
    "            \n",
    "            loop_ngram = []\n",
    "            \n",
    "            #//*** For a singular string we only run this once\n",
    "            if input_type=='string':\n",
    "                token = self.corpus_tokens\n",
    "            \n",
    "            \n",
    "            \n",
    "            #//*** Use an index based range to loop through tokens\n",
    "            for x in range(0,len(token) ):\n",
    "\n",
    "                #//*** Check if index + ngram_size exceeds the length of tokens\n",
    "                if x+self.ngram_size <= len(token):\n",
    "\n",
    "                    result = \"\"\n",
    "\n",
    "                    #//*** Build the ngram\n",
    "                    for y in range(self.ngram_size):\n",
    "                        #print(self.tokens[x+y])\n",
    "                        result += token[x+y] + \" \"\n",
    "\n",
    "                    loop_ngram.append(result[:-1])\n",
    "\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            #//*** Grab Token Element Count, Keep the greatest count value    \n",
    "            if len(loop_ngram) > self.max_element_count:\n",
    "                self.max_element_count = len(loop_ngram)\n",
    "            \n",
    "            \n",
    "            #//*** Token group ngram is built. Add loop_ngram to corpus_ngram\n",
    "            self.corpus_ngrams.append(loop_ngram)\n",
    "            \n",
    "            if input_type=='string':\n",
    "                self.corpus_ngrams = loop_ngram\n",
    "                break\n",
    "\n",
    "    \n",
    "    def build_vocabulary(self,corpus):\n",
    "        self.tokens = []\n",
    "\n",
    "        if isinstance(corpus,list) :\n",
    "            print(\"Tokenizing List of Lists...\")\n",
    "            #//*** Tokenize each text entry in the corpus\n",
    "            for raw_text in corpus:\n",
    "                self.corpus_tokens.append(self.tokenize(raw_text))\n",
    "                \n",
    "            print(\"Building ngrams...type List\")\n",
    "            #//*** Build ngrams (Defaults to unigrams)\n",
    "            self.build_ngrams()\n",
    "        elif isinstance(corpus,str):\n",
    "            print(\"Tokenizing String...\")\n",
    "            self.corpus_tokens = self.tokenize(corpus)\n",
    "            print(\"Building ngrams...type string\")\n",
    "            #//*** Build ngrams (Defaults to unigrams)\n",
    "            self.build_ngrams(input_type=\"string\")\n",
    "        else:\n",
    "            print(\"Vectorizer required format is list or string!\")\n",
    "        \n",
    "        word_freq = {}\n",
    "        \n",
    "        print(\"Building Vocabulary...\")\n",
    "        #//*** Build dictionary of unique words\n",
    "        #//*** Loop through each element of the corpus\n",
    "        for element in self.corpus_ngrams:\n",
    "            \n",
    "            if isinstance(corpus,str):\n",
    "                element = self.corpus_ngrams\n",
    "            \n",
    "            #//*** Grab Token Element Count, Keep the greatest count value    \n",
    "            if len(element) > self.max_element_count:\n",
    "                self.max_element_count = len(element)\n",
    "            \n",
    "            #//*** Process each individual ngram\n",
    "            for ngram in element:\n",
    "\n",
    "                \n",
    "                #//*** Add unique words to dictionaries\n",
    "                if ngram not in self.vocabulary_index.keys():\n",
    "                    index = len(self.ngram_index.values())\n",
    "                    self.ngram_index[ index ] = ngram\n",
    "                    self.vocabulary_index [ ngram ] = index\n",
    "                    \n",
    "                    #//*** Initialize Word Frequency\n",
    "                    word_freq[ ngram ] = 1\n",
    "                else:\n",
    "                    #//*** Increment Word Frequency\n",
    "                    word_freq[ ngram ] += 1\n",
    "            \n",
    "            if isinstance(corpus,str):\n",
    "                break\n",
    "\n",
    "       \n",
    "        #//*** END for element in self.corpus_ngrams:\n",
    "        if self.max_tokens != None:\n",
    "            \n",
    "            #//*** Check if token count exceeds max tokens\n",
    "            if self.max_tokens < len(self.ngram_index.items()):\n",
    "                \n",
    "                print(\"Sorting Word Frequency...\")\n",
    "                #//*** Sort the Word Frequency Dictionary. Keep the highest frequency words\n",
    "                word_freq = dict(sorted(word_freq.items(), key=lambda x: x[1], reverse=True))\n",
    "                \n",
    "                print(\"Building Token Dictionary\")\n",
    "                #//*** Get list of keys that are lowest frequency\n",
    "                for key in list(word_freq.keys())[self.max_tokens:]:\n",
    "                    #//*** Delete Low Frequency ngrams\n",
    "                    del word_freq[ key ]\n",
    "                \n",
    "                self.ngram_index = {}\n",
    "                self.vocabulary_index = {}\n",
    "                \n",
    "                print(\"Rebuilding Vocabulary\")\n",
    "                #//*** Rebuild ngram_index & vocabulary_index\n",
    "                for ngram in word_freq.keys():\n",
    "                    index = len(self.ngram_index.values())\n",
    "                    self.ngram_index[ index ] = ngram\n",
    "                    self.vocabulary_index [ ngram ] = index        \n",
    "            \n",
    "            #//*** END Trim Low Frequency ngrams\n",
    "        self.word_freq = word_freq\n",
    "       \n",
    "    \n",
    "    #//***\n",
    "    def integer_encode(self,corpus):\n",
    "        #//*** Encoded Results\n",
    "        results = []\n",
    "        \n",
    "        #//*** Set the Max array size to the total number of items in self.ngram_index\n",
    "        array_size = len(self.ngram_index.keys())\n",
    "        \n",
    "        datetime = self.datetime\n",
    "        \n",
    "        start_time = datetime.datetime.now()\n",
    "        count = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        for element in corpus:\n",
    "            #//*** hot encode each ngram\n",
    "            result = []\n",
    "            for ngram in element:\n",
    "                \n",
    "                #//*** Skip words not in self.vocabulary_index\n",
    "                #//*** These are skipped due to max_tokens limitations\n",
    "                if ngram not in self.vocabulary_index.keys():\n",
    "                    continue    \n",
    "                \n",
    "                #//*** Get integer value of ngram from dictionary.\n",
    "                #//*** Add to result\n",
    "                result.append(self.vocabulary_index[ngram])\n",
    "                \n",
    "\n",
    "            #//*** END for ngram in tokens:\n",
    "            \n",
    "            #//*** result is a complete encoded element\n",
    "            results.append( np.array(result).astype(np.float32) )\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            #//*** Print a status update every 1000 items\n",
    "            if count % 5000 == 0:\n",
    "                print(f\"{count} / {len(corpus)} Encoded: {datetime.datetime.now() - start_time}\")\n",
    "        \n",
    "        print(f\"Encoding Complete: {datetime.datetime.now() - start_time}\")\n",
    "        \n",
    "        #//*** results is a collection of encoded elements\n",
    "        return np.array(results,dtype=object)\n",
    "    \n",
    "    \n",
    "    def encode(self,corpus,encoding='int'):\n",
    "        \n",
    "        if not isinstance(corpus,list) :\n",
    "            print(\"Vectorizer Requires a corpus (list of text):\")\n",
    "            return None\n",
    "\n",
    "        self.corpus_tokens = []\n",
    "        self.corpus_ngrams = []\n",
    "        print(\"Tokenizing...\")\n",
    "        #//*** Tokenize each text entry in the corpus\n",
    "        for raw_text in corpus:\n",
    "            self.corpus_tokens.append(self.tokenize(raw_text))\n",
    "        \n",
    "        print(\"Building ngrams...\")\n",
    "        #//*** Build ngrams (Defaults to unigrams)\n",
    "        self.build_ngrams()\n",
    "        \n",
    "        if encoding == 'onehot':\n",
    "            print(\"One Hot Coding....\")\n",
    "\n",
    "            #//*** One Hot Encode Values. These are actually sparse tensors for speed.\n",
    "            encoded = self.one_hot_encode(self.corpus_ngrams)\n",
    "   \n",
    "        if encoding == 'int':\n",
    "            print(\"Interger encoding....\")\n",
    "\n",
    "            #//*** Convert ngrams to integers. These are actually sparse tensors for speed.\n",
    "            encoded = self.integer_encode(self.corpus_ngrams)\n",
    "            \n",
    "            #//*** Convert lists to Numpy array of float 32 type. This is the expected structure for Keras\n",
    "            #encoded = np.asarray(encoded).astype('float32')\n",
    "            \n",
    "        #//*** TidyUp (Delete) ngrams and Tokens\n",
    "        if self.tidyup:\n",
    "            self.corpus_tokens = []\n",
    "            self.corpus_ngrams = []\n",
    "        \n",
    "        return encoded\n",
    "\n",
    "\n",
    "    #//*** Convert One-Hot-Encoding to text\n",
    "    def decode(self,elements):\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        #//*** For Each element in Corpus\n",
    "            \n",
    "        decoded = \"\"\n",
    "\n",
    "        #//*** For Each ngram (word(s)) in Elements\n",
    "        for ngram in elements:\n",
    "\n",
    "            #//*** Grab Index of 1 from sparse tensor\n",
    "            index = ngram.indices[0].numpy()[1]\n",
    "            \n",
    "            #ngram = list(ngram.numpy())\n",
    "\n",
    "            decoded += self.ngram_index[ index ] + \" \"\n",
    "\n",
    "        #//*** END for ngram in elements:\n",
    "        results.append( decoded[:-1])\n",
    "        \n",
    "    #//*** One Hot encode the corpus.\n",
    "    #//*** Handling the corpus as a whole increases processing speed\n",
    "    #//*** Hot encode to a sparse tensor to for increased encoding speed compared to a dense array\n",
    "    #//*** S\n",
    "    def one_hot_encode(self,corpus,targets,phrase_size):\n",
    "        \n",
    "        #//*** Encoded Results\n",
    "        results = []\n",
    "        \n",
    "        #//*** Set the Max array size to the total number of items in self.ngram_index\n",
    "        array_size = len(self.ngram_index.keys())\n",
    "        \n",
    "        datetime = self.datetime\n",
    " \n",
    "        start_time = datetime.datetime.now()\n",
    "        count = 0\n",
    "        print( len(corpus), phrase_size, len(self.vocabulary_index.keys()) )\n",
    "        \n",
    "        #//*** Build 3 Dimensional Array for LSTM\n",
    "        x = np.zeros( (len(corpus), phrase_size, len(self.vocabulary_index.keys() ) ), dtype=bool )\n",
    "        y = np.zeros( (len(corpus), len(self.vocabulary_index.keys())), dtype=bool )\n",
    "        print(x.shape)\n",
    "        \n",
    "        #//*** Loop through the corpus, i = index of first dimension of each element\n",
    "        for i, element in enumerate(corpus):\n",
    "            \n",
    "            \n",
    "            #//*** Loop through each ngram\n",
    "            for t, ngram, in enumerate(element):\n",
    "                \n",
    "                #//*** Print this to help make sense of the loop\n",
    "                #print(i, t, element, ngram, self.vocabulary_index[ngram])\n",
    "               \n",
    "                #//*** row, element, ngram\n",
    "                #//*** Corpus / Training\n",
    "                x[ i, t, self.vocabulary_index[ngram] ] = 1\n",
    "            \n",
    "            #//*** Print this to help make sense of the loop   \n",
    "            #print(\"y = \", i, targets[i], self.vocabulary_index[ targets[i] ])         \n",
    "            \n",
    "            #//*** y = 2D array. i == row, and index of the target value\n",
    "            y[ i, self.vocabulary_index[ targets[i] ] ] = 1\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            #//*** Print a status update every 1000 items\n",
    "            if count % 10000 == 0:\n",
    "                print(f\"{count} / {len(corpus)} Encoded: {datetime.datetime.now() - start_time}\")\n",
    "\n",
    "        #//*** Tensorflow is attached as a local variable\n",
    "        #tf = self.tf  \n",
    "        \"\"\"\n",
    "        for element in corpus:\n",
    "            #//*** hot encode each ngram\n",
    "            indexes = []\n",
    "            values = []\n",
    "            dense_shape =  [ len(element),len(self.vocabulary_index.keys()) ]\n",
    "            \n",
    "            #//*** Build the indexes for the sparse tensor. \n",
    "            #//*** This is the row, and the index of the ngram\n",
    "            \n",
    "            #//*** values are always 1. We could get clever and build this once. But we'll just add a one each time we build an index\n",
    "            for row,ngram in enumerate(element):\n",
    "                \n",
    "                #//*** Skip words not in self.vocabulary_index\n",
    "                #//*** These are skipped due to max_tokens limitations\n",
    "                if ngram not in self.vocabulary_index.keys():\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                #//*** Add the one-hot-encoded word to encoded text\n",
    "                indexes.append( [ row,self.vocabulary_index[ngram] ] )\n",
    "                values.append(1)\n",
    "\n",
    "            #//*** END for ngram in tokens:\n",
    "            \n",
    "            #print(dense_shape, len(element), len(indexes), len(values))\n",
    "            \n",
    "            #//*** No items in the dictionary to process. Skip this element\n",
    "            if len(indexes) == 0:\n",
    "                continue\n",
    "            #//*** Build Sparse Tensor\n",
    "            #sparse_tensor = tf.SparseTensor(indices=indexes,values=values,dense_shape=dense_shape)\n",
    "            \n",
    "            results.append(sparse_tensor)\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            #print(sparse_tensor)\n",
    "            \n",
    "            #//*** Print a status update every 1000 items\n",
    "            if count % 10000 == 0:\n",
    "                print(f\"{count} / {len(corpus)} Encoded: {datetime.datetime.now() - start_time}\")\n",
    "        \"\"\"\n",
    "        print(f\"Encoding Complete: {datetime.datetime.now() - start_time}\")\n",
    "        \n",
    "        return x,y\n",
    "            \n",
    "    #//*** Use corpus for text prediction\n",
    "    #//*** Convert Corpus to string if neccessary\n",
    "    #//*** Tokenize & ngram\n",
    "    def encode_words_for_text_prediction(self,corpus,phrase_size=10,step=3):\n",
    "        \n",
    "        big_text = \"\"\n",
    "        if isinstance(corpus,list) :\n",
    "            print(\"Vectorizer compressing list into string\")\n",
    "            \n",
    "            for text in corpus:\n",
    "                big_text += \"\".join(text)\n",
    "            \n",
    "            corpus = big_text\n",
    "\n",
    "        self.corpus_tokens = []\n",
    "        self.corpus_ngrams = []\n",
    "        \n",
    "        self.corpus_tokens = self.tokenize(corpus)\n",
    "        self.build_ngrams(input_type=\"string\")\n",
    "\n",
    "        #//*** use ngrams that are on in vocabulary\n",
    "        self.vocabulary_ngrams = []\n",
    "        \n",
    "        \n",
    "        for ngram in self.corpus_ngrams:\n",
    "            if ngram in self.vocabulary_index.keys():\n",
    "                self.vocabulary_ngrams.append(ngram)\n",
    "                \n",
    "        print(\"Total ngram Count:\",len(self.corpus_ngrams))\n",
    "        print(\"Max Token ngrams for modeling:\",len(self.vocabulary_ngrams))\n",
    "          \n",
    "        train = []\n",
    "        targets = []\n",
    "        \n",
    "        for i in range(0, len(self.vocabulary_ngrams) - phrase_size, step):\n",
    "            #//*** Assign training words.\n",
    "            #//*** Range of Phase size in increments of step\n",
    "            train.append( self.vocabulary_ngrams[i:i+phrase_size] )\n",
    "            \n",
    "            #//*** Build target word, which is the word following the training words\n",
    "            #//*** If Unigram, keep the whole phrase\n",
    "            \n",
    "                \n",
    "            targets.append(self.vocabulary_ngrams[i+phrase_size])\n",
    "            \n",
    "        \n",
    "        return self.one_hot_encode(train,targets,phrase_size=phrase_size)\n",
    "        \n",
    "sample_text = raw_text[:int(len(raw_text)*.5)]\n",
    "max_tokens = 1000\n",
    "phrase_size = 3\n",
    "step = int(phrase_size *.3)\n",
    "\n",
    "if step <= 0:\n",
    "    step = 1\n",
    "print(\"Step: \", step)\n",
    "ngrams=1\n",
    "#//*** Test the Vectorizer with some sample data\n",
    "vectorizer = Vectorizer(max_tokens=max_tokens,ngrams=ngrams, tidyup=False)\n",
    "\n",
    "vectorizer.build_vocabulary(sample_text)   \n",
    "print(len(vectorizer.corpus_tokens),len(vectorizer.corpus_ngrams))\n",
    "#hot_vals = vectorizer.encode(raw_email[:100],encoding='onehot')\n",
    "train,targets = vectorizer.encode_words_for_text_prediction(sample_text,phrase_size=phrase_size,step=step)\n",
    "\n",
    "print(train.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167420, 3, 1000)\n",
      "(167420, 1000)\n",
      "167420\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_21 (LSTM)              (None, 128)               578048    \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1000)              129000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 707,048\n",
      "Trainable params: 707,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(targets.shape)\n",
    "print(len(targets))\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import Dropout\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(phrase_size, max_tokens)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(layers.Dense(max_tokens, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"rmsprop\",metrics=[\"accuracy\"])\n",
    "              \n",
    "model.summary()\n",
    "\n",
    "#model.fit(train, targets, batch_size=128, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1308/1308 [==============================] - 27s 20ms/step - loss: 5.2741 - accuracy: 0.1136\n",
      "Epoch 2/10\n",
      "1308/1308 [==============================] - 25s 19ms/step - loss: 4.9315 - accuracy: 0.1575\n",
      "Epoch 3/10\n",
      "1308/1308 [==============================] - 25s 19ms/step - loss: 4.8016 - accuracy: 0.1712\n",
      "Epoch 4/10\n",
      "1308/1308 [==============================] - 25s 19ms/step - loss: 4.7345 - accuracy: 0.1774\n",
      "Epoch 5/10\n",
      "1308/1308 [==============================] - 25s 19ms/step - loss: 4.6918 - accuracy: 0.1824\n",
      "Epoch 6/10\n",
      "1308/1308 [==============================] - 25s 19ms/step - loss: 4.6561 - accuracy: 0.1853\n",
      "Epoch 7/10\n",
      "1308/1308 [==============================] - 25s 19ms/step - loss: 4.6282 - accuracy: 0.1881\n",
      "Epoch 8/10\n",
      "1308/1308 [==============================] - 25s 19ms/step - loss: 4.5979 - accuracy: 0.1909\n",
      "Epoch 9/10\n",
      "1308/1308 [==============================] - 25s 19ms/step - loss: 4.5685 - accuracy: 0.1938\n",
      "Epoch 10/10\n",
      "1308/1308 [==============================] - 26s 20ms/step - loss: 4.5406 - accuracy: 0.1945\n"
     ]
    }
   ],
   "source": [
    "#//*** 50% Text, 1000 Tokens, size=3, step=.3 size, ngram=1\n",
    "results = model.fit(train, targets, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "262/262 [==============================] - 8s 26ms/step - loss: 5.4465 - accuracy: 0.0985\n",
      "Epoch 2/10\n",
      "262/262 [==============================] - 7s 26ms/step - loss: 5.2845 - accuracy: 0.1014\n",
      "Epoch 3/10\n",
      "262/262 [==============================] - 7s 26ms/step - loss: 5.2082 - accuracy: 0.1215\n",
      "Epoch 4/10\n",
      "262/262 [==============================] - 7s 26ms/step - loss: 5.1279 - accuracy: 0.1334\n",
      "Epoch 5/10\n",
      "262/262 [==============================] - 7s 26ms/step - loss: 5.0531 - accuracy: 0.1405\n",
      "Epoch 6/10\n",
      "262/262 [==============================] - 7s 26ms/step - loss: 4.9788 - accuracy: 0.1500\n",
      "Epoch 7/10\n",
      "262/262 [==============================] - 7s 27ms/step - loss: 4.9035 - accuracy: 0.1599\n",
      "Epoch 8/10\n",
      "262/262 [==============================] - 7s 27ms/step - loss: 4.8367 - accuracy: 0.1666\n",
      "Epoch 9/10\n",
      "262/262 [==============================] - 7s 27ms/step - loss: 4.7745 - accuracy: 0.1721\n",
      "Epoch 10/10\n",
      "262/262 [==============================] - 7s 26ms/step - loss: 4.7210 - accuracy: 0.1763\n"
     ]
    }
   ],
   "source": [
    "#//*** 50% Text, 1000 Tokens, size=5, step=5, ngram=1\n",
    "results = model.fit(train, targets, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "436/436 [==============================] - 14s 26ms/step - loss: 5.4107 - accuracy: 0.0981\n",
      "Epoch 2/10\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 5.2483 - accuracy: 0.1128\n",
      "Epoch 3/10\n",
      "436/436 [==============================] - 11s 26ms/step - loss: 5.1068 - accuracy: 0.1340\n",
      "Epoch 4/10\n",
      "436/436 [==============================] - 11s 26ms/step - loss: 4.9819 - accuracy: 0.1511\n",
      "Epoch 5/10\n",
      "436/436 [==============================] - 12s 26ms/step - loss: 4.8873 - accuracy: 0.1604\n",
      "Epoch 6/10\n",
      "436/436 [==============================] - 12s 26ms/step - loss: 4.8144 - accuracy: 0.1689\n",
      "Epoch 7/10\n",
      "436/436 [==============================] - 12s 27ms/step - loss: 4.7533 - accuracy: 0.1725\n",
      "Epoch 8/10\n",
      "436/436 [==============================] - 12s 27ms/step - loss: 4.6977 - accuracy: 0.1775\n",
      "Epoch 9/10\n",
      "436/436 [==============================] - 12s 27ms/step - loss: 4.6485 - accuracy: 0.1820\n",
      "Epoch 10/10\n",
      "436/436 [==============================] - 12s 27ms/step - loss: 4.6036 - accuracy: 0.1858\n"
     ]
    }
   ],
   "source": [
    "#//*** 50% Text, 1000 Tokens, size=5, step=3, ngram=1\n",
    "results = model.fit(train, targets, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1308/1308 [==============================] - 35s 26ms/step - loss: 5.2555 - accuracy: 0.1147\n",
      "Epoch 2/5\n",
      "1308/1308 [==============================] - 34s 26ms/step - loss: 4.9302 - accuracy: 0.1583\n",
      "Epoch 3/5\n",
      "1308/1308 [==============================] - 34s 26ms/step - loss: 4.7962 - accuracy: 0.1717\n",
      "Epoch 4/5\n",
      "1308/1308 [==============================] - 34s 26ms/step - loss: 4.7273 - accuracy: 0.1778\n",
      "Epoch 5/5\n",
      "1308/1308 [==============================] - 34s 26ms/step - loss: 4.6807 - accuracy: 0.1828\n"
     ]
    }
   ],
   "source": [
    "#//*** 50% Text, 1000 Tokens, size=5, step=1, ngram=1\n",
    "results = model.fit(train, targets, batch_size=128, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "262/262 [==============================] - 13s 42ms/step - loss: 5.4142 - accuracy: 0.0988\n",
      "Epoch 2/10\n",
      "262/262 [==============================] - 11s 43ms/step - loss: 5.2745 - accuracy: 0.1102\n",
      "Epoch 3/10\n",
      "262/262 [==============================] - 11s 42ms/step - loss: 5.1748 - accuracy: 0.1311\n",
      "Epoch 4/10\n",
      "262/262 [==============================] - 11s 41ms/step - loss: 5.0826 - accuracy: 0.1383\n",
      "Epoch 5/10\n",
      "262/262 [==============================] - 11s 42ms/step - loss: 4.9964 - accuracy: 0.1489\n",
      "Epoch 6/10\n",
      "262/262 [==============================] - 11s 41ms/step - loss: 4.9197 - accuracy: 0.1565\n",
      "Epoch 7/10\n",
      "262/262 [==============================] - 11s 41ms/step - loss: 4.8506 - accuracy: 0.1637\n",
      "Epoch 8/10\n",
      "262/262 [==============================] - 11s 41ms/step - loss: 4.7884 - accuracy: 0.1713\n",
      "Epoch 9/10\n",
      "262/262 [==============================] - 11s 41ms/step - loss: 4.7291 - accuracy: 0.1762\n",
      "Epoch 10/10\n",
      "262/262 [==============================] - 11s 41ms/step - loss: 4.6726 - accuracy: 0.1817\n"
     ]
    }
   ],
   "source": [
    "#//*** 50% Text, 1000 Tokens, size=10, step=5, ngram=1\n",
    "results = model.fit(train, targets, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "436/436 [==============================] - 20s 42ms/step - loss: 5.3558 - accuracy: 0.1017\n",
      "Epoch 2/5\n",
      "436/436 [==============================] - 19s 44ms/step - loss: 5.1635 - accuracy: 0.1286\n",
      "Epoch 3/5\n",
      "436/436 [==============================] - 18s 42ms/step - loss: 5.0050 - accuracy: 0.1467\n",
      "Epoch 4/5\n",
      "436/436 [==============================] - 18s 41ms/step - loss: 4.8938 - accuracy: 0.1624\n",
      "Epoch 5/5\n",
      "436/436 [==============================] - 18s 41ms/step - loss: 4.8121 - accuracy: 0.1709\n"
     ]
    }
   ],
   "source": [
    "#//*** 50% Text, 1000 Tokens, size=10, step=3, ngram=1\n",
    "results = model.fit(train, targets, batch_size=128, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "print(len(vectorizer.vocabulary_ngrams))\n",
    "start_index = random.randint(0, len(vectorizer.vocabulary_ngrams) - phrase_size - 1)\n",
    "\n",
    "generated_text = vectorizer.vocabulary_ngrams[start_index : start_index + phrase_size]\n",
    "\n",
    "generated_text\n",
    "\n",
    "for temperature in [.1,1.0, 2.0,3.0,4.0]:\n",
    "    print(\"----- temperature: \", temperature)\n",
    "    print(\"----- Seed Text:   \",\" \".join(generated_text))\n",
    "    predicted_words = []\n",
    "    for i in range(60):\n",
    "        sampled = np.zeros( (1, phrase_size,len(vectorizer.vocabulary_index.keys())) )\n",
    "\n",
    "        for t, char in enumerate(generated_text):\n",
    "\n",
    "            sampled[0, t, vectorizer.vocabulary_index[char]]\n",
    "\n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "\n",
    "        next_index = sample(preds, temperature)\n",
    "\n",
    "        next_char = vectorizer.vocabulary_ngrams[next_index]\n",
    "\n",
    "        generated_text.pop(0)\n",
    "        generated_text.append(next_char)\n",
    "\n",
    "        predicted_words.append(next_char)\n",
    "    print(\" \".join(predicted_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1,60):\n",
    "    print('epoch', epoch)\n",
    "    model.fit(train, targets, batch_size=128, epochs=1)\n",
    "    continue\n",
    "    start_index = random.randint(0, len(vectorizer.vocabulary_ngrams) - phrase_size - 1)\n",
    "\n",
    "    generated_text = vectorizer.vocabulary_ngrams[start_index : start_index + phrase_size]\n",
    "\n",
    "\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print(\"----- temperature: \", temperature)\n",
    "        predicted_words = []\n",
    "        for i in range(60):\n",
    "            sampled = np.zeros( (1, phrase_size,len(vectorizer.vocabulary_index.keys())) )\n",
    "\n",
    "            for t, char in enumerate(generated_text):\n",
    "\n",
    "                sampled[0, t, vectorizer.vocabulary_index[char]]\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = vectorizer.vocabulary_ngrams[next_index]\n",
    "\n",
    "            generated_text.pop(0)\n",
    "            generated_text.append(next_char)\n",
    "\n",
    "            predicted_words.append(next_char)\n",
    "        print(\" \".join(predicted_words))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1,60):\n",
    "    print('epoch', epoch)\n",
    "    model.fit(train, targets, batch_size=128, epochs=1)\n",
    "\n",
    "    start_index = random.randint(0, len(vectorizer.vocabulary_ngrams) - phrase_size - 1)\n",
    "\n",
    "    generated_text = vectorizer.vocabulary_ngrams[start_index : start_index + phrase_size]\n",
    "\n",
    "\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print(\"----- temperature: \", temperature)\n",
    "        predicted_words = []\n",
    "        for i in range(60):\n",
    "            sampled = np.zeros( (1, phrase_size,len(vectorizer.vocabulary_index.keys())) )\n",
    "\n",
    "            for t, char in enumerate(generated_text):\n",
    "\n",
    "                sampled[0, t, vectorizer.vocabulary_index[char]]\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = vectorizer.vocabulary_ngrams[next_index]\n",
    "\n",
    "            generated_text.pop(0)\n",
    "            generated_text.append(next_char)\n",
    "\n",
    "            predicted_words.append(next_char)\n",
    "        print(\" \".join(predicted_words))\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //*** CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
