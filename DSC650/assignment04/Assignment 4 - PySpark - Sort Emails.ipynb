{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Highlights #\n",
    "\n",
    "Parse all sub-folders and files from a given folder:\n",
    "\n",
    "- Universal Detector: Detects the encoding type of a file and returns the file text properly decoded. This is so helpful!\n",
    "\n",
    "\n",
    "- os.walk\n",
    "\n",
    "    for root, dirs, files in os.walk(enron_data_dir):\n",
    "        for file_path in files:\n",
    "        \n",
    "- Parse email using the python email package: https://docs.python.org/3/library/email.examples.html\n",
    "    \n",
    "- Create PySpark Dataframe and Schema: \n",
    "    - https://www.geeksforgeeks.org/how-to-create-pyspark-dataframe-with-schema/\n",
    "    \n",
    "    - https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stonk013\\Anaconda3\\envs\\tensorflow_1\\lib\\site-packages\\pyspark\\context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import email\n",
    "from email.policy import default\n",
    "from email.parser import Parser\n",
    "from datetime import timezone\n",
    "import datetime\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import pandas as pd\n",
    "#import s3fs\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "from chardet.universaldetector import UniversalDetector\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.pipeline import Transformer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "results_dir = current_dir.joinpath('results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "data_dir = current_dir.joinpath('data')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "enron_data_dir = data_dir.joinpath('enron')\n",
    "\n",
    "output_columns = [\n",
    "        'username',\n",
    "        'original_msg',\n",
    "        'payload',\n",
    "        'Message-ID',\n",
    "        'Date',\n",
    "        'From',\n",
    "        'To',\n",
    "        'Subject',\n",
    "        'Mime-Version',\n",
    "        'Content-Type',\n",
    "        'Content-Transfer-Encoding',\n",
    "        'X-From',\n",
    "        'X-To',\n",
    "        'X-cc',\n",
    "        'X-bcc',\n",
    "        'X-Folder',\n",
    "        'X-Origin',\n",
    "        'X-FileName',\n",
    "        'Cc',\n",
    "        'Bcc' \n",
    "]\n",
    "\n",
    "columns = [column.replace('-', '_') for column in output_columns]\n",
    "\n",
    "ParsedEmail = namedtuple('ParsedEmail', columns)\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Assignment04\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Dir:  C:\\Users\\stonk013\\DSC\\DSC650\\assignment04\n",
      "Results Dir:  C:\\Users\\stonk013\\DSC\\DSC650\\assignment04\\results\n",
      "Data Dir:  C:\\Users\\stonk013\\DSC\\DSC650\\assignment04\\data\n",
      "Enron Data Dir:  C:\\Users\\stonk013\\DSC\\DSC650\\assignment04\\data\\enron\n",
      "<property object at 0x000001ADDA561B88>\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Dir: \", current_dir)\n",
    "print(\"Results Dir: \", results_dir)\n",
    "print(\"Data Dir: \", data_dir)\n",
    "print(\"Enron Data Dir: \", enron_data_dir)\n",
    "print(ParsedEmail.username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads data to your local JupyterHub instance. You only need to run this once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#//*** Copied Files manually, avoided Amazon S3 due to ongoing S3 issues.\n",
    "\"\"\"\n",
    "def copy_data_to_local():\n",
    "    dst_data_path = data_dir.joinpath('enron.zip')\n",
    "    endpoint_url='https://storage.budsc.midwest-datascience.com'\n",
    "    enron_data_path = 'data/external/enron.zip'\n",
    "\n",
    "    s3 = s3fs.S3FileSystem(\n",
    "        anon=True,\n",
    "        client_kwargs={\n",
    "            'endpoint_url': endpoint_url\n",
    "        }\n",
    "    )\n",
    "\n",
    "    \n",
    "    s3.get(enron_data_path, str(dst_data_path))\n",
    "    \n",
    "    with zipfile.ZipFile(dst_data_path) as f_zip:\n",
    "        f_zip.extractall(path=data_dir)\n",
    "    \n",
    "copy_data_to_local()\n",
    "\"\"\"\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code reads emails and creates a Spark dataframe with three columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#//*** Use Universal Detector to ascertain the message encoding type.\n",
    "#//*** Returns a text based on the detected encoding type\n",
    "def read_raw_email(email_path):\n",
    "    detector = UniversalDetector()\n",
    "    \n",
    "    try:\n",
    "        with open(email_path) as f:\n",
    "            original_msg = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        detector.reset()\n",
    "        with open(email_path, 'rb') as f:\n",
    "            for line in f.readlines():\n",
    "                detector.feed(line)\n",
    "                if detector.done:\n",
    "                    break\n",
    "        detector.close()\n",
    "        encoding = detector.result['encoding']\n",
    "        with open(email_path, encoding=encoding) as f:\n",
    "            original_msg = f.read()\n",
    "            \n",
    "    return original_msg \n",
    "\n",
    "def make_spark_df():\n",
    "    \n",
    "    #//*** All Fields are Stringtype except Date which is Timestamp type\n",
    "    #//*** PySpark will accept a Datetime Object for timestamp type\n",
    "    schema = StructType([\n",
    "        StructField(\"username\",StringType(),True),\n",
    "        StructField(\"id\",StringType(),True),\n",
    "        StructField(\"original_msg\",StringType(),True),        \n",
    "  ])\n",
    "    records = []\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    for root, dirs, files in os.walk(enron_data_dir):\n",
    "        for file_path in files:\n",
    "            ## Current path is now the file path to the current email.  \n",
    "            ## Use this path to read the following information\n",
    "            ## original_msg\n",
    "            ## username (Hint: It is the root folder)\n",
    "            ## id (The relative path of the email message)\n",
    "            current_path = Path(root).joinpath(file_path)\n",
    "            \n",
    "            #//*** Get raw Email Text Message from File\n",
    "            raw_email = read_raw_email(current_path)\n",
    "            \n",
    "            \n",
    "            row = []\n",
    "            \n",
    "            #//*** Append ID\n",
    "            row.append(Path(root))\n",
    "\n",
    "            #//*** Append Username\n",
    "            row.append(Path(root).parents[0].name)\n",
    "    \n",
    "            #//*** Add Original Message\n",
    "            row.append(raw_email)\n",
    "            \n",
    "            \n",
    "                    \n",
    "            records.append(row)\n",
    "    \n",
    "        if len(records) > 10:\n",
    "            break\n",
    "    \n",
    "    ## TODO: Complete the code to code to create the Spark dataframe\n",
    "    return spark.createDataFrame(records,schema)\n",
    "\n",
    "df = make_spark_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[username: string, id: string, original_msg: string]\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o39.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 0.0 failed 1 times, most recent failure: Lost task 4.0 in stage 0.0 (TID 4) (OW-CASF-REMOTE8.swna.wdpr.disney.com executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for pathlib.WindowsPath)\r\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\r\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\r\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\r\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\r\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\r\n\tat org.apache.spark.api.python.SerDeUtil$.$anonfun$pythonToJava$2(SerDeUtil.scala:121)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for pathlib.WindowsPath)\r\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\r\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\r\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\r\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\r\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\r\n\tat org.apache.spark.api.python.SerDeUtil$.$anonfun$pythonToJava$2(SerDeUtil.scala:121)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-792eedc6cc4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#df.show(n=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#df.show(truncate=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_1\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m         \"\"\"\n\u001b[1;32m--> 680\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_1\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1310\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1312\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_1\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_1\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o39.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 0.0 failed 1 times, most recent failure: Lost task 4.0 in stage 0.0 (TID 4) (OW-CASF-REMOTE8.swna.wdpr.disney.com executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for pathlib.WindowsPath)\r\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\r\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\r\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\r\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\r\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\r\n\tat org.apache.spark.api.python.SerDeUtil$.$anonfun$pythonToJava$2(SerDeUtil.scala:121)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for pathlib.WindowsPath)\r\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\r\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\r\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\r\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\r\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\r\n\tat org.apache.spark.api.python.SerDeUtil$.$anonfun$pythonToJava$2(SerDeUtil.scala:121)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "print(df)\n",
    "df.count()\n",
    "#df.show(n=1)\n",
    "#df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- username: string (nullable = true)\n",
      " |-- original_msg: string (nullable = true)\n",
      " |-- payload: string (nullable = true)\n",
      " |-- Message-ID: string (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- From: string (nullable = true)\n",
      " |-- To: string (nullable = true)\n",
      " |-- Subject: string (nullable = true)\n",
      " |-- Mime-Version: string (nullable = true)\n",
      " |-- Content-Type: string (nullable = true)\n",
      " |-- Content-Transfer-Encoding: string (nullable = true)\n",
      " |-- X-From: string (nullable = true)\n",
      " |-- X-To: string (nullable = true)\n",
      " |-- X-cc: string (nullable = true)\n",
      " |-- X-bcc: string (nullable = true)\n",
      " |-- X-Folder: string (nullable = true)\n",
      " |-- X-Origin: string (nullable = true)\n",
      " |-- X-FileName: string (nullable = true)\n",
      " |-- Cc: string (nullable = true)\n",
      " |-- Bcc: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4.2\n",
    "\n",
    "Use `plain_msg_example` and `html_msg_example` to create a function that parses an email message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_msg_example = \"\"\"\n",
    "Message-ID: <6742786.1075845426893.JavaMail.evans@thyme>\n",
    "Date: Thu, 7 Jun 2001 11:05:33 -0700 (PDT)\n",
    "From: jeffrey.hammad@enron.com\n",
    "To: andy.zipper@enron.com\n",
    "Subject: Thanks for the interview\n",
    "Mime-Version: 1.0\n",
    "Content-Type: text/plain; charset=us-ascii\n",
    "Content-Transfer-Encoding: 7bit\n",
    "X-From: Hammad, Jeffrey </O=ENRON/OU=NA/CN=RECIPIENTS/CN=NOTESADDR/CN=CBBE377A-24F58854-862567DD-591AE7>\n",
    "X-To: Zipper, Andy </O=ENRON/OU=NA/CN=RECIPIENTS/CN=AZIPPER>\n",
    "X-cc: \n",
    "X-bcc: \n",
    "X-Folder: \\Zipper, Andy\\Zipper, Andy\\Inbox\n",
    "X-Origin: ZIPPER-A\n",
    "X-FileName: Zipper, Andy.pst\n",
    "\n",
    "Andy,\n",
    "\n",
    "Thanks for giving me the opportunity to meet with you about the Analyst/ Associate program.  I enjoyed talking to you, and look forward to contributing to the success that the program has enjoyed.  \n",
    "\n",
    "Thanks and Best Regards,\n",
    "\n",
    "Jeff Hammad\n",
    "\"\"\"\n",
    "\n",
    "html_msg_example = \"\"\"\n",
    "Message-ID: <21013632.1075862392611.JavaMail.evans@thyme>\n",
    "Date: Mon, 19 Nov 2001 12:15:44 -0800 (PST)\n",
    "From: insynconline.6jy5ympb.d@insync-palm.com\n",
    "To: tstaab@enron.com\n",
    "Subject: Last chance for special offer on Palm OS Upgrade!\n",
    "Mime-Version: 1.0\n",
    "Content-Type: text/plain; charset=us-ascii\n",
    "Content-Transfer-Encoding: 7bit\n",
    "X-From: InSync Online <InSyncOnline.6jy5ympb.d@insync-palm.com>\n",
    "X-To: THERESA STAAB <tstaab@enron.com>\n",
    "X-cc: \n",
    "X-bcc: \n",
    "X-Folder: \\TSTAAB (Non-Privileged)\\Staab, Theresa\\Deleted Items\n",
    "X-Origin: Staab-T\n",
    "X-FileName: TSTAAB (Non-Privileged).pst\n",
    "\n",
    "<html>\n",
    "\n",
    "<html>\n",
    "<head>\n",
    "<title>Paprika</title>\n",
    "<meta http-equiv=\"Content-Type\" content=\"text/html;\">\n",
    "</head>\n",
    "<body bgcolor=\"#FFFFFF\" TEXT=\"#333333\" LINK=\"#336699\" VLINK=\"#6699cc\" ALINK=\"#ff9900\">\n",
    "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"582\">\n",
    "<tr valign=\"top\">\n",
    "  <td width=\"582\" colspan=\"9\"><nobr><a href=\"http://insync-online.p04.com/u.d?BEReaQA5eczXB=1\"><img src=\"http://images4.postdirect.com/master-images/404707/upper_left.gif\" alt=\"\" width=\"103\" height=\"110\" hspace=\"0\" vspace=\"0\" border=\"0\"></a><a href=\"http://insync-online.p04.com/u.d?AkReaQA5eczXE=11\"><img src=\"http://images4.postdirect.com/master-images/404707/upper_right.gif\" alt=\"\" width=\"479\" height=\"110\" hspace=\"0\" vspace=\"0\" border=\"0\"></a></nobr></td>\n",
    "</tr>\n",
    "<tr valign=\"top\">\n",
    "  <td width=\"4\" bgcolor=\"#CCCCCC\"><img src=\"http://images4.postdirect.com/master-images/404707/clear.gif\" width=\"4\" height=\"1\" hspace=\"0\" vspace=\"0\" border=\"0\" alt=\"\"></td>\n",
    "  <td width=\"20\"><img src=\"http://images4.postdirect.com/master-images/404707/clear.gif\" width=\"20\" height=\"1\" hspace=\"0\" vspace=\"0\" border=\"0\" alt=\"\"></td>\n",
    "  <td width=\"165\"><br><a href=\"http://insync-online.p04.com/u.d?LkReaQA5eczXL=21\"><img src=\"http://images4.postdirect.com/master-images/404707/screen1.gif\" alt=\"\" width=\"165\" height=\"159\" hspace=\"0\" vspace=\"0\" border=\"0\"></a><br><img src=\"http://images4.postdirect.com/master-images/404707/screen1_text.gif\" alt=\"\" width=\"93\" height=\"26\" hspace=\"0\" vspace=\"0\" border=\"0\"></td>\n",
    "  <td width=\"20\"><img src=\"http://images4.postdirect.com/master-images/404707/clear.gif\" width=\"20\" height=\"1\" hspace=\"0\" vspace=\"0\" border=\"0\" alt=\"\"></td>\n",
    "  <td width=\"165\"><br><a href=\"http://insync-online.p04.com/u.d?BkReaQA5eczXO=31\"><img src=\"http://images4.postdirect.com/master-images/404707/screen2.gif\" alt=\"\" width=\"165\" height=\"159\" hspace=\"0\" vspace=\"0\" border=\"0\"></a><br><img src=\"http://images4.postdirect.com/master-images/404707/screen2_text.gif\" alt=\"\" width=\"93\" height=\"26\" hspace=\"0\" vspace=\"0\" border=\"0\"></td>\n",
    "  <td width=\"20\"><img src=\"http://images4.postdirect.com/master-images/404707/clear.gif\" width=\"20\" height=\"1\" hspace=\"0\" vspace=\"0\" border=\"0\" alt=\"\"></td>\n",
    "  <td width=\"165\"><br><a href=\"http://insync-online.p04.com/u.d?JkReaQA5eczXRs=41\"><img src=\"http://images4.postdirect.com/master-images/404707/screen3.gif\" alt=\"\" width=\"165\" height=\"159\" hspace=\"0\" vspace=\"0\" border=\"0\"></a><br><img src=\"http://images4.postdirect.com/master-images/404707/screen3_text.gif\" alt=\"\" width=\"93\" height=\"26\" hspace=\"0\" vspace=\"0\" border=\"0\"></td>\n",
    "  <td width=\"19\"><img src=\"http://images4.postdirect.com/master-images/404707/clear.gif\" width=\"19\" height=\"1\" hspace=\"0\" vspace=\"0\" border=\"0\" alt=\"\"></td>\n",
    "  <td width=\"4\" bgcolor=\"#CCCCCC\"><img src=\"http://images4.postdirect.com/master-images/404707/clear.gif\" width=\"4\" height=\"1\" hspace=\"0\" vspace=\"0\" border=\"0\" alt=\"\"></td>\n",
    "</tr>\n",
    "</table>\n",
    "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"582\">\n",
    "<tr valign=\"top\">\n",
    "  <td width=\"4\" bgcolor=\"#CCCCCC\"><img src=\"http://images4.postdirect.com/master-images/404707/clear.gif\" width=\"4\" height=\"1\" hspace=\"0\" vspace=\"0\" border=\"0\" alt=\"\"></td>\n",
    "  <td width=\"574\"><br>\n",
    "    <table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"574\" bgcolor=\"#99ccff\">\n",
    "    <tr>\n",
    "      <td width=\"50\"><img src=\"http://images4.postdirect.com/master-images/404707/clear.gif\" width=\"50\" height=\"1\" hspace=\"0\" vspace=\"0\" border=\"0\" alt=\"\"></td>\n",
    "      <td width=\"474\"><font face=\"verdana, arial\" size=\"-2\"color=\"#000000\">\n",
    "        <br>\n",
    "        Dear THERESA,\n",
    "        <br><br>\n",
    "        Due to overwhelming demand for the Palm OS&#174; v4.1 Upgrade with Mobile Connectivity, we are \n",
    "        extending the special offer of 25% off through November 30, 2001. So there's still time to significantly \n",
    "        increase the functionality of your Palm&#153; III, IIIx, IIIxe, IIIc, V or Vx handheld. Step up to the \n",
    "        new Palm OS v4.1 through this extended special offer. You'll receive the brand new Palm OS v4.1 \n",
    "        <b>for just $29.95 when you use Promo Code <font color=\"#FF0000\">OS41WAVE</font></b>. That's a \n",
    "        <b>$10 savings</b> off the list price. \n",
    "        <br><br>\n",
    "        <a href=\"http://insync-online.p04.com/u.d?NkReaQA5eczXRh=51\">Click here to view a full product demo now</a>.\n",
    "        <br><br>\n",
    "        <a href=\"http://insync-online.p04.com/u.d?MkReaQA5eczXRm=61\"><img src=\"http://images4.postdirect.com/master-images/404707/title1.gif\" alt=\"\" width=\"336\" height=\"20\" hspace=\"0\" vspace=\"0\" border=\"0\"></a>\n",
    "        <br><br>\n",
    "        You can do a lot more with your Palm&#153; handheld when you upgrade to the Palm OS v4.1. All your \n",
    "        favorite features just got even better and there are some terrific new additions:\n",
    "        <br><br>\n",
    "        <LI> Handwrite notes and even draw pictures right on your Palm&#153 handheld</LI>\n",
    "        <LI> Tap letters with your stylus and use Graffiti&#174; at the same time with the enhanced onscreen keyboard</LI>\n",
    "        <LI> Improved Date Book functionality lets you view, snooze or clear multiple alarms all with a single tap </LI>\n",
    "        <LI> You can easily change time-zone settings</LI>\n",
    "        \n",
    "        <br><br>\n",
    "        <a href=\"http://insync-online.p04.com/u.d?WkReaQA5eczXRb=71\"><img src=\"http://images4.postdirect.com/master-images/404707/title2.gif\" alt=\"\" width=\"460\" height=\"20\" hspace=\"0\" vspace=\"0\" border=\"0\"></a>\n",
    "        <br><br>\n",
    "        <LI> <nobr>Mask/unmask</nobr> private records or hide/unhide directly within the application</LI>\n",
    "        <LI> Lock your device automatically at a designated time using the new Autolocking feature</LI>\n",
    "        <LI> Always remember your password with our new Hint feature*</LI>\n",
    "        \n",
    "        <br><br>\n",
    "        <a href=\"http://insync-online.p04.com/u.d?VEReaQA5eczXRQ=81\"><img src=\"http://images4.postdirect.com/master-images/404707/title3.gif\" alt=\"\" width=\"461\" height=\"31\" hspace=\"0\" vspace=\"0\" border=\"0\"></a>\n",
    "        <br><br>\n",
    "        <LI> Use your GSM compatible mobile phone or modem to get online and access the web</LI>\n",
    "        <LI> Stay connected with email, instant messaging and text messaging to GSM mobile phones</LI>\n",
    "        <LI> Send applications or records through your cell phone to schedule meetings and even \"beam\" \n",
    "             important information to others</LI>\n",
    "        \n",
    "        <br><br>\n",
    "        All this comes in a new operating system that can be yours for just $29.95! <a href=\"http://insync-online.p04.com/u.d?MkReaQA5eczXRV=91\">Click here to \n",
    "        upgrade to the new Palm&#153; OS v4.1</a> and you'll also get the latest Palm desktop software. Or call \n",
    "        <nobr>1-800-881-7256</nobr> to order via phone. \n",
    "        <br><br>\n",
    "        Sincerely,<br>\n",
    "        The Palm Team\n",
    "        <br><br>\n",
    "        P.S. Remember, this extended offer opportunity of 25% savings absolutely ends on November 30, 2001 \n",
    "        and is only available through the Palm Store when you use Promo Code <b><font color=\"#FF0000\">OS41WAVE</font></b>.\n",
    "        <br><br>\n",
    "        <img src=\"http://images4.postdirect.com/master-images/404707/bottom_button.gif\" align=\"right\" alt=\"\" width=\"295\" height=\"60\" hspace=\"0\" vspace=\"0\" border=\"0\">\n",
    "        <br><img src=\"http://images4.postdirect.com/master-images/404707/clear.gif\" width=\"474\" height=\"1\" hspace=\"0\" vspace=\"0\" border=\"0\" alt=\"\">\n",
    "        </font></td>\n",
    "      <td width=\"50\"><img src=\"http://images4.postdirect.com/master-images/404707/clear.gif\" width=\"50\" height=\"1\" hspace=\"0\" vspace=\"0\" border=\"0\" alt=\"\"></td>\n",
    "    </tr>\n",
    "    </table></td>\n",
    "    <td width=\"4\" bgcolor=\"#CCCCCC\"><img src=\"http://images4.postdirect.com/master-images/404707/clear.gif\" width=\"4\" height=\"1\" hspace=\"0\" vspace=\"0\" border=\"0\" alt=\"\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td colspan=\"3\"><img src=\"http://images4.postdirect.com/master-images/404707/bottom.gif\" width=\"582\" height=\"67\" hspace=\"0\" vspace=\"0\" border=\"0\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"582\">\n",
    "  <tr>\n",
    "    <td width=\"54\"><img src=\"http://images4.postdirect.com/master-images/404707/clear.gif\" width=\"54\" height=\"1\" hspace=\"0\" vspace=\"0\" border=\"0\" alt=\"\"></td>\n",
    "    <td width=\"474\"><font face=\"arial, verdana\" size=\"-2\" color=\"#000000\"><br>\n",
    "    * This feature is available on the Palm&#153; IIIx, Palm&#153; IIIxe, and Palm&#153; Vx. <br><br>\n",
    "    ** Note: To use the MIK functionality, you need either a Palm OS&#174; compatible modem or a phone \n",
    "    with  <nobr>built-in</nobr> modem or data capability that has either an infrared port or cable exits.  If you \n",
    "    are using a phone, you must have data services from your mobile service provider.  <a href=\"http://insync-online.p04.com/u.d?RkReaQA5eczXRK=101\">Click here</a> for \n",
    "    a list of tested and supported phones that you can use with the MIK. Cable not provided.\n",
    "    <br><br>\n",
    "    ------------------<br>\n",
    "    To modify your profile or unsubscribe from Palm newsletters, <a href=\"http://insync-online.p04.com/u.d?KkReaQA5eczXRE=121\">click here</a>. \n",
    "    Or, unsubscribe by replying to this message, with \"unsubscribe\" as the subject line of the message. \n",
    "    <br><br>\n",
    "    ------------------<br>\n",
    "    Copyright&#169; 2001 Palm, Inc. Palm OS, Palm Computing, HandFAX, HandSTAMP, HandWEB, Graffiti, \n",
    "    HotSync, iMessenger, MultiMail, Palm.Net, PalmConnect, PalmGlove, PalmModem, PalmPoint, PalmPrint, \n",
    "    and the Palm Platform Compatible Logo are registered trademarks of Palm, Inc. Palm, the Palm logo, \n",
    "    AnyDay, EventClub, HandMAIL, the HotSync Logo, PalmGear, PalmGlove, PalmPix, Palm Powered, the Palm \n",
    "    trade dress, PalmSource, Smartcode, and Simply Palm are trademarks of Palm, Inc. All other brands and \n",
    "    product names may be trademarks or registered trademarks of their respective owners.</font>\n",
    "    <img src=\"http://images4.postdirect.com/master-images/404707/clear.gif\" width=\"474\" height=\"1\" hspace=\"0\" vspace=\"0\" border=\"0\" alt=\"\"></td>\n",
    "    <td width=\"54\"><img src=\"http://images4.postdirect.com/master-images/404707/clear.gif\" width=\"54\" height=\"1\" hspace=\"0\" vspace=\"0\" border=\"0\" alt=\"\"></td>\n",
    "  </tr>\n",
    "</table><br><br><br><br>\n",
    "<!-- The following image is included for message detection -->\n",
    "<img src=\"http://p04.com/1x1.dyn\" border=\"0\" alt=\"\" width=\"1\" height=\"1\">\n",
    "<img src=\"http://p04.com/1x1.dyn?0vEGou8Ig30ba2L2bLn\" width=1 height=1></body>\n",
    "</html>\n",
    "\n",
    "</html>\n",
    "\"\"\"\n",
    "plain_msg_example = plain_msg_example.strip()\n",
    "html_msg_example = html_msg_example.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html_payload(payload):\n",
    "    \"\"\"\n",
    "    This function uses Beautiful Soup to read HTML data\n",
    "    and return the text.  If the payload is plain text, then\n",
    "    Beautiful Soup will return the original content\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(payload, 'html.parser')\n",
    "    return str(soup.get_text()).encode('utf-8').decode('utf-8')\n",
    "\n",
    "def parse_email(original_msg):\n",
    "    result = {}\n",
    "    msg = Parser(policy=default).parsestr(original_msg)\n",
    "    ## TODO: Use Python's email library to read the payload and the headers\n",
    "    ## https://docs.python.org/3/library/email.examples.html\n",
    "    tuple_result = tuple([str(result.get(column, None)) for column in columns])\n",
    "    return ParsedEmail(*tuple_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parsed_msg = parse_email(plain_msg_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parsed_msg.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parsed_html_msg = parse_email(html_msg_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(parsed_html_msg.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This creates a schema for the email data\n",
    "email_struct = StructType()\n",
    "\n",
    "for column in columns:\n",
    "    email_struct.add(column, StringType(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## This creates a user-defined function which can be used in Spark\n",
    "parse_email_func = udf(lambda z: parse_email(z), email_struct)\n",
    "\n",
    "def parse_emails(input_df):\n",
    "    new_df = input_df.select(\n",
    "        'username', 'id', 'original_msg', parse_email_func('original_msg').alias('parsed_email')\n",
    "    )\n",
    "    for column in columns:\n",
    "        new_df = new_df.withColumn(column, new_df.parsed_email[column])\n",
    "    \n",
    "    new_df = new_df.drop('parsed_email')\n",
    "    return new_df\n",
    "        \n",
    "class ParseEmailsTransformer(Transformer):\n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"\n",
    "        Transforms the input dataset.\n",
    "\n",
    "        :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
    "        :returns: transformed dataset\n",
    "        \"\"\"\n",
    "        return dataset.transform(parse_emails)\n",
    "\n",
    "## Use the custom ParseEmailsTransformer, Tokenizer, and CountVectorizer \n",
    "## to create a spark pipeline \n",
    "email_pipeline = Pipeline(\n",
    "    ## TODO: Complete code\n",
    ")\n",
    "model = email_pipeline.fit(df)\n",
    "result = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.select('id', 'words', 'features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Use Universal Detector to ascertain the message encoding type.\n",
    "#//*** Returns a text based on the detected encoding type\n",
    "def read_raw_email(email_path):\n",
    "    detector = UniversalDetector()\n",
    "    \n",
    "    try:\n",
    "        with open(email_path) as f:\n",
    "            original_msg = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        detector.reset()\n",
    "        with open(email_path, 'rb') as f:\n",
    "            for line in f.readlines():\n",
    "                detector.feed(line)\n",
    "                if detector.done:\n",
    "                    break\n",
    "        detector.close()\n",
    "        encoding = detector.result['encoding']\n",
    "        with open(email_path, encoding=encoding) as f:\n",
    "            original_msg = f.read()\n",
    "            \n",
    "    return original_msg \n",
    "\n",
    "def make_spark_df():\n",
    "    \n",
    "    #//*** All Fields are Stringtype except Date which is Timestamp type\n",
    "    #//*** PySpark will accept a Datetime Object for timestamp type\n",
    "    schema = StructType([\n",
    "        StructField(\"username\",StringType(),True),\n",
    "        StructField(\"original_msg\",StringType(),True),\n",
    "        StructField(\"payload\",StringType(),True),\n",
    "        StructField(\"Message-ID\",StringType(),True),\n",
    "        StructField(\"Date\",TimestampType(),True),\n",
    "        StructField(\"From\",StringType(),True),\n",
    "        StructField(\"To\",StringType(),True),\n",
    "        StructField(\"Subject\",StringType(),True),\n",
    "        StructField(\"Mime-Version\",StringType(),True),\n",
    "        StructField(\"Content-Type\",StringType(),True),\n",
    "        StructField(\"Content-Transfer-Encoding\",StringType(),True),\n",
    "        StructField(\"X-From\",StringType(),True),\n",
    "        StructField(\"X-To\",StringType(),True),\n",
    "        StructField(\"X-cc\",StringType(),True),\n",
    "        StructField(\"X-bcc\",StringType(),True),\n",
    "        StructField(\"X-Folder\",StringType(),True),\n",
    "        StructField(\"X-Origin\",StringType(),True),\n",
    "        StructField(\"X-FileName\",StringType(),True),\n",
    "        StructField(\"Cc\",StringType(),True),\n",
    "        StructField(\"Bcc\",StringType(),True),\n",
    "        \n",
    "  ])\n",
    "    records = []\n",
    "    #sc = spark.sparkContext\n",
    "\n",
    "    for root, dirs, files in os.walk(enron_data_dir):\n",
    "        for file_path in files:\n",
    "            ## Current path is now the file path to the current email.  \n",
    "            ## Use this path to read the following information\n",
    "            ## original_msg\n",
    "            ## username (Hint: It is the root folder)\n",
    "            ## id (The relative path of the email message)\n",
    "            current_path = Path(root).joinpath(file_path)\n",
    "            \n",
    "            #//*** Get raw Email Text Message from File\n",
    "            raw_email = read_raw_email(current_path)\n",
    "            \n",
    "            #//*** Parse the Email into Headers Format\n",
    "            headers = Parser(policy=default).parsestr(raw_email)\n",
    "            \n",
    "            row = []\n",
    "\n",
    "            for column in output_columns:\n",
    "                \n",
    "                #//*** Username is the root parent folder name\n",
    "                if column == 'username':\n",
    "                    #print(\"Username: \",Path(root).parents[0].name)\n",
    "                    row.append(Path(root).parents[0].name)\n",
    "                    continue\n",
    "                \n",
    "                #//*** Append the whole unprocessed text file\n",
    "                if column == 'original_msg':\n",
    "                    #print(\"original_msg: \")\n",
    "                    row.append(raw_email)\n",
    "                    continue\n",
    "                \n",
    "                #//*** Append the Payload, which is the message body\n",
    "                if column == 'payload':\n",
    "                    #print('payload: ')\n",
    "                    row.append(headers.get_content())\n",
    "                    continue\n",
    "                \n",
    "                #//*** Convert Date to Datetime\n",
    "                if column == 'Date':\n",
    "                    #//*** Convert Text to Datetime Object\n",
    "                    dt = datetime.datetime.strptime(headers[column],\"%a, %d %b %Y %H:%M:%S %z\")\n",
    "                    row.append(dt)\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                #//*** All other headers are pass-thru strings\n",
    "                if column in headers.keys():\n",
    "                    #print(column, )\n",
    "                    row.append(headers[column])\n",
    "                else:\n",
    "                    #print(column, \" EMPTY\")\n",
    "                    #//*** Empty Field: return Zero Length String\n",
    "                    row.append(\" \")\n",
    "                    \n",
    "            records.append(row)\n",
    "    \n",
    "        if len(records) > 10:\n",
    "            break\n",
    "    \n",
    "    ## TODO: Complete the code to code to create the Spark dataframe\n",
    "    return spark.createDataFrame(records,schema)\n",
    "\n",
    "df = make_spark_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.141600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "        Usage: pi [partitions]\n",
    "    \"\"\"\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonPi\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    partitions = 1\n",
    "    n = 100000 * partitions\n",
    "\n",
    "    def f(_):\n",
    "        x = random() * 2 - 1\n",
    "        y = random() * 2 - 1\n",
    "        return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "    count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "    print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
    "\n",
    "    spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
