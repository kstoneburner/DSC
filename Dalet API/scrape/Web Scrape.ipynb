{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad90e15",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/data-science-skills-web-scraping-javascript-using-python-97a29738353f\n",
    "\n",
    "https://exceptionshub.com/selenium-cant-click-element-because-other-element-obscures-it.html\n",
    "\n",
    "https://www.toolsqa.com/selenium-webdriver/handle-iframes-in-selenium/\n",
    "\n",
    "https://www.selenium.dev/documentation/webdriver/elements/finders/\n",
    "\n",
    "https://www.selenium.dev/selenium/docs/api/javascript/module/selenium-webdriver/index_exports_By.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0473787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d6e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime, requests,random, os, hashlib\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import pandas as pd\n",
    "#//*** install xlsxwriter for pandas\n",
    "#pip install xlsxwriter\n",
    "\n",
    "#//*********************************************************\n",
    "#//*** Build the tgt_date from the slected Quarter and Year\n",
    "#//*********************************************************\n",
    "tgt_year = 2022\n",
    "quarter = \"Q4\"\n",
    "\n",
    "if quarter == \"Q1\":\n",
    "    tgt_date = f\"{tgt_year}-01-01\"\n",
    "elif quarter == \"Q2\": \n",
    "    tgt_date = f\"{tgt_year}-04-01\"\n",
    "elif quarter == \"Q3\": \n",
    "    tgt_date = f\"{tgt_year}-07-01\"\n",
    "elif quarter == \"Q4\": \n",
    "    tgt_date = f\"{tgt_year}-10-01\"\n",
    "\n",
    "tgt_date = datetime.datetime.strptime(tgt_date, \"%Y-%m-%d\")\n",
    "\n",
    "print(tgt_date)\n",
    "\n",
    "#//*** Get Filename for dataframe Cache\n",
    "cache_filepath =  f\"{tgt_year}_{quarter}_scripts.dat\"\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "cache_filepath = current_dir.joinpath(cache_filepath)\n",
    "\n",
    "#//*** If Cache File Exists Load it. Else Start a Fresh DataFrame\n",
    "if os.path.exists(cache_filepath):\n",
    "    cache_df = pd.read_pickle(cache_filepath)\n",
    "else:\n",
    "    cache_df = pd.DataFrame()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cd995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Parses a \n",
    "def get_scripts(source_urls,headless=True):\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    #//*** Initialize Headless Firefox options \n",
    "    options = webdriver.FirefoxOptions()\n",
    "    options.add_argument('-headless')   \n",
    "    \n",
    "    if headless:\n",
    "\n",
    "\n",
    "        #//*** Span Headless Instance\n",
    "        page_driver = webdriver.Firefox(options=options)\n",
    "    else:\n",
    "        # run firefox webdriver from executable path of your choice\n",
    "        page_driver = webdriver.Firefox()\n",
    "\n",
    "    \n",
    "    \n",
    "    for i,url in enumerate(source_urls):\n",
    "        \n",
    "        error_counter = 1\n",
    "        print(f\"({i+1}/{len(source_urls)}) - {url}\")\n",
    "        \n",
    "        while error_counter < 6:\n",
    "            print(\"Downloading Page: Attempt \",error_counter )\n",
    "            try:\n",
    "                page_driver.get(url)\n",
    "                error_counter = 10\n",
    "            except:\n",
    "                error_counter += 1\n",
    "                \n",
    "                print(\"Problem Getting Page...Trying Again\")\n",
    "\n",
    "        #//*** Reset any Story errors \n",
    "        story_error = False\n",
    "        \n",
    "        loop_story = {\n",
    "                \"url\" : url,\n",
    "                \"headline\" : None,\n",
    "                \"time_text\" : None,\n",
    "                \"body_text\" : None,\n",
    "                'time_date' : None,\n",
    "            }\n",
    "        #//*******************\n",
    "        #//**** Get Headline\n",
    "        #//*******************\n",
    "\n",
    "        #FITT_Article_main__body\n",
    "        main_body = page_driver.find_elements(By.CLASS_NAME,\"FITT_Article_main__body\")\n",
    "        \n",
    "        #//*** Verify we have main_body elements\n",
    "        if len(main_body) > 0:\n",
    "            #//*** Assume Main Body is the first element. If not we in trouble\n",
    "            #//*** mmmmm dirty dirty Type conversion\n",
    "            main_body = main_body[0]\n",
    "            \n",
    "\n",
    "            #//*******************\n",
    "            #//**** Get Headline\n",
    "            #//*******************\n",
    "            #//*** Only assign valid headline\n",
    "            \n",
    "            #//*** Get the Headline from the main Body. It should be the first (and only) h1. \n",
    "            headline = main_body.find_elements(By.CSS_SELECTOR,\"h1\")\n",
    "            \n",
    "            #//*** Check to ensure we actually have h1 elements\n",
    "            if len(headline) > 0:\n",
    "                #//*** Dirty dirty type conversion\n",
    "                headline = headline[0].text\n",
    "                \n",
    "                #//*** Assign Headline\n",
    "                loop_story[\"headline\"] = headline\n",
    "        \n",
    "            #//********************\n",
    "            #//**** Get Date\n",
    "            #//********************\n",
    "\n",
    "            #//*** Date is located withing the ShareByline Class\n",
    "            byLine = main_body.find_elements(By.CLASS_NAME,\"ShareByline\")[0]\n",
    "\n",
    "            #//*** Get All divs under ShareByline\n",
    "            div_list = byLine.find_elements(By.CSS_SELECTOR,\"div\")\n",
    "\n",
    "            #//*** The Date will be in a div with no children\n",
    "            for elem in div_list:\n",
    "                #//*** If div has div children, skip it\n",
    "                if len(elem.find_elements(By.CSS_SELECTOR,\"div\")) > 0:\n",
    "                       continue\n",
    "\n",
    "                #//*** Try to convert Element Text to Date\n",
    "                #//*** Skip Element if conversion fails\n",
    "                try:\n",
    "                    d = pd.to_datetime(elem.text)        \n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                #//*** Verify d is actually a timestamp / Date.\n",
    "                #//*** Will get NaT result on empty field\n",
    "                if isinstance(d,pd._libs.tslibs.timestamps.Timestamp):\n",
    "                    loop_story[\"time_text\"] = elem.text\n",
    "                    loop_story['time_date'] = d\n",
    "                    break\n",
    "\n",
    "            #//********************************\n",
    "            #//*** Get Main Body Text\n",
    "            #//********************************\n",
    "\n",
    "            #//*** Article is contained within the article tag\n",
    "            article = main_body.find_elements(By.CSS_SELECTOR,\"article\")\n",
    "\n",
    "            whole_text = \"\"\n",
    "\n",
    "            #//*** Double check we actually have the article\n",
    "            if len(article) > 0:\n",
    "                #//*** We could take the entire article text, but it includes related links and add text.\n",
    "                #//*** We are better than that.\n",
    "                #//*** Get each p element. Keep elements that have 0 children\n",
    "                #//*** In the example, the first p element has one child, probably for datelineing. We'll keep the first\n",
    "                #//*** p element then skip any element that has children.\n",
    "\n",
    "                elems = article[0].find_elements(By.CSS_SELECTOR,\"p\")\n",
    "\n",
    "                #//*** Check to see if we actually have story text\n",
    "                if len(elems) > 0:\n",
    "                    #//*** Assume First elem is valid\n",
    "                    whole_text += elems[0].text\n",
    "                    for elem in elems:\n",
    "                        try:\n",
    "                            #//*** Only Keep elements with no children\n",
    "                            if len(elem.find_elements(By.XPATH, \".//*\")) == 0:\n",
    "                                whole_text += elem.text\n",
    "                        except:\n",
    "                            print(\"Problem processing Story Text. Element may have gone stale\")\n",
    "                            story_error = True \n",
    "\n",
    "                    loop_story[\"body_text\"] = whole_text\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "        else:\n",
    "            print(\"Problem Processing Main Body. Got a core parsing issue\")\n",
    "            story_error = True\n",
    "    \n",
    "        \n",
    "        #//*** Throw an error if any loop_story field is None:\n",
    "        for key,value in loop_story.items():\n",
    "            \n",
    "            if value == None:\n",
    "                story_error = True\n",
    "                print(\"Story Parsing Error:\",key,\"is\",value)\n",
    "            \n",
    "        #//*** Keep Story if No Errors\n",
    "        if not story_error:\n",
    "            out.append(loop_story)\n",
    "        \n",
    "        #wait between 1/2 to 3 seconds\n",
    "        time.sleep(random.randint(500, 3000)/1000)                                                              \n",
    "\n",
    "    page_driver.quit()\n",
    "                                                                        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b701b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "def get_all_urls_in_page(source_url,tgt_date,headless=True):\n",
    "\n",
    "    #//*** Keep all classnames in a dictionary\n",
    "    g = {\n",
    "        #//*** Classname for the Show More Button\n",
    "        \"class_more_button\" : \"show-button-more\",\n",
    "        \n",
    "        #//*** Grid Classname that contains the linked stories\n",
    "        \"story_grid\" : \"grid3\",\n",
    "        \n",
    "        #//*** Story Classes that contain the href Links\n",
    "        \"story_link_class\" : \"AnchorLink\",\n",
    "        \n",
    "    }\n",
    "    \n",
    "    #//*** Initialize Headless Firefox options \n",
    "    options = webdriver.FirefoxOptions()\n",
    "    options.add_argument('-headless')   \n",
    "    \n",
    "    if headless:\n",
    "\n",
    "\n",
    "        #//*** Span Headless Instance\n",
    "        driver = webdriver.Firefox(options=options)\n",
    "    else:\n",
    "        # run firefox webdriver from executable path of your choice\n",
    "        driver = webdriver.Firefox()\n",
    "\n",
    "\n",
    "    # Load Web Page\n",
    "    driver.get(source_url)\n",
    "    \n",
    "    # execute script to scroll down the page\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "\n",
    "\n",
    "    #//*****************************************************\n",
    "    #//*** Wait until the Show More Button is loaded\n",
    "    #//*****************************************************\n",
    "    elements = []\n",
    "    control = 0\n",
    "    while len(elements) == 0:\n",
    "\n",
    "        #//*** Page is Loaded when class_more_button is displayed\n",
    "        #elements = driver.find_elements_by_class_name(g[\"class_more_button\"])\n",
    "        elements = driver.find_elements(By.CLASS_NAME,g[\"class_more_button\"])\n",
    "\n",
    "        control += 1\n",
    "        #//*** Wait if element not found\n",
    "        if len(elements) == 0:\n",
    "            time.sleep(1)\n",
    "\n",
    "        if control > 15:\n",
    "            print(\"Show More Button Not Loaded!\")\n",
    "            return\n",
    "            break\n",
    "\n",
    "    #//*************************************************\n",
    "    #//*** The first elements is the button to click\n",
    "    #//*************************************************\n",
    "    elem_show_more = elements[0]\n",
    "\n",
    "    #actions = ActionChains(driver)\n",
    "    #actions.move_to_element(element).perform()\n",
    "\n",
    "    print(elem_show_more.is_displayed())\n",
    "    print(elem_show_more.location['y'])\n",
    "    \n",
    "    #results = driver.find_elements_by_class_name(g[\"story_grid\"])[0].find_elements_by_class_name(g[\"story_link_class\"])\n",
    "    results = driver.find_elements(By.CLASS_NAME,g[\"story_grid\"])[0].find_elements(By.CLASS_NAME,g[\"story_link_class\"])\n",
    "\n",
    "    if len(results) > 0:\n",
    "        print(\"Last Page: \", results[0].get_property('href'))\n",
    "\n",
    "    \n",
    "    \n",
    "    #print(results[-1].get_property('href'))\n",
    "\n",
    "    \n",
    "    #print(story_time,tgt_date, story_time < tgt_date)\n",
    "                                                                        \n",
    "    #//*** Get the last url to check in stories are within the quarter\n",
    "    last_url = results[-1].get_property('href')\n",
    "    \n",
    "    try:\n",
    "        #//*** Download the last script\n",
    "        last_script = get_scripts([last_url])[0]\n",
    "    except:\n",
    "        print(\"Problem getting last script\")\n",
    "        print(\"Last url: \", last_url)\n",
    "        print(\"get Scripts: \", get_scripts([last_url]))\n",
    "        print(\"Skipping..\")\n",
    "        return []\n",
    "    last_date = last_script['time_date']\n",
    "    print(\"Last Script: \", last_script['headline'],\"\\n\",last_script[\"time_text\"])\n",
    "    print(\"Last Script: \", last_script['time_date'] < tgt_date)\n",
    "    \n",
    "    #//*** Maximum number of while loops.\n",
    "    maxDepth = 20\n",
    "    depth = 0\n",
    "    \n",
    "    #//*** Click Show More until Last Story is older than the tgt_date\n",
    "    while last_date > tgt_date:\n",
    "        depth += 1\n",
    "        \n",
    "        #//*** Scroll to Bottom of page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "        \n",
    "        print(f\"Clicking More Depth: {depth}\")\n",
    "        #print(dir(elem_show_more))\n",
    "        while elem_show_more.is_displayed() == False:\n",
    "            print(\"Is Displayed:\",str(elem_show_more.is_displayed()))\n",
    "            time.sleep(1)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            #//*** Click Show More\n",
    "            elem_show_more.click()\n",
    "        except:\n",
    "            iframes = driver.find_elements_by_tag_name(\"iframe\")\n",
    "            for iframe in iframes:\n",
    "                print(iframe.get_property(\"name\"))\n",
    "                \n",
    "                \n",
    "                driver.execute_script(\"arguments[0].style.visibility='hidden'\", iframe)\n",
    "            #//*** Click Show More\n",
    "            elem_show_more.click()\n",
    "                \n",
    "        #    return\n",
    "\n",
    "    \n",
    "        #//*** Get Updated Results\n",
    "        #results = driver.find_elements_by_class_name(g[\"story_grid\"])[0].find_elements_by_class_name(g[\"story_link_class\"])        \n",
    "        results = driver.find_elements(By.CLASS_NAME,g[\"story_grid\"])[0].find_elements(By.CLASS_NAME,g[\"story_link_class\"])        \n",
    "        \n",
    "        #//*** Get the last url to check in stories are within the quarter\n",
    "        last_url = results[-1].get_property('href')\n",
    "\n",
    "        #//*** Download the last script\n",
    "        last_script = get_scripts([last_url])[0]\n",
    "        last_date = last_script['time_date']\n",
    "        print(\"Last Script: \", last_script['headline'],\"\\n\",last_script[\"time_text\"])\n",
    "        print(\"Last Script: \", last_script['time_date'] < tgt_date)\n",
    "\n",
    "        if depth > maxDepth:\n",
    "            print(\"Maximum Depth Reached on Clicking Show More\")\n",
    "            break\n",
    "        \n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    for result in results:\n",
    "        out.append(result.get_property('href'))\n",
    "    \n",
    "    #//*** Shut down main scrape\n",
    "    driver.quit()\n",
    "    \n",
    "    \n",
    "\n",
    "    return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d3156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Hash url list and compare to cache_df\n",
    "#//*** returns a tuple of urls (orig_urls,dupe_urls)\n",
    "def keep_original_urls(urls,cache_df):\n",
    "    orig_urls = []\n",
    "    dupe_urls = []\n",
    "    \n",
    "    tdf = pd.DataFrame()\n",
    "    tdf['urls'] = urls\n",
    "    \n",
    "    tdf['hash'] = pd.util.hash_pandas_object(tdf['urls'])\n",
    "\n",
    "\n",
    "    #//*** Check for Duplicates\n",
    "    tdf['dupe'] = tdf['hash'].isin(cache_df['hash'])\n",
    "    #tdf['dupe'] = tdf['urls'].isin(cache_df['urls'])\n",
    "    \n",
    "    #//*** Keep Only False Stories, which are not duplicates\n",
    "    #//*** This means a story can have only a single category.\n",
    "    #//*** May have to handle multiple categories at a future time\n",
    "    #print(f\"Length Before: {len(tdf)} \")\n",
    "\n",
    "    orig_urls = list(tdf[tdf['dupe'] == False]['urls'])\n",
    "    dupe_urls = list(tdf[tdf['dupe'] == True]['urls'])\n",
    "\n",
    "    \n",
    "    \n",
    "    return orig_urls,dupe_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b88fc30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db4a2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "url_items = [\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/climate-change/\",\n",
    "        \"cat\" : \"Climate\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/environment/\",\n",
    "        \"cat\" : \"Environment\"\n",
    "    },\n",
    " \n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/economy/\",\n",
    "        \"cat\" : \"Economy\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/jobs/\",\n",
    "        \"cat\" : \"Jobs\"\n",
    "    },    \n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/safety/\",\n",
    "        \"cat\" : \"Safety\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/health/\",\n",
    "        \"cat\" : \"Health\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/education/\",\n",
    "        \"cat\" : \"Education\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/race-and-culture/\",\n",
    "        \"cat\" : \"Race\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/discrimination/\",\n",
    "        \"cat\" : \"Discrimination\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/civil-rights/\",\n",
    "        \"cat\" : \"Civil Rights\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/building-a-better-bay-area/\",\n",
    "        \"cat\" : \"BABBA\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/7onyourside/\",\n",
    "        \"cat\" : \"7OYS\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/covid-19/\",\n",
    "        \"cat\" : \"COVID\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/iteam/\",\n",
    "        \"cat\" : \"I-TEAM\"\n",
    "    },\n",
    "    \n",
    "\n",
    "    \n",
    "]\n",
    "urlpages = ['https://abc7news.com/education/',\n",
    "            'https://abc7news.com/tag/building-a-better-bay-area/',\n",
    "            'https://abc7news.com/7onyourside/',\n",
    "            'https://abc7news.com/tag/covid-19/',\n",
    "            'https://abc7news.com/tag/climate-change/',\n",
    "            'https://abc7news.com/iteam/'\n",
    "           ]\n",
    "\n",
    "#urlpage ='https://abc7news.com/education/'\n",
    "#urls = get_all_urls_in_page(urlpages[1],tgt_date,False)\n",
    "#print(urlpage)\n",
    "\n",
    "#//*** Loop through top level url_items\n",
    "for item in url_items:\n",
    "    cat_url = item['url']\n",
    "    cat = item['cat']\n",
    "    \n",
    "    #//*** Download the ctageory URLs\n",
    "    urls = get_all_urls_in_page(cat_url,tgt_date,False)\n",
    "    \n",
    "    #//*** Get non-duplicate urls to download and a list of the duplicate urls\n",
    "    #//*** We may need to incorporate duplicates with different categories...later\n",
    "    if len(cache_df) > 0:\n",
    "        urls_to_dl,dupe_urls = keep_original_urls(urls,cache_df)\n",
    "    else:\n",
    "        #//*** There is no cache initialize and move one\n",
    "        urls_to_dl = urls\n",
    "        dupe_urls = []\n",
    "    print(\"=====================\")\n",
    "    print(f\"Category: {cat}\")\n",
    "    print(\"=====================\")\n",
    "    print(f\"DL Items: {len(urls_to_dl)} - Duplicate: {len(dupe_urls)}\")\n",
    "    \n",
    "    if len(urls_to_dl) == 0:\n",
    "        print(\"No New Items to Download!\")\n",
    "    \n",
    "    if len(urls_to_dl) > 0:\n",
    "        scripts = get_scripts(urls_to_dl)\n",
    "        \n",
    "        #//*** Convert Scripts to DataFrame\n",
    "        df = pd.DataFrame()\n",
    "        #df['urls'] = urls_to_dl\n",
    "\n",
    "        s = {}\n",
    "\n",
    "        #//*** Get each script value as a list\n",
    "        for script in scripts:\n",
    "\n",
    "            for key,value in script.items():\n",
    "                if key not in s.keys():\n",
    "                    s[key] = []\n",
    "                s[key].append(value)\n",
    "\n",
    "        #//*** build Urls field\n",
    "        #//*** Use the Story URL in case an url was skipped.\n",
    "        df['urls'] = s['url']\n",
    "        df['hash'] = pd.util.hash_pandas_object(df['urls'])\n",
    "        df['cat'] = cat\n",
    "\n",
    "        #//*** Add lists of script values as columns\n",
    "        for key,value in s.items():\n",
    "            df[key] = value\n",
    "\n",
    "        #//*** Combine cache_df and df\n",
    "        cache_df = pd.concat([cache_df,df])\n",
    "\n",
    "\n",
    "#//*** Everything is Gathered. Write Cache_df to disk\n",
    "#//*** Write DF to file\n",
    "pd.to_pickle(cache_df,cache_filepath)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4638fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6375ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache_df['time_date'] = cache_df['time_date'].dt.date\n",
    "#print(\"Length Before de-dupe:\",len(cache_df)\n",
    "#cache_df = cache_df.drop_duplicates(subset=\"url\")\n",
    "      \n",
    "#print(\"Length Before de-dupe:\",len(cache_df)\n",
    "\n",
    "cache_df.drop_duplicates(subset=\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Write to excel file\n",
    "output_filename = f\"{tgt_year}_{quarter}_Collected_Stories.xlsx\"\n",
    "print(output_filename)\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter(output_filename, engine='xlsxwriter')\n",
    "\n",
    "#//*** Build Sheet and Data from From each url_item element\n",
    "for url_item in url_items:\n",
    "    sheet = url_item['cat']\n",
    "    \n",
    "    tdf = cache_df[cache_df['cat'] == sheet].copy()\n",
    "    \n",
    "    tdf = tdf[['urls','time_date','time_text','headline','body_text']].sort_values('time_date',ascending=False)\n",
    "    \n",
    "    tdf.to_excel(writer,sheet_name=sheet)\n",
    "    \n",
    "try:\n",
    "    writer.save()    \n",
    "except:\n",
    "    print(\"Trouble Saving the Spreadsheet. It's Probably Open Somewhere. Close it and Retry\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c84e9104",
   "metadata": {},
   "source": [
    "writer.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9658c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76957a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_df.drop_duplicates(subset=\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25667394",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_items = [\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/climate-change/\",\n",
    "        \"cat\" : \"Climate\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/environment/\",\n",
    "        \"cat\" : \"Environment\"\n",
    "    },\n",
    " \n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/economy/\",\n",
    "        \"cat\" : \"Economy\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/jobs/\",\n",
    "        \"cat\" : \"Jobs\"\n",
    "    },    \n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/safety/\",\n",
    "        \"cat\" : \"Safety\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/health/\",\n",
    "        \"cat\" : \"Health\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/education/\",\n",
    "        \"cat\" : \"Education\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/race-and-culture/\",\n",
    "        \"cat\" : \"Race\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/discrimination/\",\n",
    "        \"cat\" : \"Discrimination\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/civil-rights/\",\n",
    "        \"cat\" : \"Civil Rights\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/building-a-better-bay-area/\",\n",
    "        \"cat\" : \"BABBA\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/7onyourside/\",\n",
    "        \"cat\" : \"7OYS\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/covid-19/\",\n",
    "        \"cat\" : \"COVID\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/iteam/\",\n",
    "        \"cat\" : \"I-TEAM\"\n",
    "    },\n",
    "    \n",
    "\n",
    "    \n",
    "]\n",
    "\n",
    "cache_df['time_date'] = cache_df['time_date'].dt.date\n",
    "\n",
    "#//*** Write to excel file\n",
    "output_filename = f\"{tgt_year}_{quarter}_Collected_Stories.xlsx\"\n",
    "print(output_filename)\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter(output_filename, engine='xlsxwriter')\n",
    "\n",
    "#//*** Build Sheet and Data from From each url_item element\n",
    "for url_item in url_items:\n",
    "    sheet = url_item['cat']\n",
    "    \n",
    "    tdf = cache_df[cache_df['cat'] == sheet].copy()\n",
    "    \n",
    "    tdf = tdf[['urls','time_date','time_text','headline','body_text']]\n",
    "    \n",
    "    tdf.to_excel(writer,sheet_name=sheet)\n",
    "    \n",
    "try:\n",
    "    writer.save()    \n",
    "except:\n",
    "    print(\"Trouble Saving the Spreadsheet. It's Probably Open Somewhere. Close it and Retry\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b0a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "END MAIN PROGRAM\n",
    "Degbugging Code Below\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10967e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98baa66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f513df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8228ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://abc7news.com/california-drought-santa-cruz-county-seawater-intrusion-soquel-creek-water-district/11869614/']\n",
    "get_scripts(urls,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_driver = webdriver.Firefox()\n",
    "page_driver.get(urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab1c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = urls[0]\n",
    "loop_story = {\n",
    "    \"url\" : url,\n",
    "    \"headline\" : page_driver.find_elements(By.CLASS_NAME,\"headline\")[0].text,\n",
    "    \"time_text\" : page_driver.find_elements(By.CLASS_NAME,\"lastmodified\")[0].text,\n",
    "    \"body_text\" : page_driver.find_elements(By.CLASS_NAME,\"body-text\")[0].text,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec116d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "element = page_driver.find_element_by_tag_name(\"body\")\n",
    "raw_text = element.get_attribute('innerHTML')\n",
    "for x in raw_text.split(\">\"):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6a1a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*******************\n",
    "#//**** Get Headline\n",
    "#//*******************\n",
    "\n",
    "#FITT_Article_main__body\n",
    "main_body = page_driver.find_elements(By.CLASS_NAME,\"FITT_Article_main__body\")[0]\n",
    "\n",
    "#//*** Main Body Headline\n",
    "main_body.find_elements(By.CSS_SELECTOR,\"h1\")[0].text\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6644d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#//********************\n",
    "#//**** Get Date\n",
    "#//********************\n",
    "\n",
    "#//*** Date is located withing the ShareByline Class\n",
    "byLine = main_body.find_elements(By.CLASS_NAME,\"ShareByline\")[0]\n",
    " \n",
    "#//*** Get All divs under ShareByline\n",
    "div_list = byLine.find_elements(By.CSS_SELECTOR,\"div\")\n",
    "\n",
    "#//*** The Date will be in a div with no children\n",
    "for elem in div_list:\n",
    "    #//*** If div has div children, skip it\n",
    "    if len(elem.find_elements(By.CSS_SELECTOR,\"div\")) > 0:\n",
    "           continue\n",
    "            \n",
    "    #//*** Try to convert Element Text to Date\n",
    "    #//*** Skip Element if conversion fails\n",
    "    try:\n",
    "        d = pd.to_datetime(elem.text)        \n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    #//*** Verify d is actually a timestamp / Date.\n",
    "    #//*** Will get NaT result on empty field\n",
    "    if isinstance(d,pd._libs.tslibs.timestamps.Timestamp):\n",
    "        print(\"Date is \",d)\n",
    "        break\n",
    "    else:\n",
    "        d = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f0c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Extract Main Body.\n",
    "\n",
    "#//*** Article is contained within the article tag\n",
    "article = main_body.find_elements(By.CSS_SELECTOR,\"article\")\n",
    "\n",
    "whole_text = \"\"\n",
    "\n",
    "#//*** Double check we actually have the article\n",
    "if len(article) > 0:\n",
    "    #//*** We could take the entire article text, but it includes related links and add text.\n",
    "    #//*** We are better than that.\n",
    "    #//*** Get each p element. Keep elements that have 0 children\n",
    "    #//*** In the example, the first p element has one child, probably for datelineing. We'll keep the first\n",
    "    #//*** p element then skip any element that has children.\n",
    "    \n",
    "    elems = article[0].find_elements(By.CSS_SELECTOR,\"p\")\n",
    "    \n",
    "    #//*** Assume First elem is valid\n",
    "    whole_text += elems[0].text\n",
    "    for elem in elems:\n",
    "        #//*** Only Keep elements with no children\n",
    "        if len(elem.find_elements(By.XPATH, \".//*\")) == 0:\n",
    "            whole_text += elem.text\n",
    "else:\n",
    "    pass\n",
    "\n",
    "print(whole_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28da3e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in url_items:\n",
    "    cat_url = item['url']\n",
    "    cat = item['cat']\n",
    "    \n",
    "    #//*** Download the ctageory URLs\n",
    "    urls = get_all_urls_in_page(cat_url,tgt_date,False)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Parses a \n",
    "def get_scripts(source_urls,headless=True):\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    #//*** Initialize Headless Firefox options \n",
    "    options = webdriver.FirefoxOptions()\n",
    "    options.add_argument('-headless')   \n",
    "    \n",
    "    if headless:\n",
    "\n",
    "\n",
    "        #//*** Span Headless Instance\n",
    "        page_driver = webdriver.Firefox(options=options)\n",
    "    else:\n",
    "        # run firefox webdriver from executable path of your choice\n",
    "        page_driver = webdriver.Firefox()\n",
    "\n",
    "    \n",
    "    \n",
    "    for i,url in enumerate(source_urls):\n",
    "        \n",
    "        print(f\"({i+1}/{len(source_urls)}) - {url}\")\n",
    "        try:\n",
    "            page_driver.get(url)\n",
    "        except:\n",
    "            print(\"Problem Getting Page...Skipping\")\n",
    "            continue\n",
    "        \n",
    "        #//*** Reset any Story errors \n",
    "        story_error = False\n",
    "        \n",
    "        loop_story = {\n",
    "                \"url\" : url,\n",
    "                \"headline\" : None,\n",
    "                \"time_text\" : None,\n",
    "                \"body_text\" : None,\n",
    "            }\n",
    "        #//*******************\n",
    "        #//**** Get Headline\n",
    "        #//*******************\n",
    "\n",
    "        #FITT_Article_main__body\n",
    "        main_body = page_driver.find_elements(By.CLASS_NAME,\"FITT_Article_main__body\")\n",
    "        \n",
    "        #//*** Verify we have main_body elements\n",
    "        if len(main_body) > 0:\n",
    "            #//*** Assume Main Body is the first element. If not we in trouble\n",
    "            #//*** mmmmm dirty dirty Type conversion\n",
    "            main_body = main_body[0]\n",
    "            \n",
    "\n",
    "            #//*******************\n",
    "            #//**** Get Headline\n",
    "            #//*******************\n",
    "            #//*** Only assign valid headline\n",
    "            \n",
    "            #//*** Get the Headline from the main Body. It should be the first (and only) h1. \n",
    "            headline = main_body.find_elements(By.CSS_SELECTOR,\"h1\")\n",
    "            \n",
    "            #//*** Check to ensure we actually have h1 elements\n",
    "            if len(headline) > 0:\n",
    "                #//*** Dirty dirty type conversion\n",
    "                headline = headline[0].text\n",
    "                \n",
    "                #//*** Assign Headline\n",
    "                loop_story[\"headline\"] = headline\n",
    "        \n",
    "            #//********************\n",
    "            #//**** Get Date\n",
    "            #//********************\n",
    "\n",
    "            #//*** Date is located withing the ShareByline Class\n",
    "            byLine = main_body.find_elements(By.CLASS_NAME,\"ShareByline\")[0]\n",
    "\n",
    "            #//*** Get All divs under ShareByline\n",
    "            div_list = byLine.find_elements(By.CSS_SELECTOR,\"div\")\n",
    "\n",
    "            #//*** The Date will be in a div with no children\n",
    "            for elem in div_list:\n",
    "                #//*** If div has div children, skip it\n",
    "                if len(elem.find_elements(By.CSS_SELECTOR,\"div\")) > 0:\n",
    "                       continue\n",
    "\n",
    "                #//*** Try to convert Element Text to Date\n",
    "                #//*** Skip Element if conversion fails\n",
    "                try:\n",
    "                    d = pd.to_datetime(elem.text)        \n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                #//*** Verify d is actually a timestamp / Date.\n",
    "                #//*** Will get NaT result on empty field\n",
    "                if isinstance(d,pd._libs.tslibs.timestamps.Timestamp):\n",
    "                    loop_story[\"time_text\"] = d\n",
    "                    break\n",
    "\n",
    "            #//********************************\n",
    "            #//*** Get Main Body Text\n",
    "            #//********************************\n",
    "\n",
    "            #//*** Article is contained within the article tag\n",
    "            article = main_body.find_elements(By.CSS_SELECTOR,\"article\")\n",
    "\n",
    "            whole_text = \"\"\n",
    "\n",
    "            #//*** Double check we actually have the article\n",
    "            if len(article) > 0:\n",
    "                #//*** We could take the entire article text, but it includes related links and add text.\n",
    "                #//*** We are better than that.\n",
    "                #//*** Get each p element. Keep elements that have 0 children\n",
    "                #//*** In the example, the first p element has one child, probably for datelineing. We'll keep the first\n",
    "                #//*** p element then skip any element that has children.\n",
    "\n",
    "                elems = article[0].find_elements(By.CSS_SELECTOR,\"p\")\n",
    "\n",
    "                #//*** Assume First elem is valid\n",
    "                whole_text += elems[0].text\n",
    "                for elem in elems:\n",
    "                    #//*** Only Keep elements with no children\n",
    "                    if len(elem.find_elements(By.XPATH, \".//*\")) == 0:\n",
    "                        whole_text += elem.text\n",
    "                \n",
    "                loop_story[\"body_text\"] = whole_text\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "        else:\n",
    "            print(\"Problem Processing Main Body. Got a core parsing issue\")\n",
    "            story_error = True\n",
    "        \n",
    "        loop_story[\"body_text\"] = None\n",
    "        \n",
    "        #//*** Throw an error if any loop_story field is None:\n",
    "        for key,value in loop_story.items():\n",
    "            \n",
    "            if value == None:\n",
    "                story_error = True\n",
    "                print(\"Story Parsing Error:\",key,\"is\",value)\n",
    "            \n",
    "        #//*** Keep Story if No Errors\n",
    "        if not story_error:\n",
    "            out.append(loop_story)\n",
    "        \n",
    "        #wait between 1/2 to 3 seconds\n",
    "        time.sleep(random.randint(500, 3000)/1000)                                                              \n",
    "\n",
    "    page_driver.quit()\n",
    "                                                                        \n",
    "    return out\n",
    "\n",
    "scripts = get_scripts(['https://abc7news.com/president-joe-biden-climate-change-executive-actions-emergency-declaration/12062764/'])\n",
    "print(scripts[0])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c08b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#//*** Split the Date\n",
    "loop_story['time_date'] = loop_story['time_text'].split()\n",
    "\n",
    "#//*** Rebuild Date string using only the first four fields. Drop the extra time\n",
    "loop_story['time_date'] = \" \".join(loop_story['time_date'][:4])\n",
    "\n",
    "loop_story['time_date'] = datetime.datetime.strptime(loop_story['time_date'], \"%A, %B %d, %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cb8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Backup Copy of original get_scripts\n",
    "\"\"\"\n",
    "#//*** Parses a \n",
    "def get_scripts(source_urls,headless=True):\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    #//*** Initialize Headless Firefox options \n",
    "    options = webdriver.FirefoxOptions()\n",
    "    options.add_argument('-headless')   \n",
    "    \n",
    "    if headless:\n",
    "    if headless:\n",
    "\n",
    "\n",
    "        #//*** Span Headless Instance\n",
    "        page_driver = webdriver.Firefox(options=options)\n",
    "    else:\n",
    "        # run firefox webdriver from executable path of your choice\n",
    "        page_driver = webdriver.Firefox()\n",
    "\n",
    "    \n",
    "    \n",
    "    for i,url in enumerate(source_urls):\n",
    "        \n",
    "        print(f\"({i+1}/{len(source_urls)}) - {url}\")\n",
    "        try:\n",
    "            page_driver.get(url)\n",
    "        except:\n",
    "            print(\"Problem Getting Page...Skipping\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        try:\n",
    "        \n",
    "            loop_story = {\n",
    "                \"url\" : url,\n",
    "                \"headline\" : page_driver.find_elements(By.CLASS_NAME,\"headline\")[0].text,\n",
    "                \"time_text\" : page_driver.find_elements(By.CLASS_NAME,\"lastmodified\")[0].text,\n",
    "                \"body_text\" : page_driver.find_elements(By.CLASS_NAME,\"body-text\")[0].text,\n",
    "            }\n",
    "            #//*** Split the Date\n",
    "            loop_story['time_date'] = loop_story['time_text'].split()\n",
    "\n",
    "            #//*** Rebuild Date string using only the first four fields. Drop the extra time\n",
    "            loop_story['time_date'] = \" \".join(loop_story['time_date'][:4])\n",
    "\n",
    "            loop_story['time_date'] = datetime.datetime.strptime(loop_story['time_date'], \"%A, %B %d, %Y\")\n",
    "        except:\n",
    "            print(\"Problem Processing Story. Skipping\")\n",
    "            continue\n",
    "        out.append(loop_story)\n",
    "        \n",
    "        #wait between 1/2 to 3 seconds\n",
    "        time.sleep(random.randint(500, 3000)/1000)                                                              \n",
    "\n",
    "    page_driver.quit()\n",
    "                                                                        \n",
    "    return out\n",
    "\"\"\"\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
