{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad90e15",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/data-science-skills-web-scraping-javascript-using-python-97a29738353f\n",
    "\n",
    "https://exceptionshub.com/selenium-cant-click-element-because-other-element-obscures-it.html\n",
    "\n",
    "https://www.toolsqa.com/selenium-webdriver/handle-iframes-in-selenium/\n",
    "\n",
    "https://www.selenium.dev/documentation/webdriver/elements/finders/\n",
    "\n",
    "https://www.selenium.dev/selenium/docs/api/javascript/module/selenium-webdriver/index_exports_By.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0473787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d6e2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import time, datetime, requests,random, os, hashlib\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import pandas as pd\n",
    "#//*** install xlsxwriter for pandas\n",
    "#pip install xlsxwriter\n",
    "\n",
    "#//*********************************************************\n",
    "#//*** Build the tgt_date from the slected Quarter and Year\n",
    "#//*********************************************************\n",
    "tgt_year = 2022\n",
    "quarter = \"Q1\"\n",
    "\n",
    "if quarter == \"Q1\":\n",
    "    tgt_date = f\"{tgt_year}-01-01\"\n",
    "elif quarter == \"Q2\": \n",
    "    tgt_date = f\"{tgt_year}-04-01\"\n",
    "elif quarter == \"Q3\": \n",
    "    tgt_date = f\"{tgt_year}-07-01\"\n",
    "elif quarter == \"Q4\": \n",
    "    tgt_date = f\"{tgt_year}-10-01\"\n",
    "\n",
    "tgt_date = datetime.datetime.strptime(tgt_date, \"%Y-%m-%d\")\n",
    "\n",
    "print(tgt_date)\n",
    "\n",
    "#//*** Get Filename for dataframe Cache\n",
    "cache_filepath =  f\"{tgt_year}_{quarter}_scripts.dat\"\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "cache_filepath = current_dir.joinpath(cache_filepath)\n",
    "\n",
    "#//*** If Cache File Exists Load it. Else Start a Fresh DataFrame\n",
    "if os.path.exists(cache_filepath):\n",
    "    cache_df = pd.read_pickle(cache_filepath)\n",
    "else:\n",
    "    cache_df = pd.DataFrame()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58cb8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Parses a \n",
    "def get_scripts(source_urls,headless=True):\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    #//*** Initialize Headless Firefox options \n",
    "    options = webdriver.FirefoxOptions()\n",
    "    options.add_argument('-headless')   \n",
    "    \n",
    "    if headless:\n",
    "\n",
    "\n",
    "        #//*** Span Headless Instance\n",
    "        page_driver = webdriver.Firefox(options=options)\n",
    "    else:\n",
    "        # run firefox webdriver from executable path of your choice\n",
    "        page_driver = webdriver.Firefox()\n",
    "\n",
    "    \n",
    "    \n",
    "    for i,url in enumerate(source_urls):\n",
    "        \n",
    "        print(f\"({i+1}/{len(source_urls)}) - {url}\")\n",
    "        try:\n",
    "            page_driver.get(url)\n",
    "        except:\n",
    "            print(\"Problem Getting Page...Skipping\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        try:\n",
    "        \n",
    "            loop_story = {\n",
    "                \"url\" : url,\n",
    "                \"headline\" : page_driver.find_elements(By.CLASS_NAME,\"headline\")[0].text,\n",
    "                \"time_text\" : page_driver.find_elements(By.CLASS_NAME,\"lastmodified\")[0].text,\n",
    "                \"body_text\" : page_driver.find_elements(By.CLASS_NAME,\"body-text\")[0].text,\n",
    "            }\n",
    "            #//*** Split the Date\n",
    "            loop_story['time_date'] = loop_story['time_text'].split()\n",
    "\n",
    "            #//*** Rebuild Date string using only the first four fields. Drop the extra time\n",
    "            loop_story['time_date'] = \" \".join(loop_story['time_date'][:4])\n",
    "\n",
    "            loop_story['time_date'] = datetime.datetime.strptime(loop_story['time_date'], \"%A, %B %d, %Y\")\n",
    "        except:\n",
    "            print(\"Problem Processing Story. Skipping\")\n",
    "            continue\n",
    "        out.append(loop_story)\n",
    "        \n",
    "        #wait between 1/2 to 3 seconds\n",
    "        time.sleep(random.randint(500, 3000)/1000)                                                              \n",
    "\n",
    "    page_driver.quit()\n",
    "                                                                        \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b701b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "def get_all_urls_in_page(source_url,tgt_date,headless=True):\n",
    "\n",
    "    #//*** Keep all classnames in a dictionary\n",
    "    g = {\n",
    "        #//*** Classname for the Show More Button\n",
    "        \"class_more_button\" : \"show-button-more\",\n",
    "        \n",
    "        #//*** Grid Classname that contains the linked stories\n",
    "        \"story_grid\" : \"grid3\",\n",
    "        \n",
    "        #//*** Story Classes that contain the href Links\n",
    "        \"story_link_class\" : \"AnchorLink\",\n",
    "        \n",
    "    }\n",
    "    \n",
    "    #//*** Initialize Headless Firefox options \n",
    "    options = webdriver.FirefoxOptions()\n",
    "    options.add_argument('-headless')   \n",
    "    \n",
    "    if headless:\n",
    "\n",
    "\n",
    "        #//*** Span Headless Instance\n",
    "        driver = webdriver.Firefox(options=options)\n",
    "    else:\n",
    "        # run firefox webdriver from executable path of your choice\n",
    "        driver = webdriver.Firefox()\n",
    "\n",
    "\n",
    "    # Load Web Page\n",
    "    driver.get(source_url)\n",
    "    \n",
    "    # execute script to scroll down the page\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "\n",
    "\n",
    "    #//*****************************************************\n",
    "    #//*** Wait until the Show More Button is loaded\n",
    "    #//*****************************************************\n",
    "    elements = []\n",
    "    control = 0\n",
    "    while len(elements) == 0:\n",
    "\n",
    "        #//*** Page is Loaded when class_more_button is displayed\n",
    "        #elements = driver.find_elements_by_class_name(g[\"class_more_button\"])\n",
    "        elements = driver.find_elements(By.CLASS_NAME,g[\"class_more_button\"])\n",
    "\n",
    "        control += 1\n",
    "        #//*** Wait if element not found\n",
    "        if len(elements) == 0:\n",
    "            time.sleep(1)\n",
    "\n",
    "        if control > 15:\n",
    "            print(\"Show More Button Not Loaded!\")\n",
    "            return\n",
    "            break\n",
    "\n",
    "    #//*************************************************\n",
    "    #//*** The first elements is the button to click\n",
    "    #//*************************************************\n",
    "    elem_show_more = elements[0]\n",
    "\n",
    "    #actions = ActionChains(driver)\n",
    "    #actions.move_to_element(element).perform()\n",
    "\n",
    "    print(elem_show_more.is_displayed())\n",
    "    print(elem_show_more.location['y'])\n",
    "    \n",
    "    #results = driver.find_elements_by_class_name(g[\"story_grid\"])[0].find_elements_by_class_name(g[\"story_link_class\"])\n",
    "    results = driver.find_elements(By.CLASS_NAME,g[\"story_grid\"])[0].find_elements(By.CLASS_NAME,g[\"story_link_class\"])\n",
    "\n",
    "    if len(results) > 0:\n",
    "        print(\"Last Page: \", results[0].get_property('href'))\n",
    "\n",
    "    \n",
    "    \n",
    "    #print(results[-1].get_property('href'))\n",
    "\n",
    "    \n",
    "    #print(story_time,tgt_date, story_time < tgt_date)\n",
    "                                                                        \n",
    "    #//*** Get the last url to check in stories are within the quarter\n",
    "    last_url = results[-1].get_property('href')\n",
    "    \n",
    "    #//*** Download the last script\n",
    "    last_script = get_scripts([last_url])[0]\n",
    "    last_date = last_script['time_date']\n",
    "    print(\"Last Script: \", last_script['headline'],\"\\n\",last_script[\"time_text\"])\n",
    "    print(\"Last Script: \", last_script['time_date'] < tgt_date)\n",
    "    \n",
    "    #//*** Maximum number of while loops.\n",
    "    maxDepth = 20\n",
    "    depth = 0\n",
    "    \n",
    "    #//*** Click Show More until Last Story is older than the tgt_date\n",
    "    while last_date > tgt_date:\n",
    "        depth += 1\n",
    "        \n",
    "        #//*** Scroll to Bottom of page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "        \n",
    "        print(f\"Clicking More Depth: {depth}\")\n",
    "        #print(dir(elem_show_more))\n",
    "        while elem_show_more.is_displayed() == False:\n",
    "            print(\"Is Displayed:\",str(elem_show_more.is_displayed()))\n",
    "            time.sleep(1)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            #//*** Click Show More\n",
    "            elem_show_more.click()\n",
    "        except:\n",
    "            iframes = driver.find_elements_by_tag_name(\"iframe\")\n",
    "            for iframe in iframes:\n",
    "                print(iframe.get_property(\"name\"))\n",
    "                \n",
    "                \n",
    "                driver.execute_script(\"arguments[0].style.visibility='hidden'\", iframe)\n",
    "            #//*** Click Show More\n",
    "            elem_show_more.click()\n",
    "                \n",
    "        #    return\n",
    "\n",
    "    \n",
    "        #//*** Get Updated Results\n",
    "        #results = driver.find_elements_by_class_name(g[\"story_grid\"])[0].find_elements_by_class_name(g[\"story_link_class\"])        \n",
    "        results = driver.find_elements(By.CLASS_NAME,g[\"story_grid\"])[0].find_elements(By.CLASS_NAME,g[\"story_link_class\"])        \n",
    "        \n",
    "        #//*** Get the last url to check in stories are within the quarter\n",
    "        last_url = results[-1].get_property('href')\n",
    "\n",
    "        #//*** Download the last script\n",
    "        last_script = get_scripts([last_url])[0]\n",
    "        last_date = last_script['time_date']\n",
    "        print(\"Last Script: \", last_script['headline'],\"\\n\",last_script[\"time_text\"])\n",
    "        print(\"Last Script: \", last_script['time_date'] < tgt_date)\n",
    "\n",
    "        if depth > maxDepth:\n",
    "            print(\"Maximum Depth Reached on Clicking Show More\")\n",
    "            break\n",
    "        \n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    for result in results:\n",
    "        out.append(result.get_property('href'))\n",
    "    \n",
    "    #//*** Shut down main scrape\n",
    "    driver.quit()\n",
    "    \n",
    "    \n",
    "\n",
    "    return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d3156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Hash url list and compare to cache_df\n",
    "#//*** returns a tuple of urls (orig_urls,dupe_urls)\n",
    "def keep_original_urls(urls,cache_df):\n",
    "    orig_urls = []\n",
    "    dupe_urls = []\n",
    "    \n",
    "    tdf = pd.DataFrame()\n",
    "    tdf['urls'] = urls\n",
    "    \n",
    "    tdf['hash'] = pd.util.hash_pandas_object(tdf['urls'])\n",
    "\n",
    "\n",
    "    #//*** Check for Duplicates\n",
    "    tdf['dupe'] = tdf['hash'].isin(cache_df['hash'])\n",
    "    #tdf['dupe'] = tdf['urls'].isin(cache_df['urls'])\n",
    "    \n",
    "    #//*** Keep Only False Stories, which are not duplicates\n",
    "    #//*** This means a story can have only a single category.\n",
    "    #//*** May have to handle multiple categories at a future time\n",
    "    #print(f\"Length Before: {len(tdf)} \")\n",
    "\n",
    "    orig_urls = list(tdf[tdf['dupe'] == False]['urls'])\n",
    "    dupe_urls = list(tdf[tdf['dupe'] == True]['urls'])\n",
    "\n",
    "    \n",
    "    \n",
    "    return orig_urls,dupe_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b88fc30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82db4a2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1797\n",
      "Last Page:  https://abc7news.com/california-wildfires-artificial-intelligence-wifire-lab-burnpro-3d/11596528/\n",
      "(1/1) - https://abc7news.com/penguin-antarctica-colony-climate-change-crisis/11491709/\n",
      "Last Script:  Discovery of penguin colony in Antarctic appears to be another sign of climate crisis \n",
      " Thursday, January 20, 2022\n",
      "Last Script:  False\n",
      "Clicking More Depth: 1\n",
      "(1/1) - https://abc7news.com/penguin-antarctica-colony-climate-change-crisis/11491709/\n",
      "Last Script:  Discovery of penguin colony in Antarctic appears to be another sign of climate crisis \n",
      " Thursday, January 20, 2022\n",
      "Last Script:  False\n",
      "Clicking More Depth: 2\n",
      "(1/1) - https://abc7news.com/walnut-creek-canal-gasoline-spill-settlement-sfpp/11350405/\n",
      "Last Script:  Pipeline operator to pay millions for last year's 63K-gallon gasoline spill in Walnut Creek canal \n",
      " Thursday, December 16, 2021\n",
      "Last Script:  True\n",
      "=====================\n",
      "Category: Climate\n",
      "=====================\n",
      "DL Items: 17 - Duplicate: 7\n",
      "(1/17) - https://abc7news.com/california-wildfires-artificial-intelligence-wifire-lab-burnpro-3d/11596528/\n",
      "(2/17) - https://abc7news.com/colorado-fire-big-sur-california-wildfires-ca-drought-climate-change/11517556/\n",
      "(3/17) - https://abc7news.com/newsom-zero-emission-vehicles-2022-budget-california-blueprint-package-ca/11511342/\n",
      "(4/17) - https://abc7news.com/colorado-fire-big-sur-monterey-county-california-wildfires/11500438/\n",
      "(5/17) - https://abc7news.com/penguin-antarctica-colony-climate-change-crisis/11491709/\n",
      "(6/17) - https://abc7news.com/sf-crab-season-delayed-fishermans-wharf-crabs/11485170/\n",
      "(7/17) - https://abc7news.com/california-water-underground-basins-reservoirs-climate-change/11464838/\n",
      "(8/17) - https://abc7news.com/california-governor-gavin-newsom-transportation-infrastructure-climate-change/11463759/\n",
      "(9/17) - https://abc7news.com/the-new-york-times-best-places-to-visit-52-for-a-changed-world-san-francisco/11461709/\n",
      "(10/17) - https://abc7news.com/microplastics-plastic-pollution-plastics-in-drains-how-to-reduce/11451236/\n",
      "(11/17) - https://abc7news.com/newsom-2022-budget-ca-california-surplus/11451458/\n",
      "(12/17) - https://abc7news.com/california-drought-salmon-shortage-impact-dying/11429850/\n",
      "(13/17) - https://abc7news.com/ca-academy-of-sciences-new-species-coral-reef-climate-change-fish/11425138/\n",
      "(14/17) - https://abc7news.com/compost-ca-laws-food-scraps-composting-2022/11403775/\n",
      "(15/17) - https://abc7news.com/ca-composting-law-compost-how-to-california-new/11416032/\n",
      "(16/17) - https://abc7news.com/snow-survey-sierra-snowpack-lake-tahoe/11408449/\n",
      "(17/17) - https://abc7news.com/walnut-creek-canal-gasoline-spill-settlement-sfpp/11350405/\n",
      "True\n",
      "1805\n",
      "Last Page:  https://abc7news.com/biden-newsom-lithium-mineral-mining-in-california-imperial-valley-salton-sea/11590753/\n",
      "(1/1) - https://abc7news.com/microplastics-plastic-pollution-plastics-in-drains-how-to-reduce/11451236/\n",
      "Last Script:  Recent storms washed microplastics into San Francisco Bay, studies show \n",
      " Wednesday, January 12, 2022\n",
      "Last Script:  False\n",
      "Clicking More Depth: 1\n",
      "(1/1) - https://abc7news.com/buy-nothing-day-greenpeace-black-friday-initiative/11272840/\n",
      "Last Script:  Buy Nothing Day: A counteractive movement to consumerism of Black Friday \n",
      " Friday, November 26, 2021\n",
      "Last Script:  True\n",
      "=====================\n",
      "Category: Environment\n",
      "=====================\n",
      "DL Items: 0 - Duplicate: 24\n",
      "No New Items to Download!\n",
      "True\n",
      "1797\n",
      "Last Page:  https://abc7news.com/amc-theaters-surge-pricing/11618294/\n",
      "(1/1) - https://abc7news.com/right-to-repair-sb-983-electronics-do-it-yourself-repairs/11574258/\n",
      "Last Script:  Right to Repair bill could save families $330 per year, say supporters \n",
      " Tuesday, February 22, 2022\n",
      "Last Script:  False\n",
      "Clicking More Depth: 1\n",
      "(1/1) - https://abc7news.com/appraisal-home-appraisals-industry-reform/11489431/\n",
      "Problem Getting Page...Skipping\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_73412/3544412991.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m#//*** Download the ctageory URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_all_urls_in_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtgt_date\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m#//*** Get non-duplicate urls to download and a list of the duplicate urls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_73412/3659220232.py\u001b[0m in \u001b[0;36mget_all_urls_in_page\u001b[1;34m(source_url, tgt_date, headless)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;31m#//*** Download the last script\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mlast_script\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_scripts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlast_url\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[0mlast_date\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlast_script\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time_date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Last Script: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_script\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'headline'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlast_script\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"time_text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "url_items = [\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/climate-change/\",\n",
    "        \"cat\" : \"Climate\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/environment/\",\n",
    "        \"cat\" : \"Environment\"\n",
    "    },\n",
    " \n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/economy/\",\n",
    "        \"cat\" : \"Economy\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/jobs/\",\n",
    "        \"cat\" : \"Jobs\"\n",
    "    },    \n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/safety/\",\n",
    "        \"cat\" : \"Safety\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/health/\",\n",
    "        \"cat\" : \"Health\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/education/\",\n",
    "        \"cat\" : \"Education\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/race-and-culture/\",\n",
    "        \"cat\" : \"Race\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/discrimination/\",\n",
    "        \"cat\" : \"Discrimination\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/civil-rights/\",\n",
    "        \"cat\" : \"Civil Rights\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/building-a-better-bay-area/\",\n",
    "        \"cat\" : \"BABBA\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/7onyourside/\",\n",
    "        \"cat\" : \"7OYS\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/covid-19/\",\n",
    "        \"cat\" : \"COVID\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/iteam/\",\n",
    "        \"cat\" : \"I-TEAM\"\n",
    "    },\n",
    "    \n",
    "\n",
    "    \n",
    "]\n",
    "urlpages = ['https://abc7news.com/education/',\n",
    "            'https://abc7news.com/tag/building-a-better-bay-area/',\n",
    "            'https://abc7news.com/7onyourside/',\n",
    "            'https://abc7news.com/tag/covid-19/',\n",
    "            'https://abc7news.com/tag/climate-change/',\n",
    "            'https://abc7news.com/iteam/'\n",
    "           ]\n",
    "\n",
    "#urlpage ='https://abc7news.com/education/'\n",
    "#urls = get_all_urls_in_page(urlpages[1],tgt_date,False)\n",
    "#print(urlpage)\n",
    "\n",
    "#//*** Loop through top level url_items\n",
    "for item in url_items:\n",
    "    cat_url = item['url']\n",
    "    cat = item['cat']\n",
    "    \n",
    "    #//*** Download the ctageory URLs\n",
    "    urls = get_all_urls_in_page(cat_url,tgt_date,False)\n",
    "    \n",
    "    #//*** Get non-duplicate urls to download and a list of the duplicate urls\n",
    "    #//*** We may need to incorporate duplicates with different categories...later\n",
    "    if len(cache_df) > 0:\n",
    "        urls_to_dl,dupe_urls = keep_original_urls(urls,cache_df)\n",
    "    else:\n",
    "        #//*** There is no cache initialize and move one\n",
    "        urls_to_dl = urls\n",
    "        dupe_urls = []\n",
    "    print(\"=====================\")\n",
    "    print(f\"Category: {cat}\")\n",
    "    print(\"=====================\")\n",
    "    print(f\"DL Items: {len(urls_to_dl)} - Duplicate: {len(dupe_urls)}\")\n",
    "    \n",
    "    if len(urls_to_dl) == 0:\n",
    "        print(\"No New Items to Download!\")\n",
    "    \n",
    "    if len(urls_to_dl) > 0:\n",
    "        scripts = get_scripts(urls_to_dl)\n",
    "        \n",
    "        #//*** Convert Scripts to DataFrame\n",
    "        df = pd.DataFrame()\n",
    "        #df['urls'] = urls_to_dl\n",
    "\n",
    "        s = {}\n",
    "\n",
    "        #//*** Get each script value as a list\n",
    "        for script in scripts:\n",
    "\n",
    "            for key,value in script.items():\n",
    "                if key not in s.keys():\n",
    "                    s[key] = []\n",
    "                s[key].append(value)\n",
    "\n",
    "        #//*** build Urls field\n",
    "        #//*** Use the Story URL in case an url was skipped.\n",
    "        df['urls'] = s['url']\n",
    "        df['hash'] = pd.util.hash_pandas_object(df['urls'])\n",
    "        df['cat'] = cat\n",
    "\n",
    "        #//*** Add lists of script values as columns\n",
    "        for key,value in s.items():\n",
    "            df[key] = value\n",
    "\n",
    "        #//*** Combine cache_df and df\n",
    "        cache_df = pd.concat([cache_df,df])\n",
    "\n",
    "\n",
    "#//*** Everything is Gathered. Write Cache_df to disk\n",
    "#//*** Write DF to file\n",
    "pd.to_pickle(cache_df,cache_filepath)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4638fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(last_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6375ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_df['time_date'] = cache_df['time_date'].dt.date\n",
    "print(\"Length Before de-dupe:\",len(cache_df)\n",
    "cache_df = cache_df.drop_duplicates(subset=\"url\")\n",
    "      \n",
    "print(\"Length Before de-dupe:\",len(cache_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Write to excel file\n",
    "output_filename = f\"{tgt_year}_{quarter}_Collected_Stories.xlsx\"\n",
    "print(output_filename)\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter(output_filename, engine='xlsxwriter')\n",
    "\n",
    "#//*** Build Sheet and Data from From each url_item element\n",
    "for url_item in url_items:\n",
    "    sheet = url_item['cat']\n",
    "    \n",
    "    tdf = cache_df[cache_df['cat'] == sheet].copy()\n",
    "    \n",
    "    tdf = tdf[['urls','time_date','time_text','headline','body_text']]\n",
    "    \n",
    "    tdf.to_excel(writer,sheet_name=sheet)\n",
    "    \n",
    "try:\n",
    "    writer.save()    \n",
    "except:\n",
    "    print(\"Trouble Saving the Spreadsheet. It's Probably Open Somewhere. Close it and Retry\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9658c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76957a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_df.drop_duplicates(subset=\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25667394",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_items = [\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/climate-change/\",\n",
    "        \"cat\" : \"Climate\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/environment/\",\n",
    "        \"cat\" : \"Environment\"\n",
    "    },\n",
    " \n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/economy/\",\n",
    "        \"cat\" : \"Economy\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/jobs/\",\n",
    "        \"cat\" : \"Jobs\"\n",
    "    },    \n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/safety/\",\n",
    "        \"cat\" : \"Safety\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/health/\",\n",
    "        \"cat\" : \"Health\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/education/\",\n",
    "        \"cat\" : \"Education\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/race-and-culture/\",\n",
    "        \"cat\" : \"Race\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/discrimination/\",\n",
    "        \"cat\" : \"Discrimination\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/civil-rights/\",\n",
    "        \"cat\" : \"Civil Rights\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/building-a-better-bay-area/\",\n",
    "        \"cat\" : \"BABBA\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/7onyourside/\",\n",
    "        \"cat\" : \"7OYS\"\n",
    "    },\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/tag/covid-19/\",\n",
    "        \"cat\" : \"COVID\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"url\" : \"https://abc7news.com/iteam/\",\n",
    "        \"cat\" : \"I-TEAM\"\n",
    "    },\n",
    "    \n",
    "\n",
    "    \n",
    "]\n",
    "\n",
    "cache_df['time_date'] = cache_df['time_date'].dt.date\n",
    "\n",
    "#//*** Write to excel file\n",
    "output_filename = f\"{tgt_year}_{quarter}_Collected_Stories.xlsx\"\n",
    "print(output_filename)\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter(output_filename, engine='xlsxwriter')\n",
    "\n",
    "#//*** Build Sheet and Data from From each url_item element\n",
    "for url_item in url_items:\n",
    "    sheet = url_item['cat']\n",
    "    \n",
    "    tdf = cache_df[cache_df['cat'] == sheet].copy()\n",
    "    \n",
    "    tdf = tdf[['urls','time_date','time_text','headline','body_text']]\n",
    "    \n",
    "    tdf.to_excel(writer,sheet_name=sheet)\n",
    "    \n",
    "try:\n",
    "    writer.save()    \n",
    "except:\n",
    "    print(\"Trouble Saving the Spreadsheet. It's Probably Open Somewhere. Close it and Retry\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98baa66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f513df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8228ddfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
