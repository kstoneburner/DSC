{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stoneburner, Kurt\n",
    "- ## DSC 550 - Week 09/10\n",
    "\n",
    "Due the to complex nature of this assignment I've broken the assignment into three notebooks. Assignment 3 would only run on Intel PCs I have an AMD processor. I managed to address this issue by connectly remotely to a work PC and doing the assignment there. This first assignment was a real resource hog. I tried to apply some learnings from Keras to improve the model accurracy but I simply didn't have the time for proper trial and error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Neural Network Classifier with Scikit ### \n",
    "\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using scikit-learn. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//***************************************\n",
    "#//*** Apply Common Cleanup operations\n",
    "#//***************************************\n",
    "#//*** These cleanup functions are based on Week 02 cleanup code, and rebuilt for Week 04\n",
    "\n",
    "#//*****************************************\n",
    "#//*** Functions:\n",
    "#//*****************************************\n",
    "#//*** Mr_clean_text: Converts to lowercase, removes punctuation, newlines and html markup\n",
    "#//****************************************************************************************************\n",
    "#//*** Tokenize_series: Converts a Series containing strings, to a series containing tokenized lists\n",
    "#//****************************************************************************************************\n",
    "#//*** Remove_stop_words: Removes Stop words based on nltk stopwords 'english' dictionary\n",
    "#//****************************************************************************************************\n",
    "#//*** Apply_stemmer: Stem tokenized words using nltk.stem.porter.PorterStemme\n",
    "#//****************************************************************************************************\n",
    "#//*** apply_pos_tag: Builds Part of Speech Tagging from tokeninzed text\n",
    "#//****************************************************************************************************\n",
    "\n",
    "#//****************************************************************************************************\n",
    "\n",
    "#//****************************************************************************************************\n",
    "#//*** Key values will default to true. If code needs to be defaulted to False, a default_false list can be added later\n",
    "#//*** All Boolean kwarg keya are stored in kwarg list. This speeds up the coding of the action_dict.\n",
    "#//*** As Kwargs are added \n",
    "def mr_clean_text(input_series, input_options={}):\n",
    "    \n",
    "    def clean_text(input_string):\n",
    "        clean1 = re.sub(r'['+string.punctuation + '’—”'+']', \"\", input_string.lower())\n",
    "        return re.sub(r'\\W+', ' ', clean1)\n",
    "\n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "    #//*** Add some data validation. I'm preparing this function for additional use. I'm checking if future users (ie future me)\n",
    "    #//*** may throw some garbage at this function. Experience has taught me to fail safely wherever possible.\n",
    "\n",
    "    #//*** All kwargs are listed here. These initialize TRUE by default.\n",
    "    key_list = [ \"lower\", \"newline\", \"html\", \"punctuation\" ]\n",
    "    \n",
    "    default_false = [\"remove_empty\"]\n",
    "    \n",
    "    #//*** Build Action Dictionary\n",
    "    action_dict = { } \n",
    "    \n",
    "    #//*** Build the keys from kwarg_list and default them to TRUE\n",
    "    for key in key_list:\n",
    "        action_dict[key] = True\n",
    "    \n",
    "    for key in default_false:\n",
    "        action_dict[key] = False\n",
    "        \n",
    "    #//*** Loop through the input kwargs (if any). Assign the action_dict values based on the kwargs:\n",
    "    for key,value in input_options.items():\n",
    "        print(key,value)\n",
    "        action_dict[key] = value\n",
    "    \n",
    "    \n",
    "    #//*************************************************************************\n",
    "    #//*** The Cleanup/Processing code is a straight lift from DSC550 - Week02\n",
    "    #//*************************************************************************\n",
    "    #//*** Convert to Lower Case, Default to True\n",
    "    if action_dict[\"lower\"]:\n",
    "        input_series = input_series.str.lower()\n",
    "    \n",
    "   \n",
    "    #//*** Remove New Lines\n",
    "    if action_dict[\"newline\"]:\n",
    "        #//*** Rmove \\r\\n\n",
    "        input_series = input_series.str.replace('\\r?\\n',\"\")\n",
    "\n",
    "        #//*** Remove \\n new lines\n",
    "        input_series = input_series.str.replace('\\n',\"\")\n",
    "    \n",
    "    \n",
    "    input_series = input_series.str.replace(\"\\\\(http.+\\\\)\",\"\")\n",
    "    \n",
    "    #//*** Print Elements between brackets\n",
    "    #print(input_series[ input_series == input_series.str.match('[.*]')])\n",
    "\n",
    "     \n",
    "    #//*** Remove html entities, observed entities are &gt; and &lt;. All HTML entities begin with & and end with ;.\n",
    "    #//*** Let's use regex to remove html entities\n",
    "    if action_dict[\"html\"]:\n",
    "        input_series = input_series.str.replace(r'&.*;',\"\")\n",
    "\n",
    "    #//*** Remove the empty lines\n",
    "    if action_dict[\"remove_empty\"]:\n",
    "        input_series = input_series[ input_series.str.len() > 0]\n",
    "\n",
    "    #//*** Remove punctuation\n",
    "    if action_dict[\"punctuation\"]:\n",
    "        #//*** Load libraries for punctuation if not already loaded.\n",
    "        #//*** Wrapping these in a try, no sense in importing libraries that already exist.\n",
    "        #//*** Unsure of the cost of reimporting libraries (if any). But testing if library is already loaded feels\n",
    "        #//*** like a good practice\n",
    "\n",
    "        #input_series = input_series.apply(lambda x: clean_text(x))\n",
    "\n",
    "        try:\n",
    "            type(sys)\n",
    "        except:\n",
    "            import sys\n",
    "\n",
    "        try:\n",
    "            type(unicodedata)\n",
    "        except:\n",
    "            import unicodedata\n",
    "\n",
    "        #//*** replace Comma and Period with a space.\n",
    "        for punct in [\",\",\".\"]:\n",
    "            input_series = input_series.str.replace(punct,\" \")\n",
    "\n",
    "        #//*** Remove punctuation using the example from the book\n",
    "        punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P') )\n",
    "        input_series = input_series.str.translate(punctuation)\n",
    "\n",
    "        #table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}\n",
    "        #print(table )\n",
    "        #input_series = input_series.str.translate(table)\n",
    "\n",
    "    print(f\"Text Cleaning Time: {time.time() - start_time}\")\n",
    "\n",
    "    return input_series\n",
    "\n",
    "                                          \n",
    "#//*** Tokenize a Series containing Strings.\n",
    "#//*** Breaking this out into it's own function for later reuse.\n",
    "#//*** Not a lot of code here, but it helps to keep the libraries localized. This creates standarization for future\n",
    "#//*** Stoneburner projects. Also has the ability to add functionality as needed.\n",
    "\n",
    "def tokenize_series(input_series,slices=20,input_options={}):\n",
    "    \n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "    \n",
    "    word_tokenize = nltk.tokenize.word_tokenize \n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "        \n",
    "    #//*** All kwargs are listed here. These initialize False by default.\n",
    "    key_list = [ \"fast\", \"quiet\" ]\n",
    "    \n",
    "    #//*** Build Action Dictionary\n",
    "    action_dict = { } \n",
    "    \n",
    "    #//*** Build the keys from kwarg_list and default them to False\n",
    "    for key in key_list:\n",
    "        action_dict[key] = False\n",
    "        \n",
    "    #//*** Loop through the input kwargs (if any). Assign the action_dict values based on the kwargs:\n",
    "    for key,value in input_options.items():\n",
    "        print(key,value)\n",
    "        action_dict[key] = value\n",
    "    \n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "            \n",
    "    #input_series = input_series.apply(word_tokenize)\n",
    "    \n",
    "    if action_dict['fast'] == False:\n",
    "        print(\"Processing Tokens with NLTK Word Tokenize\")\n",
    "        input_series = apply_with_progress(input_series,word_tokenize,slices)\n",
    "    else:\n",
    "        print(\"Process Tokens with Split()\")\n",
    "        input_series = apply_with_progress(input_series,lambda x: x.split(),slices)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"Tokenize Time: {time.time() - start_time}\")\n",
    "    \n",
    "    return input_series\n",
    "\n",
    "#//*** Remove Stop words from the input list\n",
    "def remove_stop_words(input_series):\n",
    "    \n",
    "    #//*** This function removes stop_words from a series.\n",
    "    #//*** Works with series.apply()\n",
    "    def apply_stop_words(input_list):\n",
    "\n",
    "        #//*** Load Stopwords   \n",
    "        for word in input_list:\n",
    "            if word in stop_words:\n",
    "                input_list.remove(word)\n",
    "                #print(f\"Removing: {word}\")\n",
    "        return input_list\n",
    "\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "        \n",
    "    stopwords = nltk.corpus.stopwords\n",
    "\n",
    "    #//*** Stopwords requires an additional download\n",
    "    try:\n",
    "        type(stopwords)\n",
    "    except:\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "\n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "\n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    #//*** The stop_words include punctuation. Stop Word Contractions will not be filtered out.\n",
    "    #//*** Manually adding word the\n",
    "    stop_words = []\n",
    "    \n",
    "    #//*** Remove apostrophies from the stop_words\n",
    "    for stop in stopwords.words('english'):\n",
    "        stop_words.append(stop.replace(\"'\",\"\"))\n",
    "\n",
    "    #print(\"Stop Words: \")\n",
    "    print(stop_words)\n",
    "    print (\"Processing Stop Words\")\n",
    "    input_series = apply_with_progress(input_series, apply_stop_words)\n",
    "    \n",
    "    print(f\"Stop Words Time: {time.time() - start_time}\")\n",
    "    \n",
    "    return input_series\n",
    "\n",
    "def apply_stemmer(input_series,trim_single_words = True,slices=100):\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "\n",
    "    #//*** Instantiate the Stemmer\n",
    "    porter = nltk.stem.porter.PorterStemmer()\n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    #//*** 1.) Apply() an action to each row\n",
    "    #//*** 2.) lambda word_list, each row is treated as word_list for the subsequent expression\n",
    "    #//*** 3.) The base [ word for word in wordlist] would return each word in word_list as a list. \n",
    "    #//*** 4.) [porter.stem(word) for word in word_list] - performs stemming on each word and returns a list\n",
    "    #input_series = input_series.apply(lambda word_list: [porter.stem(word) for word in word_list] )\n",
    "    print(\"Begin: Apply Stemmer\")\n",
    "    input_series = apply_with_progress(input_series, lambda word_list: [porter.stem(word) for word in word_list],slices)\n",
    "    \n",
    "    #//*** Remove Single letter words after stemming\n",
    "    \n",
    "    \"\"\"\n",
    "    if trim_single_words:\n",
    "        for word_list in input_series:\n",
    "            for word in word_list:\n",
    "                if len(word) < 2:\n",
    "                    word_list.remove(word)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Apply Stemmer Time: {time.time() - start_time}\")\n",
    "    return input_series\n",
    "\n",
    "def apply_pos_tag(input_series,slices=100):\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "\n",
    "    from nltk import pos_tag\n",
    "\n",
    "    #//pos_tag requires an additional download\n",
    "    try:\n",
    "        pos_tag([\"the\",\"quick\",\"brown\",\"fox\"])\n",
    "    except: \n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    print(\"Begin Part of Speech tagging\")\n",
    "    \n",
    "    input_series = apply_with_progress(input_series,pos_tag,slices)\n",
    "    \n",
    "    print(f\"Part of Speech Tagging Time: {round(time.time() - start_time,2)}s\")\n",
    "    \n",
    "    return input_series\n",
    "    \n",
    "def apply_lemmatization(input_series,slices=20):\n",
    "            \n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    from nltk.corpus import wordnet    \n",
    "    \n",
    "    #nltk.download('wordnet')\n",
    "    \n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    # Initialize the Lemmatizer instance\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    \n",
    "    #//*** 1.) Apply() an action to each row\n",
    "    #//*** 2.) lambda word_list, each row is treated as word_list for the subsequent expression\n",
    "    #//*** 3.) The base [ word for word in wordlist] would return each word in word_list as a list. \n",
    "    #//*** 4.) [lemmatizer.lemmatize(word) for word in word_list] - performs lemmtization on each word and returns a list\n",
    "    #lemmatized = input_series.apply(lambda word_list: [lemmatizer.lemmatize(*word) for word in word_list] )\n",
    "    \n",
    "    print(\"Begin Lemmatization...\")\n",
    "    \n",
    "    input_series = apply_with_progress(input_series,lambda word_list: [lemmatizer.lemmatize(word) for word in word_list],20)\n",
    "    \n",
    "    print(f\"Lemmatization Time: {time.time() - start_time}\")\n",
    "    \n",
    "    #if detoken:\n",
    "    #    return tokenize_series(input_series,5,{\"fast\":True})\n",
    "\n",
    "    return input_series\n",
    "\n",
    "#//*** Apply a function to a Series and display processing progress\n",
    "#//*** Slices is the total number of intervals to report progress.\n",
    "#//*** Slices = 20 displays processing after every 5% is processed\n",
    "#//*** Slices = 100 displays processing after every 1% is processed\n",
    "def apply_with_progress(input_series,input_function,slices=20):\n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Get the time at the start of the loop, used for elapsed time.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** The interval is the number of elements to process in each Loop. The default is 20.\n",
    "    #//*** Which displays results at 5% intervals.\n",
    "    interval = int(len(input_series)/slices)\n",
    "    \n",
    "    #//*** Total number of items to process\n",
    "    total = len(input_series)\n",
    "    \n",
    "\n",
    "    #//*** Loop through slice times and display processing statistics for each slice.\n",
    "    for x in range(0, slices ):\n",
    "        #//*** Get time at the start of the slice.\n",
    "        loop_time = time.time()\n",
    "        \n",
    "        #//*** Set the start index\n",
    "        begin_dex = interval*x\n",
    "        \n",
    "        #//*** Set the end index\n",
    "        end_dex = interval*x+interval-1\n",
    "        \n",
    "        #//*** Apply the input function to a slice of the input_series\n",
    "        #//*** This part does all the actual 'work'\n",
    "        input_series[begin_dex:end_dex] = input_series[begin_dex:end_dex].apply(input_function)\n",
    "        \n",
    "        #//*** Get the time after the slice of work is done\n",
    "        now = time.time()\n",
    "        \n",
    "        #//*** Compute the estimated remaining time\n",
    "        #//*** Total elapsed time / % of completed slices = Estimated total time\n",
    "        #//*** Estimated total time - elaped time = Remaining time\n",
    "        est_remain = round( ( ( now - start_time ) /  ( (x+1)/slices ) - (now-start_time)),2)\n",
    "\n",
    "        #//*** Display Results so we know how much time is left (so we can effectively multi-task: ie comments, research and Valheim)\n",
    "        print(f\"Processed {x}/{slices}: {begin_dex}:{end_dex} [{total}] in {round(now-loop_time,2)}s elapsed: {round(now-start_time,2)}s est Remain: {est_remain}s\")\n",
    "    \n",
    "    #//*** END For Slice Loop\n",
    "    \n",
    "    #//*** Process the remaining values (Since interval is an int there should be a remainder)\n",
    "    loop_time = time.time()\n",
    "    begin_dex = end_dex+1\n",
    "    if begin_dex < len(input_series):\n",
    "        print(f\"Processing Remaining values: {begin_dex} : {total} \")\n",
    "        #print(input_series[begin_dex:])\n",
    "        input_series[begin_dex:] = input_series[begin_dex:].apply(input_function)\n",
    "    \n",
    "    #//*** Display Final output\n",
    "    print(f\"Processed {slices}/{slices}: {begin_dex}:{end_dex} [{total}] in {round(time.time()-loop_time,2)}s elapsed: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "    #//*** return Series\n",
    "    return input_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#//******************************************************************\n",
    "#//*** Process categorized-comments.jsonl and create a local copy\n",
    "#//******************************************************************\n",
    "#//*** Processed comments are stored in a local pickled file. \n",
    "#//*** Only run processing if a local copy if needed. Local builds are required since github has a 100 mb file size limit.\n",
    "#//*** If it's too annoying I should manually split the dataframe into multiple files, or see if pickle supports multi-part zip files.\n",
    "#//********************************************************************************************************************************************\n",
    "\n",
    "#//*** Runs when set to True\n",
    "if False:\n",
    "    \n",
    "    #//*** Read the File\n",
    "    df = pd.read_json(\"z_wk09_categorized-comments.jsonl\", lines=True)\n",
    "\n",
    "    #//*** Clean the Text.\n",
    "    df['processed'] = mr_clean_text(df['txt'],{\"lower\": True, \"newline\": True, \"html\": True, \"remove_empty\" : False, \"punctuation\" : True})\n",
    "\n",
    "    print(f\"Articles of length with 0 characters: {len(df[ df['processed'].str.len() == 0 ])}\")\n",
    "\n",
    "    #//Remove Items with an Arbitrary length of 0\n",
    "\n",
    "    print(f\"Articles of length with 0 characters: {len(df[ df['processed'].str.len() == 0 ])}\")\n",
    "    print(\"Remove these articles\")\n",
    "    print(f\"Article Count Before: {len(df)}\")\n",
    "    df = df[ df['processed'].str.len() > 0 ]\n",
    "    print(f\"Article Count After: {len(df)}\")\n",
    "\n",
    "    #//************************\n",
    "    #//*** Tokenize the Text\n",
    "    #//************************\n",
    "    #//*** The custom function displays progress while it's working\n",
    "    df['processed'] = tokenize_series(df['processed'],20,{\"fast\":True})\n",
    "\n",
    "    #//************************\n",
    "    #//*** Remove Stop Words\n",
    "    #//************************\n",
    "    #//*** The custom function displays progress while it's working\n",
    "    df['processed'] = remove_stop_words(df['processed'])\n",
    "\n",
    "    #//************************\n",
    "    #//*** Apply Lematization\n",
    "    #//************************\n",
    "    df['lema_stem_tokens'] = apply_lemmatization(df['processed']) \n",
    "\n",
    "    print( f\"Total Corpus Word Count: {df['lema_stem_tokens'].apply(lambda x: len(x)).sum()}\" )\n",
    "    #//*** Eliminate words with length of 0,1 or 2. This is an arbitrary value to help with feature reduction\n",
    "    #df['tokens'] = df['tokens'].apply(lambda word_list : list(filter(lambda word : len(word) >= 3, word_list))) \n",
    "    df['lema_stem_tokens'] = df['lema_stem_tokens'].apply(lambda word_list : list(filter(lambda word : len(word) >= 3, word_list))) \n",
    "\n",
    "    #//*** Build Word Count\n",
    "    df['num_wds'] = df['lema_stem_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "    print( f\"Total Corpus Word Count: {df['lema_stem_tokens'].apply(lambda x: len(x)).sum()}\" )\n",
    "\n",
    "    #//************************\n",
    "    #//*** Stem the lemma's\n",
    "    #//***********************************************************************\n",
    "    #//*** We are trying to reduce the feature set\n",
    "    #//***********************************************************************\n",
    "    df['lema_stem_tokens'] = apply_stemmer(df['lema_stem_tokens'])\n",
    "\n",
    "    #//**********************************\n",
    "    #//*** Apply Part of Speech Tagging\n",
    "    #//**********************************\n",
    "    df['pos_tag'] = apply_pos_tag(df['lema_stem_tokens'])\n",
    "    \n",
    "    #//**********************************\n",
    "    #//*** Save the Dataframe to file\n",
    "    #//**********************************\n",
    "    pd.to_pickle(df,\"z_wk09_categorized_comments_processed.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//**** Load the processed Text\n",
    "df = pd.read_pickle(\"z_wk09_categorized_comments_processed.zip\")\n",
    "\n",
    "#//*** Convert categorical string to categorical int\n",
    "#//*** Only run once to prevent iPython issues\n",
    "if (df.dtypes['cat'] == object):\n",
    "    cat_dict = dict(tuple(enumerate(df['cat'].unique())))\n",
    "    #//*** Build sexcat Categorical column\n",
    "    df['intcat'] = df['cat'].copy()\n",
    "    \n",
    "    #//*** replace values using the sex_dict dictionary\n",
    "    for key,value in cat_dict.items():\n",
    "        df['intcat'] = df['intcat'].replace(value,key)\n",
    "\n",
    "#//*** Stingify the tokeninze Lema Stems\n",
    "df['processed'] = df['lema_stem_tokens'].apply(lambda word_list: ' '.join(word_list)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                cat                                                txt  \\\n",
      "0            sports  Barely better than Gabbert? He was significant...   \n",
      "1            sports  Fuck the ducks and the Angels! But welcome to ...   \n",
      "2            sports  Should have drafted more WRs.\\n\\n- Matt Millen...   \n",
      "3            sports            [Done](https://i.imgur.com/2YZ90pm.jpg)   \n",
      "4            sports                                      No!! NOO!!!!!   \n",
      "...             ...                                                ...   \n",
      "606471  video_games             No. It's probably only happened to you   \n",
      "606472  video_games  I think most of the disappointment came from t...   \n",
      "606473  video_games  dishonored 1/2 looked like arse, so what the h...   \n",
      "606474  video_games                                          [removed]   \n",
      "606475  video_games  I wish more games provided options like Rise o...   \n",
      "\n",
      "                                                processed  \\\n",
      "0       bare better gabbert significantli better year ...   \n",
      "1            fuck duck the angel welcom all new niner fan   \n",
      "2                       have draft wr matt millen probabl   \n",
      "3                                                    done   \n",
      "4                                                     noo   \n",
      "...                                                   ...   \n",
      "606471                                 probabl happen you   \n",
      "606472  think disappoint came delay the ps+ version re...   \n",
      "606473  dishonor look like ars what hell they sacrif f...   \n",
      "606474                                              remov   \n",
      "606475  wish game provid option like rise the tomb rai...   \n",
      "\n",
      "                                         lema_stem_tokens  num_wds  \\\n",
      "0       [bare, better, gabbert, significantli, better,...       56   \n",
      "1       [fuck, duck, the, angel, welcom, all, new, nin...        9   \n",
      "2                [have, draft, wr, matt, millen, probabl]        6   \n",
      "3                                                  [done]        1   \n",
      "4                                                   [noo]        1   \n",
      "...                                                   ...      ...   \n",
      "606471                             [probabl, happen, you]        3   \n",
      "606472  [think, disappoint, came, delay, the, ps+, ver...       34   \n",
      "606473  [dishonor, look, like, ars, what, hell, they, ...       14   \n",
      "606474                                            [remov]        1   \n",
      "606475  [wish, game, provid, option, like, rise, the, ...       13   \n",
      "\n",
      "                                                  pos_tag  intcat  \n",
      "0       [(bare, NN), (better, RBR), (gabbert, NN), (si...       0  \n",
      "1       [(fuck, JJ), (duck, VBD), (the, DT), (angel, N...       0  \n",
      "2       [(have, VB), (draft, NN), (wr, NN), (matt, NN)...       0  \n",
      "3                                           [(done, VBN)]       0  \n",
      "4                                             [(noo, NN)]       0  \n",
      "...                                                   ...     ...  \n",
      "606471          [(probabl, NN), (happen, VB), (you, PRP)]       2  \n",
      "606472  [(think, VB), (disappoint, NN), (came, VBD), (...       2  \n",
      "606473  [(dishonor, JJ), (look, NN), (like, IN), (ars,...       2  \n",
      "606474                                      [(remov, NN)]       2  \n",
      "606475  [(wish, JJ), (game, NN), (provid, NN), (option...       2  \n",
      "\n",
      "[606008 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge transformers \n",
    "\n",
    "#from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*****************************************************************************************\n",
    "#//**** Lots of reference articles, keep them here for well, later reference\n",
    "#//*****************************************************************************************\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "#https://analyticsindiamag.com/a-beginners-guide-to-scikit-learns-mlpclassifier/\n",
    "#https://duckduckgo.com/?q=sklearn+cross_val_score+&t=ffab&ia=web\n",
    "#https://towardsdatascience.com/sentiment-analysis-using-classification-e73da5b4159f\n",
    "#//*****************************************************************************************\n",
    "\n",
    "#//**** Load Sci Kit Learn Libraries. Importing here makes the code easier to export for future learnings\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First pass Vectorizing\n",
      "14935\n",
      "Re-Vectorizing: max_features=1493 [14935*0.1] Data Size: 12120\n",
      "Iteration 1, loss = 0.65907100\n",
      "Iteration 2, loss = 0.46079605\n",
      "Iteration 3, loss = 0.31828674\n",
      "Iteration 4, loss = 0.19361177\n",
      "Iteration 5, loss = 0.13973386\n",
      "Iteration 6, loss = 0.12476828\n",
      "Iteration 7, loss = 0.11730836\n",
      "Iteration 8, loss = 0.11576925\n",
      "Iteration 9, loss = 0.11385224\n",
      "Iteration 10, loss = 0.11113335\n",
      "Iteration 11, loss = 0.10910524\n",
      "Iteration 12, loss = 0.10729068\n",
      "Iteration 13, loss = 0.10597469\n",
      "Iteration 14, loss = 0.10587764\n",
      "Iteration 15, loss = 0.10504469\n",
      "Iteration 16, loss = 0.10428418\n",
      "Iteration 17, loss = 0.10214203\n",
      "Iteration 18, loss = 0.10297004\n",
      "Iteration 19, loss = 0.10307815\n",
      "Iteration 20, loss = 0.10121714\n",
      "Iteration 21, loss = 0.10014710\n",
      "Iteration 22, loss = 0.10051258\n",
      "Iteration 23, loss = 0.10018764\n",
      "Iteration 24, loss = 0.10086452\n",
      "Iteration 25, loss = 0.09807039\n",
      "Iteration 26, loss = 0.09982980\n",
      "Iteration 27, loss = 0.09834192\n",
      "Iteration 28, loss = 0.09760594\n",
      "Iteration 29, loss = 0.09738191\n",
      "Iteration 30, loss = 0.09706936\n",
      "Iteration 31, loss = 0.09693396\n",
      "Iteration 32, loss = 0.09674471\n",
      "Iteration 33, loss = 0.09688760\n",
      "Iteration 34, loss = 0.09685044\n",
      "Iteration 35, loss = 0.09603804\n",
      "Iteration 36, loss = 0.09633951\n",
      "Iteration 37, loss = 0.09637580\n",
      "Iteration 38, loss = 0.09626704\n",
      "Iteration 39, loss = 0.09622372\n",
      "Iteration 40, loss = 0.09602419\n",
      "Iteration 41, loss = 0.09697162\n",
      "Iteration 42, loss = 0.09605908\n",
      "Iteration 43, loss = 0.09645468\n",
      "Iteration 44, loss = 0.09591005\n",
      "Iteration 45, loss = 0.09525198\n",
      "Iteration 46, loss = 0.09540248\n",
      "Iteration 47, loss = 0.09549211\n",
      "Iteration 48, loss = 0.09503644\n",
      "Iteration 49, loss = 0.09593532\n",
      "Iteration 50, loss = 0.09506964\n",
      "Iteration 51, loss = 0.09531820\n",
      "Iteration 52, loss = 0.09563403\n",
      "Iteration 53, loss = 0.09501426\n",
      "Iteration 54, loss = 0.09527988\n",
      "Iteration 55, loss = 0.09535397\n",
      "Iteration 56, loss = 0.09553109\n",
      "Iteration 57, loss = 0.09466851\n",
      "Iteration 58, loss = 0.09541666\n",
      "Iteration 59, loss = 0.09477839\n",
      "Iteration 60, loss = 0.09546680\n",
      "Iteration 61, loss = 0.09541127\n",
      "Iteration 62, loss = 0.13924621\n",
      "Iteration 63, loss = 0.11715223\n",
      "Iteration 64, loss = 0.10270070\n",
      "Iteration 65, loss = 0.09662625\n",
      "Iteration 66, loss = 0.09654546\n",
      "Iteration 67, loss = 0.09560312\n",
      "Iteration 68, loss = 0.09545946\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Modeling Time: 745.17s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "#//*** X is Post Processed Data to evaluate\n",
    "data_model_x = df['processed']\n",
    "\n",
    "\n",
    "#//*** Categorical Integers are the target\n",
    "data_model_y = df['intcat']\n",
    "\n",
    "\n",
    "#//*** There 600,000 ish records. Training on more than 2% of the data, really brings the system to it's knees. In the interest of time and sci kit is kind slow.\n",
    "#//*** I'm sticking with 2% training size\n",
    "test_size=.98\n",
    "#continuous scoring model\n",
    "#scoring = 'r2_score'\n",
    "\n",
    "# split the data randomly into test/train sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_model_x, data_model_y, test_size =test_size)\n",
    "\n",
    "#categorical scoring model\n",
    "#scoring = 'f1'\n",
    "\n",
    "#print(f\"Calculating Model: {len(x_train)} elements\")\n",
    "\n",
    "#//*** Initialize the Vectorizer, get all the features\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "print(\"First pass Vectorizing\")\n",
    "total_features = tfidf.fit_transform(x_train).shape[1]\n",
    "\n",
    "print(total_features)\n",
    "\n",
    "FEATURE_PERCENT = .1\n",
    "SECOND_LAYER_SIZE = .5\n",
    "\n",
    "N_FEATURES = int(total_features * FEATURE_PERCENT)\n",
    "\n",
    "print(f\"Re-Vectorizing: max_features={N_FEATURES} [{total_features}*{FEATURE_PERCENT}] Data Size: {len(x_train)}\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=N_FEATURES)\n",
    "\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)\n",
    "\n",
    "\n",
    "data_model_y = df['intcat']\n",
    "\n",
    "\n",
    "\n",
    "#//*** Define the classifier model\n",
    "classifier = Pipeline([\n",
    "    #'norm', TextNormalizer(),\n",
    "    #('tfidf',TfidfVectorizer()),\n",
    "    #('ann',MLPRegressor(hidden_layer_sizes=[500,150], verbose=True))\n",
    "    #('ann',MLPClassifier(hidden_layer_sizes=[500,150], verbose=True))\n",
    "    ('ann',MLPClassifier(hidden_layer_sizes=[N_FEATURES,int(N_FEATURES*SECOND_LAYER_SIZE)], verbose=True))\n",
    "])\n",
    "#//*** Start Timing the process\n",
    "start_time = time.time()\n",
    "\n",
    "#//*** Train the Model\n",
    "classifier.fit(x_train,y_train)\n",
    "\n",
    "print(f\"Modeling Time: {round(time.time()-start_time,2)}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Generate Predictions\n",
    "y_predicted = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 63865   1607  77317]\n",
      " [  2426   7557  14584]\n",
      " [ 42105   6052 378375]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.45      0.51    142789\n",
      "           1       0.50      0.31      0.38     24567\n",
      "           2       0.80      0.89      0.84    426532\n",
      "\n",
      "    accuracy                           0.76    593888\n",
      "   macro avg       0.63      0.55      0.58    593888\n",
      "weighted avg       0.74      0.76      0.74    593888\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, accuracy_score, recall_score, confusion_matrix,classification_report\n",
    "#//*** Score the Model\n",
    "\n",
    "print(confusion_matrix(y_test, y_predicted))\n",
    "print(classification_report(y_test, y_predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
