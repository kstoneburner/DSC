{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stoneburner, Kurt\n",
    "- ## DSC 550 - Week 09/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Neural Network Classifier with Scikit ### \n",
    "\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using scikit-learn. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//***************************************\n",
    "#//*** Apply Common Cleanup operations\n",
    "#//***************************************\n",
    "#//*** These cleanup functions are based on Week 02 cleanup code, and rebuilt for Week 04\n",
    "\n",
    "#//*****************************************\n",
    "#//*** Functions:\n",
    "#//*****************************************\n",
    "#//*** Mr_clean_text: Converts to lowercase, removes punctuation, newlines and html markup\n",
    "#//****************************************************************************************************\n",
    "#//*** Tokenize_series: Converts a Series containing strings, to a series containing tokenized lists\n",
    "#//****************************************************************************************************\n",
    "#//*** Remove_stop_words: Removes Stop words based on nltk stopwords 'english' dictionary\n",
    "#//****************************************************************************************************\n",
    "#//*** Apply_stemmer: Stem tokenized words using nltk.stem.porter.PorterStemme\n",
    "#//****************************************************************************************************\n",
    "#//*** apply_pos_tag: Builds Part of Speech Tagging from tokeninzed text\n",
    "#//****************************************************************************************************\n",
    "\n",
    "#//****************************************************************************************************\n",
    "\n",
    "#//****************************************************************************************************\n",
    "#//*** Key values will default to true. If code needs to be defaulted to False, a default_false list can be added later\n",
    "#//*** All Boolean kwarg keya are stored in kwarg list. This speeds up the coding of the action_dict.\n",
    "#//*** As Kwargs are added \n",
    "def mr_clean_text(input_series, input_options={}):\n",
    "    \n",
    "    def clean_text(input_string):\n",
    "        clean1 = re.sub(r'['+string.punctuation + '’—”'+']', \"\", input_string.lower())\n",
    "        return re.sub(r'\\W+', ' ', clean1)\n",
    "\n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "    #//*** Add some data validation. I'm preparing this function for additional use. I'm checking if future users (ie future me)\n",
    "    #//*** may throw some garbage at this function. Experience has taught me to fail safely wherever possible.\n",
    "\n",
    "    #//*** All kwargs are listed here. These initialize TRUE by default.\n",
    "    key_list = [ \"lower\", \"newline\", \"html\", \"punctuation\" ]\n",
    "    \n",
    "    default_false = [\"remove_empty\"]\n",
    "    \n",
    "    #//*** Build Action Dictionary\n",
    "    action_dict = { } \n",
    "    \n",
    "    #//*** Build the keys from kwarg_list and default them to TRUE\n",
    "    for key in key_list:\n",
    "        action_dict[key] = True\n",
    "    \n",
    "    for key in default_false:\n",
    "        action_dict[key] = False\n",
    "        \n",
    "    #//*** Loop through the input kwargs (if any). Assign the action_dict values based on the kwargs:\n",
    "    for key,value in input_options.items():\n",
    "        print(key,value)\n",
    "        action_dict[key] = value\n",
    "    \n",
    "    \n",
    "    #//*************************************************************************\n",
    "    #//*** The Cleanup/Processing code is a straight lift from DSC550 - Week02\n",
    "    #//*************************************************************************\n",
    "    #//*** Convert to Lower Case, Default to True\n",
    "    if action_dict[\"lower\"]:\n",
    "        input_series = input_series.str.lower()\n",
    "    \n",
    "   \n",
    "    #//*** Remove New Lines\n",
    "    if action_dict[\"newline\"]:\n",
    "        #//*** Rmove \\r\\n\n",
    "        input_series = input_series.str.replace('\\r?\\n',\"\")\n",
    "\n",
    "        #//*** Remove \\n new lines\n",
    "        input_series = input_series.str.replace('\\n',\"\")\n",
    "    \n",
    "    \n",
    "    input_series = input_series.str.replace(\"\\\\(http.+\\\\)\",\"\")\n",
    "    \n",
    "    #//*** Print Elements between brackets\n",
    "    #print(input_series[ input_series == input_series.str.match('[.*]')])\n",
    "\n",
    "     \n",
    "    #//*** Remove html entities, observed entities are &gt; and &lt;. All HTML entities begin with & and end with ;.\n",
    "    #//*** Let's use regex to remove html entities\n",
    "    if action_dict[\"html\"]:\n",
    "        input_series = input_series.str.replace(r'&.*;',\"\")\n",
    "\n",
    "    #//*** Remove the empty lines\n",
    "    if action_dict[\"remove_empty\"]:\n",
    "        input_series = input_series[ input_series.str.len() > 0]\n",
    "\n",
    "    #//*** Remove punctuation\n",
    "    if action_dict[\"punctuation\"]:\n",
    "        #//*** Load libraries for punctuation if not already loaded.\n",
    "        #//*** Wrapping these in a try, no sense in importing libraries that already exist.\n",
    "        #//*** Unsure of the cost of reimporting libraries (if any). But testing if library is already loaded feels\n",
    "        #//*** like a good practice\n",
    "\n",
    "        #input_series = input_series.apply(lambda x: clean_text(x))\n",
    "\n",
    "        try:\n",
    "            type(sys)\n",
    "        except:\n",
    "            import sys\n",
    "\n",
    "        try:\n",
    "            type(unicodedata)\n",
    "        except:\n",
    "            import unicodedata\n",
    "\n",
    "        #//*** replace Comma and Period with a space.\n",
    "        for punct in [\",\",\".\"]:\n",
    "            input_series = input_series.str.replace(punct,\" \")\n",
    "\n",
    "        #//*** Remove punctuation using the example from the book\n",
    "        punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P') )\n",
    "        input_series = input_series.str.translate(punctuation)\n",
    "\n",
    "        #table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}\n",
    "        #print(table )\n",
    "        #input_series = input_series.str.translate(table)\n",
    "\n",
    "    print(f\"Text Cleaning Time: {time.time() - start_time}\")\n",
    "\n",
    "    return input_series\n",
    "\n",
    "                                          \n",
    "#//*** Tokenize a Series containing Strings.\n",
    "#//*** Breaking this out into it's own function for later reuse.\n",
    "#//*** Not a lot of code here, but it helps to keep the libraries localized. This creates standarization for future\n",
    "#//*** Stoneburner projects. Also has the ability to add functionality as needed.\n",
    "\n",
    "def tokenize_series(input_series,slices=20,input_options={}):\n",
    "    \n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "    \n",
    "    word_tokenize = nltk.tokenize.word_tokenize \n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "        \n",
    "    #//*** All kwargs are listed here. These initialize False by default.\n",
    "    key_list = [ \"fast\", \"quiet\" ]\n",
    "    \n",
    "    #//*** Build Action Dictionary\n",
    "    action_dict = { } \n",
    "    \n",
    "    #//*** Build the keys from kwarg_list and default them to False\n",
    "    for key in key_list:\n",
    "        action_dict[key] = False\n",
    "        \n",
    "    #//*** Loop through the input kwargs (if any). Assign the action_dict values based on the kwargs:\n",
    "    for key,value in input_options.items():\n",
    "        print(key,value)\n",
    "        action_dict[key] = value\n",
    "    \n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "            \n",
    "    #input_series = input_series.apply(word_tokenize)\n",
    "    \n",
    "    if action_dict['fast'] == False:\n",
    "        print(\"Processing Tokens with NLTK Word Tokenize\")\n",
    "        input_series = apply_with_progress(input_series,word_tokenize,slices)\n",
    "    else:\n",
    "        print(\"Process Tokens with Split()\")\n",
    "        input_series = apply_with_progress(input_series,lambda x: x.split(),slices)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"Tokenize Time: {time.time() - start_time}\")\n",
    "    \n",
    "    return input_series\n",
    "\n",
    "#//*** Remove Stop words from the input list\n",
    "def remove_stop_words(input_series):\n",
    "    \n",
    "    #//*** This function removes stop_words from a series.\n",
    "    #//*** Works with series.apply()\n",
    "    def apply_stop_words(input_list):\n",
    "\n",
    "        #//*** Load Stopwords   \n",
    "        for word in input_list:\n",
    "            if word in stop_words:\n",
    "                input_list.remove(word)\n",
    "                #print(f\"Removing: {word}\")\n",
    "        return input_list\n",
    "\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "        \n",
    "    stopwords = nltk.corpus.stopwords\n",
    "\n",
    "    #//*** Stopwords requires an additional download\n",
    "    try:\n",
    "        type(stopwords)\n",
    "    except:\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "\n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "\n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    #//*** The stop_words include punctuation. Stop Word Contractions will not be filtered out.\n",
    "    #//*** Manually adding word the\n",
    "    stop_words = []\n",
    "    \n",
    "    #//*** Remove apostrophies from the stop_words\n",
    "    for stop in stopwords.words('english'):\n",
    "        stop_words.append(stop.replace(\"'\",\"\"))\n",
    "\n",
    "    #print(\"Stop Words: \")\n",
    "    print(stop_words)\n",
    "    print (\"Processing Stop Words\")\n",
    "    input_series = apply_with_progress(input_series, apply_stop_words)\n",
    "    \n",
    "    print(f\"Stop Words Time: {time.time() - start_time}\")\n",
    "    \n",
    "    return input_series\n",
    "\n",
    "def apply_stemmer(input_series,trim_single_words = True,slices=100):\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "\n",
    "    #//*** Instantiate the Stemmer\n",
    "    porter = nltk.stem.porter.PorterStemmer()\n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    #//*** 1.) Apply() an action to each row\n",
    "    #//*** 2.) lambda word_list, each row is treated as word_list for the subsequent expression\n",
    "    #//*** 3.) The base [ word for word in wordlist] would return each word in word_list as a list. \n",
    "    #//*** 4.) [porter.stem(word) for word in word_list] - performs stemming on each word and returns a list\n",
    "    #input_series = input_series.apply(lambda word_list: [porter.stem(word) for word in word_list] )\n",
    "    print(\"Begin: Apply Stemmer\")\n",
    "    input_series = apply_with_progress(input_series, lambda word_list: [porter.stem(word) for word in word_list],slices)\n",
    "    \n",
    "    #//*** Remove Single letter words after stemming\n",
    "    \n",
    "    \"\"\"\n",
    "    if trim_single_words:\n",
    "        for word_list in input_series:\n",
    "            for word in word_list:\n",
    "                if len(word) < 2:\n",
    "                    word_list.remove(word)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Apply Stemmer Time: {time.time() - start_time}\")\n",
    "    return input_series\n",
    "\n",
    "def apply_pos_tag(input_series,slices=100):\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "\n",
    "    from nltk import pos_tag\n",
    "\n",
    "    #//pos_tag requires an additional download\n",
    "    try:\n",
    "        pos_tag([\"the\",\"quick\",\"brown\",\"fox\"])\n",
    "    except: \n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    print(\"Begin Part of Speech tagging\")\n",
    "    \n",
    "    input_series = apply_with_progress(input_series,pos_tag,slices)\n",
    "    \n",
    "    print(f\"Part of Speech Tagging Time: {round(time.time() - start_time,2)}s\")\n",
    "    \n",
    "    return input_series\n",
    "    \n",
    "def apply_lemmatization(input_series,slices=20):\n",
    "            \n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    from nltk.corpus import wordnet    \n",
    "    \n",
    "    #nltk.download('wordnet')\n",
    "    \n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    # Initialize the Lemmatizer instance\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    \n",
    "    #//*** 1.) Apply() an action to each row\n",
    "    #//*** 2.) lambda word_list, each row is treated as word_list for the subsequent expression\n",
    "    #//*** 3.) The base [ word for word in wordlist] would return each word in word_list as a list. \n",
    "    #//*** 4.) [lemmatizer.lemmatize(word) for word in word_list] - performs lemmtization on each word and returns a list\n",
    "    #lemmatized = input_series.apply(lambda word_list: [lemmatizer.lemmatize(*word) for word in word_list] )\n",
    "    \n",
    "    print(\"Begin Lemmatization...\")\n",
    "    \n",
    "    input_series = apply_with_progress(input_series,lambda word_list: [lemmatizer.lemmatize(word) for word in word_list],20)\n",
    "    \n",
    "    print(f\"Lemmatization Time: {time.time() - start_time}\")\n",
    "    \n",
    "    #if detoken:\n",
    "    #    return tokenize_series(input_series,5,{\"fast\":True})\n",
    "\n",
    "    return input_series\n",
    "\n",
    "#//*** Apply a function to a Series and display processing progress\n",
    "#//*** Slices is the total number of intervals to report progress.\n",
    "#//*** Slices = 20 displays processing after every 5% is processed\n",
    "#//*** Slices = 100 displays processing after every 1% is processed\n",
    "def apply_with_progress(input_series,input_function,slices=20):\n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Get the time at the start of the loop, used for elapsed time.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** The interval is the number of elements to process in each Loop. The default is 20.\n",
    "    #//*** Which displays results at 5% intervals.\n",
    "    interval = int(len(input_series)/slices)\n",
    "    \n",
    "    #//*** Total number of items to process\n",
    "    total = len(input_series)\n",
    "    \n",
    "\n",
    "    #//*** Loop through slice times and display processing statistics for each slice.\n",
    "    for x in range(0, slices ):\n",
    "        #//*** Get time at the start of the slice.\n",
    "        loop_time = time.time()\n",
    "        \n",
    "        #//*** Set the start index\n",
    "        begin_dex = interval*x\n",
    "        \n",
    "        #//*** Set the end index\n",
    "        end_dex = interval*x+interval-1\n",
    "        \n",
    "        #//*** Apply the input function to a slice of the input_series\n",
    "        #//*** This part does all the actual 'work'\n",
    "        input_series[begin_dex:end_dex] = input_series[begin_dex:end_dex].apply(input_function)\n",
    "        \n",
    "        #//*** Get the time after the slice of work is done\n",
    "        now = time.time()\n",
    "        \n",
    "        #//*** Compute the estimated remaining time\n",
    "        #//*** Total elapsed time / % of completed slices = Estimated total time\n",
    "        #//*** Estimated total time - elaped time = Remaining time\n",
    "        est_remain = round( ( ( now - start_time ) /  ( (x+1)/slices ) - (now-start_time)),2)\n",
    "\n",
    "        #//*** Display Results so we know how much time is left (so we can effectively multi-task: ie comments, research and Valheim)\n",
    "        print(f\"Processed {x}/{slices}: {begin_dex}:{end_dex} [{total}] in {round(now-loop_time,2)}s elapsed: {round(now-start_time,2)}s est Remain: {est_remain}s\")\n",
    "    \n",
    "    #//*** END For Slice Loop\n",
    "    \n",
    "    #//*** Process the remaining values (Since interval is an int there should be a remainder)\n",
    "    loop_time = time.time()\n",
    "    begin_dex = end_dex+1\n",
    "    if begin_dex < len(input_series):\n",
    "        print(f\"Processing Remaining values: {begin_dex} : {total} \")\n",
    "        #print(input_series[begin_dex:])\n",
    "        input_series[begin_dex:] = input_series[begin_dex:].apply(input_function)\n",
    "    \n",
    "    #//*** Display Final output\n",
    "    print(f\"Processed {slices}/{slices}: {begin_dex}:{end_dex} [{total}] in {round(time.time()-loop_time,2)}s elapsed: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "    #//*** return Series\n",
    "    return input_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#//*** Process categorized-comments.jsonl\n",
    "#//*** Processed comments are stored in a local pickled file. \n",
    "#//*** Only run processing if a local copy if needed. Local builds are required since github has a 100 mb file size limit.\n",
    "#//*** If it's too annoying I should manually split the dataframe into multiple files, or see if pickle supports multi-part zip files.\n",
    "if False:\n",
    "    df = pd.read_json(\"z_wk09_categorized-comments.jsonl\", lines=True)\n",
    "\n",
    "    #//*** Clean the Text.\n",
    "    df['processed'] = mr_clean_text(df['txt'],{\"lower\": True, \"newline\": True, \"html\": True, \"remove_empty\" : False, \"punctuation\" : True})\n",
    "\n",
    "    print(f\"Articles of length with 0 characters: {len(df[ df['processed'].str.len() == 0 ])}\")\n",
    "\n",
    "    #//Remove Items with an Arbitrary length of 0\n",
    "\n",
    "    print(f\"Articles of length with 0 characters: {len(df[ df['processed'].str.len() == 0 ])}\")\n",
    "    print(\"Remove these articles\")\n",
    "    print(f\"Article Count Before: {len(df)}\")\n",
    "    df = df[ df['processed'].str.len() > 0 ]\n",
    "    print(f\"Article Count After: {len(df)}\")\n",
    "\n",
    "    #//************************\n",
    "    #//*** Tokenize the Text\n",
    "    #//************************\n",
    "    #//*** The custom function displays progress while it's working\n",
    "    df['processed'] = tokenize_series(df['processed'],20,{\"fast\":True})\n",
    "\n",
    "    #//************************\n",
    "    #//*** Remove Stop Words\n",
    "    #//************************\n",
    "    #//*** The custom function displays progress while it's working\n",
    "    df['processed'] = remove_stop_words(df['processed'])\n",
    "\n",
    "    #//************************\n",
    "    #//*** Apply Lematization\n",
    "    #//************************\n",
    "    df['lema_stem_tokens'] = apply_lemmatization(df['processed']) \n",
    "\n",
    "    print( f\"Total Corpus Word Count: {df['lema_stem_tokens'].apply(lambda x: len(x)).sum()}\" )\n",
    "    #//*** Eliminate words with length of 0,1 or 2. This is an arbitrary value to help with feature reduction\n",
    "    #df['tokens'] = df['tokens'].apply(lambda word_list : list(filter(lambda word : len(word) >= 3, word_list))) \n",
    "    df['lema_stem_tokens'] = df['lema_stem_tokens'].apply(lambda word_list : list(filter(lambda word : len(word) >= 3, word_list))) \n",
    "\n",
    "    #//*** Build Word Count\n",
    "    df['num_wds'] = df['lema_stem_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "    print( f\"Total Corpus Word Count: {df['lema_stem_tokens'].apply(lambda x: len(x)).sum()}\" )\n",
    "\n",
    "    #//************************\n",
    "    #//*** Stem the lemma's\n",
    "    #//***********************************************************************\n",
    "    #//*** We are trying to reduce the feature set\n",
    "    #//***********************************************************************\n",
    "    df['lema_stem_tokens'] = apply_stemmer(df['lema_stem_tokens'])\n",
    "\n",
    "    #//**********************************\n",
    "    #//*** Apply Part of Speech Tagging\n",
    "    #//**********************************\n",
    "    df['pos_tag'] = apply_pos_tag(df['lema_stem_tokens'])\n",
    "    \n",
    "    #//**********************************\n",
    "    #//*** Save the Dataframe to file\n",
    "    #//**********************************\n",
    "    pd.to_pickle(df,\"z_wk09_categorized_comments_processed.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"z_wk09_categorized_comments_processed.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                cat                                                txt  \\\n",
      "0            sports  Barely better than Gabbert? He was significant...   \n",
      "1            sports  Fuck the ducks and the Angels! But welcome to ...   \n",
      "2            sports  Should have drafted more WRs.\\n\\n- Matt Millen...   \n",
      "3            sports            [Done](https://i.imgur.com/2YZ90pm.jpg)   \n",
      "4            sports                                      No!! NOO!!!!!   \n",
      "...             ...                                                ...   \n",
      "606471  video_games             No. It's probably only happened to you   \n",
      "606472  video_games  I think most of the disappointment came from t...   \n",
      "606473  video_games  dishonored 1/2 looked like arse, so what the h...   \n",
      "606474  video_games                                          [removed]   \n",
      "606475  video_games  I wish more games provided options like Rise o...   \n",
      "\n",
      "                                                processed  \\\n",
      "0       [barely, better, gabbert, significantly, bette...   \n",
      "1       [fuck, ducks, the, angels, welcome, all, new, ...   \n",
      "2            [have, drafted, wrs, matt, millen, probably]   \n",
      "3                                                  [done]   \n",
      "4                                                   [noo]   \n",
      "...                                                   ...   \n",
      "606471                     [its, probably, happened, you]   \n",
      "606472  [think, disappointment, came, delay, of, the, ...   \n",
      "606473  [dishonored, 12, looked, like, arse, what, hel...   \n",
      "606474                                          [removed]   \n",
      "606475  [wish, games, provided, options, like, rise, t...   \n",
      "\n",
      "                                         lema_stem_tokens  num_wds  \\\n",
      "0       [bare, better, gabbert, significantli, better,...       56   \n",
      "1       [fuck, duck, the, angel, welcom, all, new, nin...        9   \n",
      "2                [have, draft, wr, matt, millen, probabl]        6   \n",
      "3                                                  [done]        1   \n",
      "4                                                   [noo]        1   \n",
      "...                                                   ...      ...   \n",
      "606471                             [probabl, happen, you]        3   \n",
      "606472  [think, disappoint, came, delay, the, ps+, ver...       34   \n",
      "606473  [dishonor, look, like, ars, what, hell, they, ...       14   \n",
      "606474                                            [remov]        1   \n",
      "606475  [wish, game, provid, option, like, rise, the, ...       13   \n",
      "\n",
      "                                                  pos_tag  \n",
      "0       [(bare, NN), (better, RBR), (gabbert, NN), (si...  \n",
      "1       [(fuck, JJ), (duck, VBD), (the, DT), (angel, N...  \n",
      "2       [(have, VB), (draft, NN), (wr, NN), (matt, NN)...  \n",
      "3                                           [(done, VBN)]  \n",
      "4                                             [(noo, NN)]  \n",
      "...                                                   ...  \n",
      "606471          [(probabl, NN), (happen, VB), (you, PRP)]  \n",
      "606472  [(think, VB), (disappoint, NN), (came, VBD), (...  \n",
      "606473  [(dishonor, JJ), (look, NN), (like, IN), (ars,...  \n",
      "606474                                      [(remov, NN)]  \n",
      "606475  [(wish, JJ), (game, NN), (provid, NN), (option...  \n",
      "\n",
      "[606008 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge transformers \n",
    "\n",
    "#from sklearn.externals import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "#//*** Convert categorical string to categorical int\n",
    "#//*** Only run once to prevent iPython issues\n",
    "if (df.dtypes['cat'] == object):\n",
    "    cat_dict = dict(tuple(enumerate(df['cat'].unique())))\n",
    "    #//*** Build sexcat Categorical column\n",
    "    df['intcat'] = df['cat'].copy()\n",
    "    \n",
    "    #//*** replace values using the sex_dict dictionary\n",
    "    for key,value in cat_dict.items():\n",
    "        df['intcat'] = df['intcat'].replace(value,key)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'MLPRegressor(hidden_layer_sizes=[500, 150], verbose=True)' (type <class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>) doesn't",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-4e77f180f71d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#cpath = df['lema_stem_tokens']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m regressor = Pipeline({\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;31m#'norm', TextNormalizer(),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m'tfidf'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, steps, memory, verbose)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_validate_steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    157\u001b[0m             if (not (hasattr(t, \"fit\") or hasattr(t, \"fit_transform\")) or not\n\u001b[0;32m    158\u001b[0m                     hasattr(t, \"transform\")):\n\u001b[1;32m--> 159\u001b[1;33m                 raise TypeError(\"All intermediate steps should be \"\n\u001b[0m\u001b[0;32m    160\u001b[0m                                 \u001b[1;34m\"transformers and implement fit and transform \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m                                 \u001b[1;34m\"or be the string 'passthrough' \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'MLPRegressor(hidden_layer_sizes=[500, 150], verbose=True)' (type <class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>) doesn't"
     ]
    }
   ],
   "source": [
    "#from transformer import TextNormalizer\n",
    "#//*** https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "#https://duckduckgo.com/?q=sklearn+cross_val_score+&t=ffab&ia=web\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#//*** Path to postprocessed, part of speech tagges review corpus\n",
    "#cpath = df['lema_stem_tokens']\n",
    "\n",
    "regressor = Pipeline({\n",
    "    #'norm', TextNormalizer(),\n",
    "    ('tfidf',TfidfVectorizer()),\n",
    "    ('ann',MLPRegressor(hidden_layer_sizes=[500,150], verbose=True))\n",
    "})\n",
    "\n",
    "#//*** X is Post Processed Data to evaluate\n",
    "x = (df['lema_stem_tokens'])\n",
    "\n",
    "#//*** Y is the Categories t\n",
    "y = np.array(df['intcat'])\n",
    "#print(df)\n",
    "\n",
    "#continuous scoring model\n",
    "#scoring = 'r2_score'\n",
    "\n",
    "#categorical scoring model\n",
    "scoring = 'f1'\n",
    "\n",
    "scores = cross_val_score(regressor,x,y,cv=12,scoring=scoring)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Neural Network Classifier with Keras###\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using Keras. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://oindrilasen.com/2021/02/how-to-install-and-import-keras-in-anaconda-jupyter-notebooks/\n",
    "\n",
    "#from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classifying Images ###\n",
    "In chapter 20 of the Machine Learning with Python Cookbook, implement the code found in section 20.15 classify MSINT images using a convolutional neural network. Report the accuracy of your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
