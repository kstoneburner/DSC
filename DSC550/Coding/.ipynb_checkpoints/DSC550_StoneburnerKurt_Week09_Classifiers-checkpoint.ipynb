{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stoneburner, Kurt\n",
    "- ## DSC 550 - Week 09/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Neural Network Classifier with Scikit ### \n",
    "\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using scikit-learn. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//***************************************\n",
    "#//*** Apply Common Cleanup operations\n",
    "#//***************************************\n",
    "#//*** These cleanup functions are based on Week 02 cleanup code, and rebuilt for Week 04\n",
    "\n",
    "#//*****************************************\n",
    "#//*** Functions:\n",
    "#//*****************************************\n",
    "#//*** Mr_clean_text: Converts to lowercase, removes punctuation, newlines and html markup\n",
    "#//****************************************************************************************************\n",
    "#//*** Tokenize_series: Converts a Series containing strings, to a series containing tokenized lists\n",
    "#//****************************************************************************************************\n",
    "#//*** Remove_stop_words: Removes Stop words based on nltk stopwords 'english' dictionary\n",
    "#//****************************************************************************************************\n",
    "#//*** Apply_stemmer: Stem tokenized words using nltk.stem.porter.PorterStemme\n",
    "#//****************************************************************************************************\n",
    "#//*** apply_pos_tag: Builds Part of Speech Tagging from tokeninzed text\n",
    "#//****************************************************************************************************\n",
    "\n",
    "#//****************************************************************************************************\n",
    "\n",
    "#//****************************************************************************************************\n",
    "#//*** Key values will default to true. If code needs to be defaulted to False, a default_false list can be added later\n",
    "#//*** All Boolean kwarg keya are stored in kwarg list. This speeds up the coding of the action_dict.\n",
    "#//*** As Kwargs are added \n",
    "def mr_clean_text(input_series, input_options={}):\n",
    "    \n",
    "    def clean_text(input_string):\n",
    "        clean1 = re.sub(r'['+string.punctuation + '’—”'+']', \"\", input_string.lower())\n",
    "        return re.sub(r'\\W+', ' ', clean1)\n",
    "\n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "    #//*** Add some data validation. I'm preparing this function for additional use. I'm checking if future users (ie future me)\n",
    "    #//*** may throw some garbage at this function. Experience has taught me to fail safely wherever possible.\n",
    "\n",
    "    #//*** All kwargs are listed here. These initialize TRUE by default.\n",
    "    key_list = [ \"lower\", \"newline\", \"html\", \"punctuation\" ]\n",
    "    \n",
    "    default_false = [\"remove_empty\"]\n",
    "    \n",
    "    #//*** Build Action Dictionary\n",
    "    action_dict = { } \n",
    "    \n",
    "    #//*** Build the keys from kwarg_list and default them to TRUE\n",
    "    for key in key_list:\n",
    "        action_dict[key] = True\n",
    "    \n",
    "    for key in default_false:\n",
    "        action_dict[key] = False\n",
    "        \n",
    "    #//*** Loop through the input kwargs (if any). Assign the action_dict values based on the kwargs:\n",
    "    for key,value in input_options.items():\n",
    "        print(key,value)\n",
    "        action_dict[key] = value\n",
    "    \n",
    "    \n",
    "    #//*************************************************************************\n",
    "    #//*** The Cleanup/Processing code is a straight lift from DSC550 - Week02\n",
    "    #//*************************************************************************\n",
    "    #//*** Convert to Lower Case, Default to True\n",
    "    if action_dict[\"lower\"]:\n",
    "        input_series = input_series.str.lower()\n",
    "    \n",
    "   \n",
    "    #//*** Remove New Lines\n",
    "    if action_dict[\"newline\"]:\n",
    "        #//*** Rmove \\r\\n\n",
    "        input_series = input_series.str.replace('\\r?\\n',\"\")\n",
    "\n",
    "        #//*** Remove \\n new lines\n",
    "        input_series = input_series.str.replace('\\n',\"\")\n",
    "    \n",
    "    \n",
    "    input_series = input_series.str.replace(\"\\\\(http.+\\\\)\",\"\")\n",
    "    \n",
    "    #//*** Print Elements between brackets\n",
    "    #print(input_series[ input_series == input_series.str.match('[.*]')])\n",
    "\n",
    "     \n",
    "    #//*** Remove html entities, observed entities are &gt; and &lt;. All HTML entities begin with & and end with ;.\n",
    "    #//*** Let's use regex to remove html entities\n",
    "    if action_dict[\"html\"]:\n",
    "        input_series = input_series.str.replace(r'&.*;',\"\")\n",
    "\n",
    "    #//*** Remove the empty lines\n",
    "    if action_dict[\"remove_empty\"]:\n",
    "        input_series = input_series[ input_series.str.len() > 0]\n",
    "\n",
    "    #//*** Remove punctuation\n",
    "    if action_dict[\"punctuation\"]:\n",
    "        #//*** Load libraries for punctuation if not already loaded.\n",
    "        #//*** Wrapping these in a try, no sense in importing libraries that already exist.\n",
    "        #//*** Unsure of the cost of reimporting libraries (if any). But testing if library is already loaded feels\n",
    "        #//*** like a good practice\n",
    "\n",
    "        #input_series = input_series.apply(lambda x: clean_text(x))\n",
    "\n",
    "        try:\n",
    "            type(sys)\n",
    "        except:\n",
    "            import sys\n",
    "\n",
    "        try:\n",
    "            type(unicodedata)\n",
    "        except:\n",
    "            import unicodedata\n",
    "\n",
    "        #//*** replace Comma and Period with a space.\n",
    "        for punct in [\",\",\".\"]:\n",
    "            input_series = input_series.str.replace(punct,\" \")\n",
    "\n",
    "        #//*** Remove punctuation using the example from the book\n",
    "        punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P') )\n",
    "        input_series = input_series.str.translate(punctuation)\n",
    "\n",
    "        #table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}\n",
    "        #print(table )\n",
    "        #input_series = input_series.str.translate(table)\n",
    "\n",
    "    print(f\"Text Cleaning Time: {time.time() - start_time}\")\n",
    "\n",
    "    return input_series\n",
    "\n",
    "                                          \n",
    "#//*** Tokenize a Series containing Strings.\n",
    "#//*** Breaking this out into it's own function for later reuse.\n",
    "#//*** Not a lot of code here, but it helps to keep the libraries localized. This creates standarization for future\n",
    "#//*** Stoneburner projects. Also has the ability to add functionality as needed.\n",
    "\n",
    "def tokenize_series(input_series,slices=20,input_options={}):\n",
    "    \n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "    \n",
    "    word_tokenize = nltk.tokenize.word_tokenize \n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "        \n",
    "    #//*** All kwargs are listed here. These initialize False by default.\n",
    "    key_list = [ \"fast\", \"quiet\" ]\n",
    "    \n",
    "    #//*** Build Action Dictionary\n",
    "    action_dict = { } \n",
    "    \n",
    "    #//*** Build the keys from kwarg_list and default them to False\n",
    "    for key in key_list:\n",
    "        action_dict[key] = False\n",
    "        \n",
    "    #//*** Loop through the input kwargs (if any). Assign the action_dict values based on the kwargs:\n",
    "    for key,value in input_options.items():\n",
    "        print(key,value)\n",
    "        action_dict[key] = value\n",
    "    \n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "            \n",
    "    #input_series = input_series.apply(word_tokenize)\n",
    "    \n",
    "    if action_dict['fast'] == False:\n",
    "        print(\"Processing Tokens with NLTK Word Tokenize\")\n",
    "        input_series = apply_with_progress(input_series,word_tokenize,slices)\n",
    "    else:\n",
    "        print(\"Process Tokens with Split()\")\n",
    "        input_series = apply_with_progress(input_series,lambda x: x.split(),slices)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"Tokenize Time: {time.time() - start_time}\")\n",
    "    \n",
    "    return input_series\n",
    "\n",
    "#//*** Remove Stop words from the input list\n",
    "def remove_stop_words(input_series):\n",
    "    \n",
    "    #//*** This function removes stop_words from a series.\n",
    "    #//*** Works with series.apply()\n",
    "    def apply_stop_words(input_list):\n",
    "\n",
    "        #//*** Load Stopwords   \n",
    "        for word in input_list:\n",
    "            if word in stop_words:\n",
    "                input_list.remove(word)\n",
    "                #print(f\"Removing: {word}\")\n",
    "        return input_list\n",
    "\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "        \n",
    "    stopwords = nltk.corpus.stopwords\n",
    "\n",
    "    #//*** Stopwords requires an additional download\n",
    "    try:\n",
    "        type(stopwords)\n",
    "    except:\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "\n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "\n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    #//*** The stop_words include punctuation. Stop Word Contractions will not be filtered out.\n",
    "    #//*** Manually adding word the\n",
    "    stop_words = []\n",
    "    \n",
    "    #//*** Remove apostrophies from the stop_words\n",
    "    for stop in stopwords.words('english'):\n",
    "        stop_words.append(stop.replace(\"'\",\"\"))\n",
    "\n",
    "    #print(\"Stop Words: \")\n",
    "    print(stop_words)\n",
    "    print (\"Processing Stop Words\")\n",
    "    input_series = apply_with_progress(input_series, apply_stop_words)\n",
    "    \n",
    "    print(f\"Stop Words Time: {time.time() - start_time}\")\n",
    "    \n",
    "    return input_series\n",
    "\n",
    "def apply_stemmer(input_series,trim_single_words = True,slices=100):\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "\n",
    "    #//*** Instantiate the Stemmer\n",
    "    porter = nltk.stem.porter.PorterStemmer()\n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    #//*** 1.) Apply() an action to each row\n",
    "    #//*** 2.) lambda word_list, each row is treated as word_list for the subsequent expression\n",
    "    #//*** 3.) The base [ word for word in wordlist] would return each word in word_list as a list. \n",
    "    #//*** 4.) [porter.stem(word) for word in word_list] - performs stemming on each word and returns a list\n",
    "    #input_series = input_series.apply(lambda word_list: [porter.stem(word) for word in word_list] )\n",
    "    print(\"Begin: Apply Stemmer\")\n",
    "    input_series = apply_with_progress(input_series, lambda word_list: [porter.stem(word) for word in word_list],slices)\n",
    "    \n",
    "    #//*** Remove Single letter words after stemming\n",
    "    \n",
    "    \"\"\"\n",
    "    if trim_single_words:\n",
    "        for word_list in input_series:\n",
    "            for word in word_list:\n",
    "                if len(word) < 2:\n",
    "                    word_list.remove(word)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Apply Stemmer Time: {time.time() - start_time}\")\n",
    "    return input_series\n",
    "\n",
    "def apply_pos_tag(input_series,slices=100):\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "\n",
    "    from nltk import pos_tag\n",
    "\n",
    "    #//pos_tag requires an additional download\n",
    "    try:\n",
    "        pos_tag([\"the\",\"quick\",\"brown\",\"fox\"])\n",
    "    except: \n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    print(\"Begin Part of Speech tagging\")\n",
    "    \n",
    "    input_series = apply_with_progress(input_series,pos_tag,slices)\n",
    "    \n",
    "    print(f\"Part of Speech Tagging Time: {round(time.time() - start_time,2)}s\")\n",
    "    \n",
    "    return input_series\n",
    "    \n",
    "def apply_lemmatization(input_series,slices=20):\n",
    "            \n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    from nltk.corpus import wordnet    \n",
    "    \n",
    "    #nltk.download('wordnet')\n",
    "    \n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    # Initialize the Lemmatizer instance\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    \n",
    "    #//*** 1.) Apply() an action to each row\n",
    "    #//*** 2.) lambda word_list, each row is treated as word_list for the subsequent expression\n",
    "    #//*** 3.) The base [ word for word in wordlist] would return each word in word_list as a list. \n",
    "    #//*** 4.) [lemmatizer.lemmatize(word) for word in word_list] - performs lemmtization on each word and returns a list\n",
    "    #lemmatized = input_series.apply(lambda word_list: [lemmatizer.lemmatize(*word) for word in word_list] )\n",
    "    \n",
    "    print(\"Begin Lemmatization...\")\n",
    "    \n",
    "    input_series = apply_with_progress(input_series,lambda word_list: [lemmatizer.lemmatize(word) for word in word_list],20)\n",
    "    \n",
    "    print(f\"Lemmatization Time: {time.time() - start_time}\")\n",
    "    \n",
    "    #if detoken:\n",
    "    #    return tokenize_series(input_series,5,{\"fast\":True})\n",
    "\n",
    "    return input_series\n",
    "\n",
    "#//*** Apply a function to a Series and display processing progress\n",
    "#//*** Slices is the total number of intervals to report progress.\n",
    "#//*** Slices = 20 displays processing after every 5% is processed\n",
    "#//*** Slices = 100 displays processing after every 1% is processed\n",
    "def apply_with_progress(input_series,input_function,slices=20):\n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Get the time at the start of the loop, used for elapsed time.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** The interval is the number of elements to process in each Loop. The default is 20.\n",
    "    #//*** Which displays results at 5% intervals.\n",
    "    interval = int(len(input_series)/slices)\n",
    "    \n",
    "    #//*** Total number of items to process\n",
    "    total = len(input_series)\n",
    "    \n",
    "\n",
    "    #//*** Loop through slice times and display processing statistics for each slice.\n",
    "    for x in range(0, slices ):\n",
    "        #//*** Get time at the start of the slice.\n",
    "        loop_time = time.time()\n",
    "        \n",
    "        #//*** Set the start index\n",
    "        begin_dex = interval*x\n",
    "        \n",
    "        #//*** Set the end index\n",
    "        end_dex = interval*x+interval-1\n",
    "        \n",
    "        #//*** Apply the input function to a slice of the input_series\n",
    "        #//*** This part does all the actual 'work'\n",
    "        input_series[begin_dex:end_dex] = input_series[begin_dex:end_dex].apply(input_function)\n",
    "        \n",
    "        #//*** Get the time after the slice of work is done\n",
    "        now = time.time()\n",
    "        \n",
    "        #//*** Compute the estimated remaining time\n",
    "        #//*** Total elapsed time / % of completed slices = Estimated total time\n",
    "        #//*** Estimated total time - elaped time = Remaining time\n",
    "        est_remain = round( ( ( now - start_time ) /  ( (x+1)/slices ) - (now-start_time)),2)\n",
    "\n",
    "        #//*** Display Results so we know how much time is left (so we can effectively multi-task: ie comments, research and Valheim)\n",
    "        print(f\"Processed {x}/{slices}: {begin_dex}:{end_dex} [{total}] in {round(now-loop_time,2)}s elapsed: {round(now-start_time,2)}s est Remain: {est_remain}s\")\n",
    "    \n",
    "    #//*** END For Slice Loop\n",
    "    \n",
    "    #//*** Process the remaining values (Since interval is an int there should be a remainder)\n",
    "    loop_time = time.time()\n",
    "    begin_dex = end_dex+1\n",
    "    if begin_dex < len(input_series):\n",
    "        print(f\"Processing Remaining values: {begin_dex} : {total} \")\n",
    "        #print(input_series[begin_dex:])\n",
    "        input_series[begin_dex:] = input_series[begin_dex:].apply(input_function)\n",
    "    \n",
    "    #//*** Display Final output\n",
    "    print(f\"Processed {slices}/{slices}: {begin_dex}:{end_dex} [{total}] in {round(time.time()-loop_time,2)}s elapsed: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "    #//*** return Series\n",
    "    return input_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"z_wk09_categorized-comments.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                cat                                                txt  \\\n",
      "0            sports  Barely better than Gabbert? He was significant...   \n",
      "1            sports  Fuck the ducks and the Angels! But welcome to ...   \n",
      "2            sports  Should have drafted more WRs.\\n\\n- Matt Millen...   \n",
      "3            sports            [Done](https://i.imgur.com/2YZ90pm.jpg)   \n",
      "4            sports                                      No!! NOO!!!!!   \n",
      "...             ...                                                ...   \n",
      "606471  video_games             No. It's probably only happened to you   \n",
      "606472  video_games  I think most of the disappointment came from t...   \n",
      "606473  video_games  dishonored 1/2 looked like arse, so what the h...   \n",
      "606474  video_games                                          [removed]   \n",
      "606475  video_games  I wish more games provided options like Rise o...   \n",
      "\n",
      "                                                processed  \n",
      "0       barely better than gabbert he was significantl...  \n",
      "1       fuck the ducks and the angels but welcome to a...  \n",
      "2       should have drafted more wrs  matt millen prob...  \n",
      "3                         donehttpsi imgur com2yz90pm jpg  \n",
      "4                                                  no noo  \n",
      "...                                                   ...  \n",
      "606471              no  its probably only happened to you  \n",
      "606472  i think most of the disappointment came from t...  \n",
      "606473  dishonored 12 looked like arse  so what the he...  \n",
      "606474                                            removed  \n",
      "606475  i wish more games provided options like rise o...  \n",
      "\n",
      "[606018 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)\n",
    "#df['pos_tag'] = apply_pos_tag(df['txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower True\n",
      "newline True\n",
      "html True\n",
      "remove_empty False\n",
      "punctuation True\n",
      "Text Cleaning Time: 4.877617120742798\n",
      "Articles of length with 0 characters: 0\n",
      "Articles of length with 0 characters: 0\n",
      "Remove these articles\n",
      "Article Count Before: 606008\n",
      "Article Count After: 606008\n"
     ]
    }
   ],
   "source": [
    "#//*** Clean the Text.\n",
    "df['processed'] = mr_clean_text(df['txt'],{\"lower\": True, \"newline\": True, \"html\": True, \"remove_empty\" : False, \"punctuation\" : True})\n",
    "\n",
    "print(f\"Articles of length with 0 characters: {len(df[ df['processed'].str.len() == 0 ])}\")\n",
    "\n",
    "#//Remove Items with an Arbitrary length of 0\n",
    "\n",
    "print(f\"Articles of length with 0 characters: {len(df[ df['processed'].str.len() == 0 ])}\")\n",
    "print(\"Remove these articles\")\n",
    "print(f\"Article Count Before: {len(df)}\")\n",
    "df = df[ df['processed'].str.len() > 0 ]\n",
    "print(f\"Article Count After: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast True\n",
      "Process Tokens with Split()\n",
      "Processed 0/20: 0:30299 [606008] in 0.19s elapsed: 0.19s est Remain: 3.57s\n",
      "Processed 1/20: 30300:60599 [606008] in 0.06s elapsed: 0.25s est Remain: 2.21s\n",
      "Processed 2/20: 60600:90899 [606008] in 0.06s elapsed: 0.31s est Remain: 1.74s\n",
      "Processed 3/20: 90900:121199 [606008] in 0.05s elapsed: 0.35s est Remain: 1.41s\n",
      "Processed 4/20: 121200:151499 [606008] in 0.08s elapsed: 0.43s est Remain: 1.3s\n",
      "Processed 5/20: 151500:181799 [606008] in 0.06s elapsed: 0.49s est Remain: 1.15s\n",
      "Processed 6/20: 181800:212099 [606008] in 0.06s elapsed: 0.55s est Remain: 1.02s\n",
      "Processed 7/20: 212100:242399 [606008] in 0.05s elapsed: 0.6s est Remain: 0.9s\n",
      "Processed 8/20: 242400:272699 [606008] in 0.56s elapsed: 1.16s est Remain: 1.42s\n",
      "Processed 9/20: 272700:302999 [606008] in 0.09s elapsed: 1.26s est Remain: 1.26s\n",
      "Processed 10/20: 303000:333299 [606008] in 0.07s elapsed: 1.33s est Remain: 1.09s\n",
      "Processed 11/20: 333300:363599 [606008] in 0.05s elapsed: 1.38s est Remain: 0.92s\n",
      "Processed 12/20: 363600:393899 [606008] in 0.06s elapsed: 1.44s est Remain: 0.78s\n",
      "Processed 13/20: 393900:424199 [606008] in 0.05s elapsed: 1.5s est Remain: 0.64s\n",
      "Processed 14/20: 424200:454499 [606008] in 0.08s elapsed: 1.58s est Remain: 0.53s\n",
      "Processed 15/20: 454500:484799 [606008] in 0.06s elapsed: 1.65s est Remain: 0.41s\n",
      "Processed 16/20: 484800:515099 [606008] in 0.07s elapsed: 1.72s est Remain: 0.3s\n",
      "Processed 17/20: 515100:545399 [606008] in 0.74s elapsed: 2.45s est Remain: 0.27s\n",
      "Processed 18/20: 545400:575699 [606008] in 0.07s elapsed: 2.52s est Remain: 0.13s\n",
      "Processed 19/20: 575700:605999 [606008] in 0.07s elapsed: 2.59s est Remain: 0.0s\n",
      "Processing Remaining values: 606000 : 606008 \n",
      "Processed 20/20: 606000:605999 [606008] in 0.0s elapsed: 2.59s\n",
      "Tokenize Time: 2.595325469970703\n"
     ]
    }
   ],
   "source": [
    "#//************************\n",
    "#//*** Tokenize the Text\n",
    "#//************************\n",
    "#//*** The custom function displays progress while it's working\n",
    "df['processed'] = tokenize_series(df['processed'],20,{\"fast\":True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre', 'youve', 'youll', 'youd', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'shes', 'her', 'hers', 'herself', 'it', 'its', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'thatll', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'dont', 'should', 'shouldve', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'arent', 'couldn', 'couldnt', 'didn', 'didnt', 'doesn', 'doesnt', 'hadn', 'hadnt', 'hasn', 'hasnt', 'haven', 'havent', 'isn', 'isnt', 'ma', 'mightn', 'mightnt', 'mustn', 'mustnt', 'needn', 'neednt', 'shan', 'shant', 'shouldn', 'shouldnt', 'wasn', 'wasnt', 'weren', 'werent', 'won', 'wont', 'wouldn', 'wouldnt']\n",
      "Processing Stop Words\n",
      "Processed 0/20: 0:30299 [606008] in 0.78s elapsed: 0.78s est Remain: 14.73s\n",
      "Processed 1/20: 30300:60599 [606008] in 0.69s elapsed: 1.46s est Remain: 13.15s\n",
      "Processed 2/20: 60600:90899 [606008] in 0.63s elapsed: 2.09s est Remain: 11.82s\n",
      "Processed 3/20: 90900:121199 [606008] in 0.55s elapsed: 2.63s est Remain: 10.54s\n",
      "Processed 4/20: 121200:151499 [606008] in 0.91s elapsed: 3.54s est Remain: 10.63s\n",
      "Processed 5/20: 151500:181799 [606008] in 0.67s elapsed: 4.21s est Remain: 9.82s\n",
      "Processed 6/20: 181800:212099 [606008] in 0.63s elapsed: 4.84s est Remain: 8.99s\n",
      "Processed 7/20: 212100:242399 [606008] in 0.54s elapsed: 5.38s est Remain: 8.07s\n",
      "Processed 8/20: 242400:272699 [606008] in 0.58s elapsed: 5.96s est Remain: 7.28s\n",
      "Processed 9/20: 272700:302999 [606008] in 1.15s elapsed: 7.1s est Remain: 7.1s\n",
      "Processed 10/20: 303000:333299 [606008] in 0.85s elapsed: 7.95s est Remain: 6.51s\n",
      "Processed 11/20: 333300:363599 [606008] in 0.6s elapsed: 8.56s est Remain: 5.71s\n",
      "Processed 12/20: 363600:393899 [606008] in 0.73s elapsed: 9.29s est Remain: 5.0s\n",
      "Processed 13/20: 393900:424199 [606008] in 0.72s elapsed: 10.01s est Remain: 4.29s\n",
      "Processed 14/20: 424200:454499 [606008] in 0.74s elapsed: 10.75s est Remain: 3.58s\n",
      "Processed 15/20: 454500:484799 [606008] in 0.73s elapsed: 11.48s est Remain: 2.87s\n",
      "Processed 16/20: 484800:515099 [606008] in 0.75s elapsed: 12.23s est Remain: 2.16s\n",
      "Processed 17/20: 515100:545399 [606008] in 0.72s elapsed: 12.94s est Remain: 1.44s\n",
      "Processed 18/20: 545400:575699 [606008] in 0.74s elapsed: 13.68s est Remain: 0.72s\n",
      "Processed 19/20: 575700:605999 [606008] in 0.74s elapsed: 14.42s est Remain: 0.0s\n",
      "Processing Remaining values: 606000 : 606008 \n",
      "Processed 20/20: 606000:605999 [606008] in 0.0s elapsed: 14.42s\n",
      "Stop Words Time: 14.431878089904785\n"
     ]
    }
   ],
   "source": [
    "#//************************\n",
    "#//*** Remove Stop Words\n",
    "#//************************\n",
    "#//*** The custom function displays progress while it's working\n",
    "df['processed'] = remove_stop_words(df['processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Lemmatization...\n",
      "Processed 0/20: 0:30299 [606008] in 3.52s elapsed: 3.52s est Remain: 66.95s\n",
      "Processed 1/20: 30300:60599 [606008] in 1.47s elapsed: 4.99s est Remain: 44.93s\n",
      "Processed 2/20: 60600:90899 [606008] in 1.32s elapsed: 6.31s est Remain: 35.77s\n",
      "Processed 3/20: 90900:121199 [606008] in 1.17s elapsed: 7.48s est Remain: 29.93s\n",
      "Processed 4/20: 121200:151499 [606008] in 1.94s elapsed: 9.42s est Remain: 28.26s\n",
      "Processed 5/20: 151500:181799 [606008] in 1.4s elapsed: 10.82s est Remain: 25.25s\n",
      "Processed 6/20: 181800:212099 [606008] in 1.34s elapsed: 12.16s est Remain: 22.58s\n",
      "Processed 7/20: 212100:242399 [606008] in 1.23s elapsed: 13.39s est Remain: 20.09s\n",
      "Processed 8/20: 242400:272699 [606008] in 1.28s elapsed: 14.67s est Remain: 17.93s\n",
      "Processed 9/20: 272700:302999 [606008] in 2.45s elapsed: 17.12s est Remain: 17.12s\n",
      "Processed 10/20: 303000:333299 [606008] in 1.84s elapsed: 18.96s est Remain: 15.51s\n",
      "Processed 11/20: 333300:363599 [606008] in 2.28s elapsed: 21.24s est Remain: 14.16s\n",
      "Processed 12/20: 363600:393899 [606008] in 1.56s elapsed: 22.8s est Remain: 12.28s\n",
      "Processed 13/20: 393900:424199 [606008] in 1.55s elapsed: 24.35s est Remain: 10.44s\n",
      "Processed 14/20: 424200:454499 [606008] in 1.59s elapsed: 25.94s est Remain: 8.65s\n",
      "Processed 15/20: 454500:484799 [606008] in 1.58s elapsed: 27.52s est Remain: 6.88s\n",
      "Processed 16/20: 484800:515099 [606008] in 1.6s elapsed: 29.11s est Remain: 5.14s\n",
      "Processed 17/20: 515100:545399 [606008] in 1.53s elapsed: 30.65s est Remain: 3.41s\n",
      "Processed 18/20: 545400:575699 [606008] in 1.54s elapsed: 32.19s est Remain: 1.69s\n",
      "Processed 19/20: 575700:605999 [606008] in 1.56s elapsed: 33.76s est Remain: 0.0s\n",
      "Processing Remaining values: 606000 : 606008 \n",
      "Processed 20/20: 606000:605999 [606008] in 0.0s elapsed: 33.76s\n",
      "Lemmatization Time: 33.80564737319946\n"
     ]
    }
   ],
   "source": [
    "df['lema_stem_tokens'] = apply_lemmatization(df['processed']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Corpus Word Count: 9135233\n",
      "Total Corpus Word Count: 9135233\n",
      "Begin: Apply Stemmer\n",
      "Processed 0/100: 0:6059 [606008] in 1.75s elapsed: 1.75s est Remain: 172.76s\n",
      "Processed 1/100: 6060:12119 [606008] in 1.6s elapsed: 3.35s est Remain: 164.09s\n",
      "Processed 2/100: 12120:18179 [606008] in 1.59s elapsed: 4.94s est Remain: 159.75s\n",
      "Processed 3/100: 18180:24239 [606008] in 1.62s elapsed: 6.56s est Remain: 157.54s\n",
      "Processed 4/100: 24240:30299 [606008] in 1.53s elapsed: 8.1s est Remain: 153.87s\n",
      "Processed 5/100: 30300:36359 [606008] in 1.45s elapsed: 9.55s est Remain: 149.62s\n",
      "Processed 6/100: 36360:42419 [606008] in 1.41s elapsed: 10.96s est Remain: 145.57s\n",
      "Processed 7/100: 42420:48479 [606008] in 1.39s elapsed: 12.34s est Remain: 141.96s\n",
      "Processed 8/100: 48480:54539 [606008] in 1.4s elapsed: 13.75s est Remain: 138.98s\n",
      "Processed 9/100: 54540:60599 [606008] in 1.37s elapsed: 15.12s est Remain: 136.04s\n",
      "Processed 10/100: 60600:66659 [606008] in 1.35s elapsed: 16.46s est Remain: 133.22s\n",
      "Processed 11/100: 66660:72719 [606008] in 1.28s elapsed: 17.75s est Remain: 130.15s\n",
      "Processed 12/100: 72720:78779 [606008] in 1.3s elapsed: 19.05s est Remain: 127.47s\n",
      "Processed 13/100: 78780:84839 [606008] in 1.31s elapsed: 20.35s est Remain: 125.03s\n",
      "Processed 14/100: 84840:90899 [606008] in 1.27s elapsed: 21.62s est Remain: 122.53s\n",
      "Processed 15/100: 90900:96959 [606008] in 0.94s elapsed: 22.56s est Remain: 118.45s\n",
      "Processed 16/100: 96960:103019 [606008] in 0.95s elapsed: 23.51s est Remain: 114.79s\n",
      "Processed 17/100: 103020:109079 [606008] in 0.92s elapsed: 24.43s est Remain: 111.31s\n",
      "Processed 18/100: 109080:115139 [606008] in 1.02s elapsed: 25.46s est Remain: 108.53s\n",
      "Processed 19/100: 115140:121199 [606008] in 1.95s elapsed: 27.41s est Remain: 109.64s\n",
      "Processed 20/100: 121200:127259 [606008] in 1.93s elapsed: 29.34s est Remain: 110.39s\n",
      "Processed 21/100: 127260:133319 [606008] in 1.93s elapsed: 31.28s est Remain: 110.9s\n",
      "Processed 22/100: 133320:139379 [606008] in 1.9s elapsed: 33.18s est Remain: 111.07s\n",
      "Processed 23/100: 139380:145439 [606008] in 1.99s elapsed: 35.17s est Remain: 111.38s\n",
      "Processed 24/100: 145440:151499 [606008] in 1.91s elapsed: 37.08s est Remain: 111.24s\n",
      "Processed 25/100: 151500:157559 [606008] in 1.49s elapsed: 38.57s est Remain: 109.78s\n",
      "Processed 26/100: 157560:163619 [606008] in 1.37s elapsed: 39.94s est Remain: 108.0s\n",
      "Processed 27/100: 163620:169679 [606008] in 1.4s elapsed: 41.35s est Remain: 106.32s\n",
      "Processed 28/100: 169680:175739 [606008] in 1.32s elapsed: 42.67s est Remain: 104.46s\n",
      "Processed 29/100: 175740:181799 [606008] in 1.35s elapsed: 44.02s est Remain: 102.71s\n",
      "Processed 30/100: 181800:187859 [606008] in 1.33s elapsed: 45.36s est Remain: 100.95s\n",
      "Processed 31/100: 187860:193919 [606008] in 1.36s elapsed: 46.71s est Remain: 99.26s\n",
      "Processed 32/100: 193920:199979 [606008] in 1.36s elapsed: 48.07s est Remain: 97.61s\n",
      "Processed 33/100: 199980:206039 [606008] in 1.39s elapsed: 49.46s est Remain: 96.01s\n",
      "Processed 34/100: 206040:212099 [606008] in 1.19s elapsed: 50.65s est Remain: 94.07s\n",
      "Processed 35/100: 212100:218159 [606008] in 1.12s elapsed: 51.77s est Remain: 92.04s\n",
      "Processed 36/100: 218160:224219 [606008] in 1.17s elapsed: 52.94s est Remain: 90.15s\n",
      "Processed 37/100: 224220:230279 [606008] in 1.09s elapsed: 54.03s est Remain: 88.15s\n",
      "Processed 38/100: 230280:236339 [606008] in 1.14s elapsed: 55.17s est Remain: 86.29s\n",
      "Processed 39/100: 236340:242399 [606008] in 1.11s elapsed: 56.28s est Remain: 84.41s\n",
      "Processed 40/100: 242400:248459 [606008] in 1.13s elapsed: 57.41s est Remain: 82.61s\n",
      "Processed 41/100: 248460:254519 [606008] in 1.13s elapsed: 58.53s est Remain: 80.83s\n",
      "Processed 42/100: 254520:260579 [606008] in 1.13s elapsed: 59.66s est Remain: 79.08s\n",
      "Processed 43/100: 260580:266639 [606008] in 1.12s elapsed: 60.77s est Remain: 77.35s\n",
      "Processed 44/100: 266640:272699 [606008] in 1.46s elapsed: 62.23s est Remain: 76.06s\n",
      "Processed 45/100: 272700:278759 [606008] in 2.44s elapsed: 64.68s est Remain: 75.93s\n",
      "Processed 46/100: 278760:284819 [606008] in 2.43s elapsed: 67.11s est Remain: 75.67s\n",
      "Processed 47/100: 284820:290879 [606008] in 2.33s elapsed: 69.43s est Remain: 75.22s\n",
      "Processed 48/100: 290880:296939 [606008] in 2.41s elapsed: 71.84s est Remain: 74.77s\n",
      "Processed 49/100: 296940:302999 [606008] in 2.3s elapsed: 74.14s est Remain: 74.14s\n",
      "Processed 50/100: 303000:309059 [606008] in 2.38s elapsed: 76.52s est Remain: 73.52s\n",
      "Processed 51/100: 309060:315119 [606008] in 1.94s elapsed: 78.45s est Remain: 72.42s\n",
      "Processed 52/100: 315120:321179 [606008] in 1.58s elapsed: 80.03s est Remain: 70.97s\n",
      "Processed 53/100: 321180:327239 [606008] in 1.52s elapsed: 81.55s est Remain: 69.47s\n",
      "Processed 54/100: 327240:333299 [606008] in 1.38s elapsed: 82.94s est Remain: 67.86s\n",
      "Processed 55/100: 333300:339359 [606008] in 1.06s elapsed: 84.0s est Remain: 66.0s\n",
      "Processed 56/100: 339360:345419 [606008] in 1.06s elapsed: 85.06s est Remain: 64.17s\n",
      "Processed 57/100: 345420:351479 [606008] in 1.03s elapsed: 86.09s est Remain: 62.34s\n",
      "Processed 58/100: 351480:357539 [606008] in 1.49s elapsed: 87.58s est Remain: 60.86s\n",
      "Processed 59/100: 357540:363599 [606008] in 1.47s elapsed: 89.05s est Remain: 59.36s\n",
      "Processed 60/100: 363600:369659 [606008] in 1.47s elapsed: 90.52s est Remain: 57.87s\n",
      "Processed 61/100: 369660:375719 [606008] in 2.76s elapsed: 93.28s est Remain: 57.17s\n",
      "Processed 62/100: 375720:381779 [606008] in 1.52s elapsed: 94.79s est Remain: 55.67s\n",
      "Processed 63/100: 381780:387839 [606008] in 1.55s elapsed: 96.34s est Remain: 54.19s\n",
      "Processed 64/100: 387840:393899 [606008] in 1.51s elapsed: 97.85s est Remain: 52.69s\n",
      "Processed 65/100: 393900:399959 [606008] in 1.56s elapsed: 99.41s est Remain: 51.21s\n",
      "Processed 66/100: 399960:406019 [606008] in 1.53s elapsed: 100.94s est Remain: 49.72s\n",
      "Processed 67/100: 406020:412079 [606008] in 1.55s elapsed: 102.49s est Remain: 48.23s\n",
      "Processed 68/100: 412080:418139 [606008] in 1.52s elapsed: 104.01s est Remain: 46.73s\n",
      "Processed 69/100: 418140:424199 [606008] in 1.55s elapsed: 105.56s est Remain: 45.24s\n",
      "Processed 70/100: 424200:430259 [606008] in 1.54s elapsed: 107.1s est Remain: 43.74s\n",
      "Processed 71/100: 430260:436319 [606008] in 1.53s elapsed: 108.63s est Remain: 42.24s\n",
      "Processed 72/100: 436320:442379 [606008] in 1.52s elapsed: 110.15s est Remain: 40.74s\n",
      "Processed 73/100: 442380:448439 [606008] in 1.54s elapsed: 111.69s est Remain: 39.24s\n",
      "Processed 74/100: 448440:454499 [606008] in 1.55s elapsed: 113.25s est Remain: 37.75s\n",
      "Processed 75/100: 454500:460559 [606008] in 1.53s elapsed: 114.78s est Remain: 36.25s\n",
      "Processed 76/100: 460560:466619 [606008] in 1.52s elapsed: 116.3s est Remain: 34.74s\n",
      "Processed 77/100: 466620:472679 [606008] in 1.55s elapsed: 117.85s est Remain: 33.24s\n",
      "Processed 78/100: 472680:478739 [606008] in 1.55s elapsed: 119.4s est Remain: 31.74s\n",
      "Processed 79/100: 478740:484799 [606008] in 1.56s elapsed: 120.96s est Remain: 30.24s\n",
      "Processed 80/100: 484800:490859 [606008] in 1.55s elapsed: 122.51s est Remain: 28.74s\n",
      "Processed 81/100: 490860:496919 [606008] in 1.68s elapsed: 124.19s est Remain: 27.26s\n",
      "Processed 82/100: 496920:502979 [606008] in 1.49s elapsed: 125.67s est Remain: 25.74s\n",
      "Processed 83/100: 502980:509039 [606008] in 1.53s elapsed: 127.2s est Remain: 24.23s\n",
      "Processed 84/100: 509040:515099 [606008] in 1.57s elapsed: 128.77s est Remain: 22.72s\n",
      "Processed 85/100: 515100:521159 [606008] in 1.53s elapsed: 130.3s est Remain: 21.21s\n",
      "Processed 86/100: 521160:527219 [606008] in 1.5s elapsed: 131.8s est Remain: 19.69s\n",
      "Processed 87/100: 527220:533279 [606008] in 1.51s elapsed: 133.3s est Remain: 18.18s\n",
      "Processed 88/100: 533280:539339 [606008] in 1.48s elapsed: 134.78s est Remain: 16.66s\n",
      "Processed 89/100: 539340:545399 [606008] in 1.46s elapsed: 136.25s est Remain: 15.14s\n",
      "Processed 90/100: 545400:551459 [606008] in 1.52s elapsed: 137.76s est Remain: 13.63s\n",
      "Processed 91/100: 551460:557519 [606008] in 1.5s elapsed: 139.26s est Remain: 12.11s\n",
      "Processed 92/100: 557520:563579 [606008] in 1.57s elapsed: 140.84s est Remain: 10.6s\n",
      "Processed 93/100: 563580:569639 [606008] in 1.59s elapsed: 142.42s est Remain: 9.09s\n",
      "Processed 94/100: 569640:575699 [606008] in 1.5s elapsed: 143.92s est Remain: 7.57s\n",
      "Processed 95/100: 575700:581759 [606008] in 1.53s elapsed: 145.46s est Remain: 6.06s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 96/100: 581760:587819 [606008] in 1.45s elapsed: 146.91s est Remain: 4.54s\n",
      "Processed 97/100: 587820:593879 [606008] in 1.44s elapsed: 148.35s est Remain: 3.03s\n",
      "Processed 98/100: 593880:599939 [606008] in 1.64s elapsed: 149.99s est Remain: 1.52s\n",
      "Processed 99/100: 599940:605999 [606008] in 1.63s elapsed: 151.62s est Remain: 0.0s\n",
      "Processing Remaining values: 606000 : 606008 \n",
      "Processed 100/100: 606000:605999 [606008] in 0.0s elapsed: 151.62s\n",
      "Apply Stemmer Time: 151.62582802772522\n"
     ]
    }
   ],
   "source": [
    "print( f\"Total Corpus Word Count: {df['lema_stem_tokens'].apply(lambda x: len(x)).sum()}\" )\n",
    "#//*** Eliminate words with length of 0,1 or 2. This is an arbitrary value to help with feature reduction\n",
    "#df['tokens'] = df['tokens'].apply(lambda word_list : list(filter(lambda word : len(word) >= 3, word_list))) \n",
    "df['lema_stem_tokens'] = df['lema_stem_tokens'].apply(lambda word_list : list(filter(lambda word : len(word) >= 3, word_list))) \n",
    "\n",
    "#//*** Build Word Count\n",
    "df['num_wds'] = df['lema_stem_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "print( f\"Total Corpus Word Count: {df['lema_stem_tokens'].apply(lambda x: len(x)).sum()}\" )\n",
    "\n",
    "#//************************\n",
    "#//*** Stem the lemma's\n",
    "#//***********************************************************************\n",
    "#//*** We are trying to reduce the feature set\n",
    "#//***********************************************************************\n",
    "df['lema_stem_tokens'] = apply_stemmer(df['lema_stem_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Part of Speech tagging\n",
      "Processed 0/100: 0:6059 [606008] in 7.28s elapsed: 7.28s est Remain: 720.63s\n",
      "Processed 1/100: 6060:12119 [606008] in 6.76s elapsed: 14.04s est Remain: 688.11s\n",
      "Processed 2/100: 12120:18179 [606008] in 6.72s elapsed: 20.77s est Remain: 671.47s\n",
      "Processed 3/100: 18180:24239 [606008] in 6.9s elapsed: 27.67s est Remain: 664.1s\n",
      "Processed 4/100: 24240:30299 [606008] in 6.67s elapsed: 34.34s est Remain: 652.54s\n",
      "Processed 5/100: 30300:36359 [606008] in 6.38s elapsed: 40.73s est Remain: 638.06s\n",
      "Processed 6/100: 36360:42419 [606008] in 6.37s elapsed: 47.1s est Remain: 625.73s\n",
      "Processed 7/100: 42420:48479 [606008] in 6.21s elapsed: 53.31s est Remain: 613.04s\n",
      "Processed 8/100: 48480:54539 [606008] in 6.28s elapsed: 59.58s est Remain: 602.47s\n",
      "Processed 9/100: 54540:60599 [606008] in 6.18s elapsed: 65.76s est Remain: 591.88s\n",
      "Processed 10/100: 60600:66659 [606008] in 6.16s elapsed: 71.93s est Remain: 581.94s\n",
      "Processed 11/100: 66660:72719 [606008] in 6.03s elapsed: 77.96s est Remain: 571.69s\n",
      "Processed 12/100: 72720:78779 [606008] in 6.12s elapsed: 84.08s est Remain: 562.71s\n",
      "Processed 13/100: 78780:84839 [606008] in 6.07s elapsed: 90.15s est Remain: 553.76s\n",
      "Processed 14/100: 84840:90899 [606008] in 6.0s elapsed: 96.15s est Remain: 544.86s\n",
      "Processed 15/100: 90900:96959 [606008] in 5.06s elapsed: 101.21s est Remain: 531.35s\n",
      "Processed 16/100: 96960:103019 [606008] in 5.07s elapsed: 106.27s est Remain: 518.87s\n",
      "Processed 17/100: 103020:109079 [606008] in 5.01s elapsed: 111.29s est Remain: 506.99s\n",
      "Processed 18/100: 109080:115139 [606008] in 5.26s elapsed: 116.55s est Remain: 496.88s\n",
      "Processed 19/100: 115140:121199 [606008] in 7.89s elapsed: 124.44s est Remain: 497.76s\n",
      "Processed 20/100: 121200:127259 [606008] in 7.77s elapsed: 132.21s est Remain: 497.35s\n",
      "Processed 21/100: 127260:133319 [606008] in 7.77s elapsed: 139.98s est Remain: 496.28s\n",
      "Processed 22/100: 133320:139379 [606008] in 7.59s elapsed: 147.57s est Remain: 494.03s\n",
      "Processed 23/100: 139380:145439 [606008] in 7.81s elapsed: 155.38s est Remain: 492.04s\n",
      "Processed 24/100: 145440:151499 [606008] in 7.69s elapsed: 163.07s est Remain: 489.2s\n",
      "Processed 25/100: 151500:157559 [606008] in 6.56s elapsed: 169.63s est Remain: 482.8s\n",
      "Processed 26/100: 157560:163619 [606008] in 6.17s elapsed: 175.81s est Remain: 475.33s\n",
      "Processed 27/100: 163620:169679 [606008] in 6.26s elapsed: 182.06s est Remain: 468.16s\n",
      "Processed 28/100: 169680:175739 [606008] in 6.07s elapsed: 188.13s est Remain: 460.59s\n",
      "Processed 29/100: 175740:181799 [606008] in 6.14s elapsed: 194.27s est Remain: 453.29s\n",
      "Processed 30/100: 181800:187859 [606008] in 6.04s elapsed: 200.3s est Remain: 445.84s\n",
      "Processed 31/100: 187860:193919 [606008] in 6.13s elapsed: 206.44s est Remain: 438.68s\n",
      "Processed 32/100: 193920:199979 [606008] in 6.11s elapsed: 212.55s est Remain: 431.53s\n",
      "Processed 33/100: 199980:206039 [606008] in 6.18s elapsed: 218.72s est Remain: 424.58s\n",
      "Processed 34/100: 206040:212099 [606008] in 5.74s elapsed: 224.46s est Remain: 416.86s\n",
      "Processed 35/100: 212100:218159 [606008] in 5.48s elapsed: 229.95s est Remain: 408.8s\n",
      "Processed 36/100: 218160:224219 [606008] in 5.57s elapsed: 235.51s est Remain: 401.01s\n",
      "Processed 37/100: 224220:230279 [606008] in 5.43s elapsed: 240.94s est Remain: 393.12s\n",
      "Processed 38/100: 230280:236339 [606008] in 5.58s elapsed: 246.52s est Remain: 385.58s\n",
      "Processed 39/100: 236340:242399 [606008] in 5.49s elapsed: 252.0s est Remain: 378.01s\n",
      "Processed 40/100: 242400:248459 [606008] in 5.55s elapsed: 257.56s est Remain: 370.63s\n",
      "Processed 41/100: 248460:254519 [606008] in 5.51s elapsed: 263.07s est Remain: 363.28s\n",
      "Processed 42/100: 254520:260579 [606008] in 5.53s elapsed: 268.59s est Remain: 356.04s\n",
      "Processed 43/100: 260580:266639 [606008] in 5.46s elapsed: 274.05s est Remain: 348.79s\n",
      "Processed 44/100: 266640:272699 [606008] in 6.31s elapsed: 280.36s est Remain: 342.67s\n",
      "Processed 45/100: 272700:278759 [606008] in 8.83s elapsed: 289.19s est Remain: 339.49s\n",
      "Processed 46/100: 278760:284819 [606008] in 8.82s elapsed: 298.01s est Remain: 336.06s\n",
      "Processed 47/100: 284820:290879 [606008] in 8.62s elapsed: 306.64s est Remain: 332.19s\n",
      "Processed 48/100: 290880:296939 [606008] in 8.73s elapsed: 315.36s est Remain: 328.23s\n",
      "Processed 49/100: 296940:302999 [606008] in 8.51s elapsed: 323.87s est Remain: 323.87s\n",
      "Processed 50/100: 303000:309059 [606008] in 8.74s elapsed: 332.61s est Remain: 319.57s\n",
      "Processed 51/100: 309060:315119 [606008] in 7.61s elapsed: 340.22s est Remain: 314.05s\n",
      "Processed 52/100: 315120:321179 [606008] in 6.78s elapsed: 347.0s est Remain: 307.72s\n",
      "Processed 53/100: 321180:327239 [606008] in 6.63s elapsed: 353.63s est Remain: 301.24s\n",
      "Processed 54/100: 327240:333299 [606008] in 6.23s elapsed: 359.86s est Remain: 294.43s\n",
      "Processed 55/100: 333300:339359 [606008] in 5.46s elapsed: 365.32s est Remain: 287.04s\n",
      "Processed 56/100: 339360:345419 [606008] in 5.47s elapsed: 370.8s est Remain: 279.72s\n",
      "Processed 57/100: 345420:351479 [606008] in 5.4s elapsed: 376.2s est Remain: 272.42s\n",
      "Processed 58/100: 351480:357539 [606008] in 6.63s elapsed: 382.83s est Remain: 266.04s\n",
      "Processed 59/100: 357540:363599 [606008] in 6.59s elapsed: 389.42s est Remain: 259.61s\n",
      "Processed 60/100: 363600:369659 [606008] in 6.54s elapsed: 395.96s est Remain: 253.15s\n",
      "Processed 61/100: 369660:375719 [606008] in 6.59s elapsed: 402.55s est Remain: 246.72s\n",
      "Processed 62/100: 375720:381779 [606008] in 6.49s elapsed: 409.04s est Remain: 240.23s\n",
      "Processed 63/100: 381780:387839 [606008] in 6.59s elapsed: 415.63s est Remain: 233.79s\n",
      "Processed 64/100: 387840:393899 [606008] in 6.48s elapsed: 422.1s est Remain: 227.29s\n",
      "Processed 65/100: 393900:399959 [606008] in 6.56s elapsed: 428.67s est Remain: 220.83s\n",
      "Processed 66/100: 399960:406019 [606008] in 6.49s elapsed: 435.16s est Remain: 214.33s\n",
      "Processed 67/100: 406020:412079 [606008] in 6.62s elapsed: 441.78s est Remain: 207.9s\n",
      "Processed 68/100: 412080:418139 [606008] in 7.86s elapsed: 449.64s est Remain: 202.01s\n",
      "Processed 69/100: 418140:424199 [606008] in 6.52s elapsed: 456.16s est Remain: 195.5s\n",
      "Processed 70/100: 424200:430259 [606008] in 6.58s elapsed: 462.74s est Remain: 189.01s\n",
      "Processed 71/100: 430260:436319 [606008] in 6.6s elapsed: 469.34s est Remain: 182.52s\n",
      "Processed 72/100: 436320:442379 [606008] in 6.58s elapsed: 475.92s est Remain: 176.03s\n",
      "Processed 73/100: 442380:448439 [606008] in 6.6s elapsed: 482.52s est Remain: 169.53s\n",
      "Processed 74/100: 448440:454499 [606008] in 6.58s elapsed: 489.09s est Remain: 163.03s\n",
      "Processed 75/100: 454500:460559 [606008] in 6.53s elapsed: 495.63s est Remain: 156.51s\n",
      "Processed 76/100: 460560:466619 [606008] in 6.52s elapsed: 502.14s est Remain: 149.99s\n",
      "Processed 77/100: 466620:472679 [606008] in 6.62s elapsed: 508.76s est Remain: 143.5s\n",
      "Processed 78/100: 472680:478739 [606008] in 6.6s elapsed: 515.36s est Remain: 137.0s\n",
      "Processed 79/100: 478740:484799 [606008] in 6.56s elapsed: 521.92s est Remain: 130.48s\n",
      "Processed 80/100: 484800:490859 [606008] in 6.76s elapsed: 528.68s est Remain: 124.01s\n",
      "Processed 81/100: 490860:496919 [606008] in 6.89s elapsed: 535.57s est Remain: 117.56s\n",
      "Processed 82/100: 496920:502979 [606008] in 6.52s elapsed: 542.09s est Remain: 111.03s\n",
      "Processed 83/100: 502980:509039 [606008] in 6.64s elapsed: 548.74s est Remain: 104.52s\n",
      "Processed 84/100: 509040:515099 [606008] in 6.89s elapsed: 555.63s est Remain: 98.05s\n",
      "Processed 85/100: 515100:521159 [606008] in 6.68s elapsed: 562.31s est Remain: 91.54s\n",
      "Processed 86/100: 521160:527219 [606008] in 6.75s elapsed: 569.06s est Remain: 85.03s\n",
      "Processed 87/100: 527220:533279 [606008] in 7.01s elapsed: 576.07s est Remain: 78.55s\n",
      "Processed 88/100: 533280:539339 [606008] in 6.76s elapsed: 582.82s est Remain: 72.03s\n",
      "Processed 89/100: 539340:545399 [606008] in 6.73s elapsed: 589.55s est Remain: 65.51s\n",
      "Processed 90/100: 545400:551459 [606008] in 6.76s elapsed: 596.31s est Remain: 58.98s\n",
      "Processed 91/100: 551460:557519 [606008] in 6.62s elapsed: 602.93s est Remain: 52.43s\n",
      "Processed 92/100: 557520:563579 [606008] in 6.83s elapsed: 609.76s est Remain: 45.9s\n",
      "Processed 93/100: 563580:569639 [606008] in 6.74s elapsed: 616.5s est Remain: 39.35s\n",
      "Processed 94/100: 569640:575699 [606008] in 6.59s elapsed: 623.09s est Remain: 32.79s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 95/100: 575700:581759 [606008] in 6.65s elapsed: 629.74s est Remain: 26.24s\n",
      "Processed 96/100: 581760:587819 [606008] in 6.48s elapsed: 636.22s est Remain: 19.68s\n",
      "Processed 97/100: 587820:593879 [606008] in 6.52s elapsed: 642.74s est Remain: 13.12s\n",
      "Processed 98/100: 593880:599939 [606008] in 6.87s elapsed: 649.61s est Remain: 6.56s\n",
      "Processed 99/100: 599940:605999 [606008] in 6.81s elapsed: 656.43s est Remain: 0.0s\n",
      "Processing Remaining values: 606000 : 606008 \n",
      "Processed 100/100: 606000:605999 [606008] in 0.01s elapsed: 656.44s\n",
      "Part of Speech Tagging Time: 656.45s\n"
     ]
    }
   ],
   "source": [
    "#//**********************************\n",
    "#//*** Apply Part of Speech Tagging\n",
    "#//**********************************\n",
    "df['pos_tag'] = apply_pos_tag(df['lema_stem_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(df,\"z_wk09_categorized_comments_processed.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-6d5be3bdb0d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'intcat'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextNormalizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMLPRegressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformer'"
     ]
    }
   ],
   "source": [
    "#conda install -c conda-forge transformers \n",
    "\n",
    "#from sklearn.externals import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "#//*** Convert categorical string to categorical int\n",
    "#//*** Only run once to prevent iPython issues\n",
    "if (df.dtypes['cat'] == object):\n",
    "    cat_dict = dict(tuple(enumerate(df['cat'].unique())))\n",
    "    #//*** Build sexcat Categorical column\n",
    "    df['intcat'] = df['cat'].copy()\n",
    "    \n",
    "    #//*** replace values using the sex_dict dictionary\n",
    "    for key,value in cat_dict.items():\n",
    "        df['intcat'] = df['intcat'].replace(value,key)\n",
    "\n",
    "x = list(df['lema_stem_tokens'])\n",
    "\n",
    "y = np.array(df['intcat'])\n",
    "\n",
    "from transformer import TextNormalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidVectorizer\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Neural Network Classifier with Keras###\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using Keras. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classifying Images ###\n",
    "In chapter 20 of the Machine Learning with Python Cookbook, implement the code found in section 20.15 classify MSINT images using a convolutional neural network. Report the accuracy of your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
