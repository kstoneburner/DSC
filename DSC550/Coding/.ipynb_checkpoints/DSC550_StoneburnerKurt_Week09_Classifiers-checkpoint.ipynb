{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stoneburner, Kurt\n",
    "- ## DSC 550 - Week 09/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Neural Network Classifier with Scikit ### \n",
    "\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using scikit-learn. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdf = pd.read_json(\"z_controversial-comments.jsonl\", lines=True)\n",
    "df = pd.read_csv(\"z_wk02_controversial_words_df.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//***************************************\n",
    "#//*** Apply Common Cleanup operations\n",
    "#//***************************************\n",
    "#//*** These cleanup functions are based on Week 02 cleanup code, and rebuilt for Week 04\n",
    "\n",
    "#//*****************************************\n",
    "#//*** Functions:\n",
    "#//*****************************************\n",
    "#//*** Mr_clean_text: Converts to lowercase, removes punctuation, newlines and html markup\n",
    "#//****************************************************************************************************\n",
    "#//*** Tokenize_series: Converts a Series containing strings, to a series containing tokenized lists\n",
    "#//****************************************************************************************************\n",
    "#//*** Remove_stop_words: Removes Stop words based on nltk stopwords 'english' dictionary\n",
    "#//****************************************************************************************************\n",
    "#//*** Apply_stemmer: Stem tokenized words using nltk.stem.porter.PorterStemme\n",
    "#//****************************************************************************************************\n",
    "#//*** apply_pos_tag: Builds Part of Speech Tagging from tokeninzed text\n",
    "#//****************************************************************************************************\n",
    "\n",
    "#//****************************************************************************************************\n",
    "\n",
    "#//****************************************************************************************************\n",
    "#//*** Key values will default to true. If code needs to be defaulted to False, a default_false list can be added later\n",
    "#//*** All Boolean kwarg keya are stored in kwarg list. This speeds up the coding of the action_dict.\n",
    "#//*** As Kwargs are added \n",
    "def mr_clean_text(input_series, input_options={}):\n",
    "    \n",
    "    def clean_text(input_string):\n",
    "        clean1 = re.sub(r'['+string.punctuation + '’—”'+']', \"\", input_string.lower())\n",
    "        return re.sub(r'\\W+', ' ', clean1)\n",
    "\n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "    #//*** Add some data validation. I'm preparing this function for additional use. I'm checking if future users (ie future me)\n",
    "    #//*** may throw some garbage at this function. Experience has taught me to fail safely wherever possible.\n",
    "\n",
    "    #//*** All kwargs are listed here. These initialize TRUE by default.\n",
    "    key_list = [ \"lower\", \"newline\", \"html\", \"punctuation\" ]\n",
    "    \n",
    "    default_false = [\"remove_empty\"]\n",
    "    \n",
    "    #//*** Build Action Dictionary\n",
    "    action_dict = { } \n",
    "    \n",
    "    #//*** Build the keys from kwarg_list and default them to TRUE\n",
    "    for key in key_list:\n",
    "        action_dict[key] = True\n",
    "    \n",
    "    for key in default_false:\n",
    "        action_dict[key] = False\n",
    "        \n",
    "    #//*** Loop through the input kwargs (if any). Assign the action_dict values based on the kwargs:\n",
    "    for key,value in input_options.items():\n",
    "        print(key,value)\n",
    "        action_dict[key] = value\n",
    "    \n",
    "    \n",
    "    #//*************************************************************************\n",
    "    #//*** The Cleanup/Processing code is a straight lift from DSC550 - Week02\n",
    "    #//*************************************************************************\n",
    "    #//*** Convert to Lower Case, Default to True\n",
    "    if action_dict[\"lower\"]:\n",
    "        input_series = input_series.str.lower()\n",
    "    \n",
    "   \n",
    "    #//*** Remove New Lines\n",
    "    if action_dict[\"newline\"]:\n",
    "        #//*** Rmove \\r\\n\n",
    "        input_series = input_series.str.replace('\\r?\\n',\"\")\n",
    "\n",
    "        #//*** Remove \\n new lines\n",
    "        input_series = input_series.str.replace('\\n',\"\")\n",
    "\n",
    "    #//*** Remove html entities, observed entities are &gt; and &lt;. All HTML entities begin with & and end with ;.\n",
    "    #//*** Let's use regex to remove html entities\n",
    "    if action_dict[\"html\"]:\n",
    "        input_series = input_series.str.replace(r'&.*;',\"\")\n",
    "\n",
    "    #//*** Remove the empty lines\n",
    "    if action_dict[\"remove_empty\"]:\n",
    "        input_series = input_series[ input_series.str.len() > 0]\n",
    "\n",
    "    #//*** Remove punctuation\n",
    "    if action_dict[\"punctuation\"]:\n",
    "        #//*** Load libraries for punctuation if not already loaded.\n",
    "        #//*** Wrapping these in a try, no sense in importing libraries that already exist.\n",
    "        #//*** Unsure of the cost of reimporting libraries (if any). But testing if library is already loaded feels\n",
    "        #//*** like a good practice\n",
    "\n",
    "        #input_series = input_series.apply(lambda x: clean_text(x))\n",
    "\n",
    "        try:\n",
    "            type(sys)\n",
    "        except:\n",
    "            import sys\n",
    "\n",
    "        try:\n",
    "            type(unicodedata)\n",
    "        except:\n",
    "            import unicodedata\n",
    "\n",
    "        #//*** replace Comma and Period with a space.\n",
    "        for punct in [\",\",\".\"]:\n",
    "            input_series = input_series.str.replace(punct,\" \")\n",
    "\n",
    "        #//*** Remove punctuation using the example from the book\n",
    "        punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P') )\n",
    "        input_series = input_series.str.translate(punctuation)\n",
    "\n",
    "        #table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}\n",
    "        #print(table )\n",
    "        #input_series = input_series.str.translate(table)\n",
    "\n",
    "    print(f\"Text Cleaning Time: {time.time() - start_time}\")\n",
    "\n",
    "    return input_series\n",
    "\n",
    "                                          \n",
    "#//*** Tokenize a Series containing Strings.\n",
    "#//*** Breaking this out into it's own function for later reuse.\n",
    "#//*** Not a lot of code here, but it helps to keep the libraries localized. This creates standarization for future\n",
    "#//*** Stoneburner projects. Also has the ability to add functionality as needed.\n",
    "\n",
    "def tokenize_series(input_series,slices=20,input_options={}):\n",
    "    \n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "    \n",
    "    word_tokenize = nltk.tokenize.word_tokenize \n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "        \n",
    "    #//*** All kwargs are listed here. These initialize False by default.\n",
    "    key_list = [ \"fast\", \"quiet\" ]\n",
    "    \n",
    "    #//*** Build Action Dictionary\n",
    "    action_dict = { } \n",
    "    \n",
    "    #//*** Build the keys from kwarg_list and default them to False\n",
    "    for key in key_list:\n",
    "        action_dict[key] = False\n",
    "        \n",
    "    #//*** Loop through the input kwargs (if any). Assign the action_dict values based on the kwargs:\n",
    "    for key,value in input_options.items():\n",
    "        print(key,value)\n",
    "        action_dict[key] = value\n",
    "    \n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "            \n",
    "    #input_series = input_series.apply(word_tokenize)\n",
    "    \n",
    "    if action_dict['fast'] == False:\n",
    "        print(\"Processing Tokens with NLTK Word Tokenize\")\n",
    "        input_series = apply_with_progress(input_series,word_tokenize,slices)\n",
    "    else:\n",
    "        print(\"Process Tokens with Split()\")\n",
    "        input_series = apply_with_progress(input_series,lambda x: x.split(),slices)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"Tokenize Time: {time.time() - start_time}\")\n",
    "    \n",
    "    return input_series\n",
    "\n",
    "#//*** Remove Stop words from the input list\n",
    "def remove_stop_words(input_series):\n",
    "    \n",
    "    #//*** This function removes stop_words from a series.\n",
    "    #//*** Works with series.apply()\n",
    "    def apply_stop_words(input_list):\n",
    "\n",
    "        #//*** Load Stopwords   \n",
    "        for word in input_list:\n",
    "            if word in stop_words:\n",
    "                input_list.remove(word)\n",
    "                #print(f\"Removing: {word}\")\n",
    "        return input_list\n",
    "\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "        \n",
    "    stopwords = nltk.corpus.stopwords\n",
    "\n",
    "    #//*** Stopwords requires an additional download\n",
    "    try:\n",
    "        type(stopwords)\n",
    "    except:\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "\n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "\n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    #//*** The stop_words include punctuation. Stop Word Contractions will not be filtered out.\n",
    "    #//*** Manually adding word the\n",
    "    stop_words = []\n",
    "    \n",
    "    #//*** Remove apostrophies from the stop_words\n",
    "    for stop in stopwords.words('english'):\n",
    "        stop_words.append(stop.replace(\"'\",\"\"))\n",
    "\n",
    "    #print(\"Stop Words: \")\n",
    "    print(stop_words)\n",
    "    print (\"Processing Stop Words\")\n",
    "    input_series = apply_with_progress(input_series, apply_stop_words)\n",
    "    \n",
    "    print(f\"Stop Words Time: {time.time() - start_time}\")\n",
    "    \n",
    "    return input_series\n",
    "\n",
    "def apply_stemmer(input_series,trim_single_words = True,slices=100):\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "\n",
    "    #//*** Instantiate the Stemmer\n",
    "    porter = nltk.stem.porter.PorterStemmer()\n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    #//*** 1.) Apply() an action to each row\n",
    "    #//*** 2.) lambda word_list, each row is treated as word_list for the subsequent expression\n",
    "    #//*** 3.) The base [ word for word in wordlist] would return each word in word_list as a list. \n",
    "    #//*** 4.) [porter.stem(word) for word in word_list] - performs stemming on each word and returns a list\n",
    "    #input_series = input_series.apply(lambda word_list: [porter.stem(word) for word in word_list] )\n",
    "    print(\"Begin: Apply Stemmer\")\n",
    "    input_series = apply_with_progress(input_series, lambda word_list: [porter.stem(word) for word in word_list],slices)\n",
    "    \n",
    "    #//*** Remove Single letter words after stemming\n",
    "    \n",
    "    \"\"\"\n",
    "    if trim_single_words:\n",
    "        for word_list in input_series:\n",
    "            for word in word_list:\n",
    "                if len(word) < 2:\n",
    "                    word_list.remove(word)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Apply Stemmer Time: {time.time() - start_time}\")\n",
    "    return input_series\n",
    "\n",
    "def apply_pos_tag(input_series,slices=100):\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "\n",
    "    from nltk import pos_tag\n",
    "\n",
    "    #//pos_tag requires an additional download\n",
    "    try:\n",
    "        pos_tag([\"the\",\"quick\",\"brown\",\"fox\"])\n",
    "    except: \n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    print(\"Begin Part of Speech tagging\")\n",
    "    \n",
    "    input_series = apply_with_progress(input_series,pos_tag,slices)\n",
    "    \n",
    "    print(f\"Part of Speech Tagging Time: {round(time.time() - start_time,2)}s\")\n",
    "    \n",
    "    return input_series\n",
    "    \n",
    "def apply_lemmatization(input_series,slices=20):\n",
    "            \n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    from nltk.corpus import wordnet    \n",
    "    \n",
    "    #nltk.download('wordnet')\n",
    "    \n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    # Initialize the Lemmatizer instance\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    \n",
    "    #//*** 1.) Apply() an action to each row\n",
    "    #//*** 2.) lambda word_list, each row is treated as word_list for the subsequent expression\n",
    "    #//*** 3.) The base [ word for word in wordlist] would return each word in word_list as a list. \n",
    "    #//*** 4.) [lemmatizer.lemmatize(word) for word in word_list] - performs lemmtization on each word and returns a list\n",
    "    #lemmatized = input_series.apply(lambda word_list: [lemmatizer.lemmatize(*word) for word in word_list] )\n",
    "    \n",
    "    print(\"Begin Lemmatization...\")\n",
    "    \n",
    "    input_series = apply_with_progress(input_series,lambda word_list: [lemmatizer.lemmatize(word) for word in word_list],20)\n",
    "    \n",
    "    print(f\"Lemmatization Time: {time.time() - start_time}\")\n",
    "    \n",
    "    #if detoken:\n",
    "    #    return tokenize_series(input_series,5,{\"fast\":True})\n",
    "\n",
    "    return input_series\n",
    "\n",
    "#//*** Apply a function to a Series and display processing progress\n",
    "#//*** Slices is the total number of intervals to report progress.\n",
    "#//*** Slices = 20 displays processing after every 5% is processed\n",
    "#//*** Slices = 100 displays processing after every 1% is processed\n",
    "def apply_with_progress(input_series,input_function,slices=20):\n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Get the time at the start of the loop, used for elapsed time.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** The interval is the number of elements to process in each Loop. The default is 20.\n",
    "    #//*** Which displays results at 5% intervals.\n",
    "    interval = int(len(input_series)/slices)\n",
    "    \n",
    "    #//*** Total number of items to process\n",
    "    total = len(input_series)\n",
    "    \n",
    "\n",
    "    #//*** Loop through slice times and display processing statistics for each slice.\n",
    "    for x in range(0, slices ):\n",
    "        #//*** Get time at the start of the slice.\n",
    "        loop_time = time.time()\n",
    "        \n",
    "        #//*** Set the start index\n",
    "        begin_dex = interval*x\n",
    "        \n",
    "        #//*** Set the end index\n",
    "        end_dex = interval*x+interval-1\n",
    "        \n",
    "        #//*** Apply the input function to a slice of the input_series\n",
    "        #//*** This part does all the actual 'work'\n",
    "        input_series[begin_dex:end_dex] = input_series[begin_dex:end_dex].apply(input_function)\n",
    "        \n",
    "        #//*** Get the time after the slice of work is done\n",
    "        now = time.time()\n",
    "        \n",
    "        #//*** Compute the estimated remaining time\n",
    "        #//*** Total elapsed time / % of completed slices = Estimated total time\n",
    "        #//*** Estimated total time - elaped time = Remaining time\n",
    "        est_remain = round( ( ( now - start_time ) /  ( (x+1)/slices ) - (now-start_time)),2)\n",
    "\n",
    "        #//*** Display Results so we know how much time is left (so we can effectively multi-task: ie comments, research and Valheim)\n",
    "        print(f\"Processed {x}/{slices}: {begin_dex}:{end_dex} [{total}] in {round(now-loop_time,2)}s elapsed: {round(now-start_time,2)}s est Remain: {est_remain}s\")\n",
    "    \n",
    "    #//*** END For Slice Loop\n",
    "    \n",
    "    #//*** Process the remaining values (Since interval is an int there should be a remainder)\n",
    "    loop_time = time.time()\n",
    "    begin_dex = end_dex+1\n",
    "    if begin_dex < len(input_series):\n",
    "        print(f\"Processing Remaining values: {begin_dex} : {total} \")\n",
    "        #print(input_series[begin_dex:])\n",
    "        input_series[begin_dex:] = input_series[begin_dex:].apply(input_function)\n",
    "    \n",
    "    #//*** Display Final output\n",
    "    print(f\"Processed {slices}/{slices}: {begin_dex}:{end_dex} [{total}] in {round(time.time()-loop_time,2)}s elapsed: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "    #//*** return Series\n",
    "    return input_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\stonk013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Part of Speech tagging\n",
      "Processed 0/100: 0:8740 [874139] in 95.56s elapsed: 95.56s est Remain: 9460.25s\n",
      "Processed 1/100: 8741:17481 [874139] in 89.77s elapsed: 185.33s est Remain: 9081.01s\n",
      "Processed 2/100: 17482:26222 [874139] in 88.65s elapsed: 273.98s est Remain: 8858.64s\n",
      "Processed 3/100: 26223:34963 [874139] in 93.18s elapsed: 367.16s est Remain: 8811.72s\n",
      "Processed 4/100: 34964:43704 [874139] in 95.91s elapsed: 463.06s est Remain: 8798.14s\n",
      "Processed 5/100: 43705:52445 [874139] in 91.39s elapsed: 554.45s est Remain: 8686.34s\n",
      "Processed 6/100: 52446:61186 [874139] in 87.11s elapsed: 641.56s est Remain: 8523.58s\n",
      "Processed 7/100: 61187:69927 [874139] in 91.08s elapsed: 732.64s est Remain: 8425.39s\n",
      "Processed 8/100: 69928:78668 [874139] in 93.17s elapsed: 825.81s est Remain: 8349.83s\n",
      "Processed 9/100: 78669:87409 [874139] in 93.71s elapsed: 919.52s est Remain: 8275.64s\n",
      "Processed 10/100: 87410:96150 [874139] in 92.12s elapsed: 1011.63s est Remain: 8185.04s\n",
      "Processed 11/100: 96151:104891 [874139] in 88.93s elapsed: 1100.56s est Remain: 8070.8s\n",
      "Processed 12/100: 104892:113632 [874139] in 87.2s elapsed: 1187.76s est Remain: 7948.88s\n",
      "Processed 13/100: 113633:122373 [874139] in 89.22s elapsed: 1276.99s est Remain: 7844.35s\n",
      "Processed 14/100: 122374:131114 [874139] in 94.6s elapsed: 1371.59s est Remain: 7772.34s\n",
      "Processed 15/100: 131115:139855 [874139] in 91.86s elapsed: 1463.46s est Remain: 7683.15s\n",
      "Processed 16/100: 139856:148596 [874139] in 87.23s elapsed: 1550.69s est Remain: 7571.01s\n",
      "Processed 17/100: 148597:157337 [874139] in 89.11s elapsed: 1639.8s est Remain: 7470.2s\n",
      "Processed 18/100: 157338:166078 [874139] in 88.28s elapsed: 1728.08s est Remain: 7367.09s\n",
      "Processed 19/100: 166079:174819 [874139] in 94.07s elapsed: 1822.15s est Remain: 7288.6s\n",
      "Processed 20/100: 174820:183560 [874139] in 89.88s elapsed: 1912.03s est Remain: 7192.87s\n",
      "Processed 21/100: 183561:192301 [874139] in 92.96s elapsed: 2004.99s est Remain: 7108.61s\n",
      "Processed 22/100: 192302:201042 [874139] in 88.43s elapsed: 2093.42s est Remain: 7008.42s\n",
      "Processed 23/100: 201043:209783 [874139] in 92.15s elapsed: 2185.58s est Remain: 6920.99s\n",
      "Processed 24/100: 209784:218524 [874139] in 85.69s elapsed: 2271.26s est Remain: 6813.79s\n",
      "Processed 25/100: 218525:227265 [874139] in 85.65s elapsed: 2356.91s est Remain: 6708.13s\n",
      "Processed 26/100: 227266:236006 [874139] in 83.84s elapsed: 2440.75s est Remain: 6599.06s\n",
      "Processed 27/100: 236007:244747 [874139] in 77.22s elapsed: 2517.97s est Remain: 6474.78s\n",
      "Processed 28/100: 244748:253488 [874139] in 57.08s elapsed: 2575.05s est Remain: 6304.42s\n",
      "Processed 29/100: 253489:262229 [874139] in 42.02s elapsed: 2617.06s est Remain: 6106.48s\n",
      "Processed 30/100: 262230:270970 [874139] in 57.55s elapsed: 2674.61s est Remain: 5953.17s\n",
      "Processed 31/100: 270971:279711 [874139] in 80.2s elapsed: 2754.82s est Remain: 5853.99s\n",
      "Processed 32/100: 279712:288452 [874139] in 86.55s elapsed: 2841.37s est Remain: 5768.83s\n",
      "Processed 33/100: 288453:297193 [874139] in 90.52s elapsed: 2931.88s est Remain: 5691.3s\n",
      "Processed 34/100: 297194:305934 [874139] in 94.37s elapsed: 3026.25s est Remain: 5620.19s\n",
      "Processed 35/100: 305935:314675 [874139] in 94.25s elapsed: 3120.5s est Remain: 5547.56s\n",
      "Processed 36/100: 314676:323416 [874139] in 88.59s elapsed: 3209.1s est Remain: 5464.14s\n",
      "Processed 37/100: 323417:332157 [874139] in 86.02s elapsed: 3295.12s est Remain: 5376.25s\n",
      "Processed 38/100: 332158:340898 [874139] in 93.81s elapsed: 3388.93s est Remain: 5300.64s\n",
      "Processed 39/100: 340899:349639 [874139] in 87.78s elapsed: 3476.72s est Remain: 5215.07s\n",
      "Processed 40/100: 349640:358380 [874139] in 90.69s elapsed: 3567.41s est Remain: 5133.59s\n",
      "Processed 41/100: 358381:367121 [874139] in 90.32s elapsed: 3657.73s est Remain: 5051.16s\n",
      "Processed 42/100: 367122:375862 [874139] in 84.24s elapsed: 3741.97s est Remain: 4960.29s\n",
      "Processed 43/100: 375863:384603 [874139] in 83.21s elapsed: 3825.18s est Remain: 4868.41s\n",
      "Processed 44/100: 384604:393344 [874139] in 91.44s elapsed: 3916.62s est Remain: 4786.98s\n",
      "Processed 45/100: 393345:402085 [874139] in 90.24s elapsed: 4006.86s est Remain: 4703.7s\n",
      "Processed 46/100: 402086:410826 [874139] in 88.21s elapsed: 4095.06s est Remain: 4617.84s\n",
      "Processed 47/100: 410827:419567 [874139] in 90.21s elapsed: 4185.28s est Remain: 4534.05s\n",
      "Processed 48/100: 419568:428308 [874139] in 89.32s elapsed: 4274.6s est Remain: 4449.07s\n",
      "Processed 49/100: 428309:437049 [874139] in 91.56s elapsed: 4366.15s est Remain: 4366.15s\n",
      "Processed 50/100: 437050:445790 [874139] in 91.45s elapsed: 4457.6s est Remain: 4282.79s\n",
      "Processed 51/100: 445791:454531 [874139] in 86.99s elapsed: 4544.59s est Remain: 4195.01s\n",
      "Processed 52/100: 454532:463272 [874139] in 88.5s elapsed: 4633.09s est Remain: 4108.59s\n",
      "Processed 53/100: 463273:472013 [874139] in 84.06s elapsed: 4717.15s est Remain: 4018.31s\n",
      "Processed 54/100: 472014:480754 [874139] in 76.7s elapsed: 4793.85s est Remain: 3922.24s\n",
      "Processed 55/100: 480755:489495 [874139] in 89.28s elapsed: 4883.13s est Remain: 3836.74s\n",
      "Processed 56/100: 489496:498236 [874139] in 78.74s elapsed: 4961.87s est Remain: 3743.16s\n",
      "Processed 57/100: 498237:506977 [874139] in 77.07s elapsed: 5038.93s est Remain: 3648.88s\n",
      "Processed 58/100: 506978:515718 [874139] in 42.25s elapsed: 5081.18s est Remain: 3530.99s\n",
      "Processed 59/100: 515719:524459 [874139] in 83.63s elapsed: 5164.81s est Remain: 3443.21s\n",
      "Processed 60/100: 524460:533200 [874139] in 82.94s elapsed: 5247.75s est Remain: 3355.12s\n",
      "Processed 61/100: 533201:541941 [874139] in 85.04s elapsed: 5332.79s est Remain: 3268.48s\n",
      "Processed 62/100: 541942:550682 [874139] in 81.11s elapsed: 5413.9s est Remain: 3179.59s\n",
      "Processed 63/100: 550683:559423 [874139] in 86.41s elapsed: 5500.31s est Remain: 3093.92s\n",
      "Processed 64/100: 559424:568164 [874139] in 87.85s elapsed: 5588.16s est Remain: 3009.01s\n",
      "Processed 65/100: 568165:576905 [874139] in 82.6s elapsed: 5670.76s est Remain: 2921.3s\n",
      "Processed 66/100: 576906:585646 [874139] in 85.33s elapsed: 5756.09s est Remain: 2835.09s\n",
      "Processed 67/100: 585647:594387 [874139] in 78.79s elapsed: 5834.88s est Remain: 2745.83s\n",
      "Processed 68/100: 594388:603128 [874139] in 55.03s elapsed: 5889.91s est Remain: 2646.19s\n",
      "Processed 69/100: 603129:611869 [874139] in 82.46s elapsed: 5972.37s est Remain: 2559.59s\n",
      "Processed 70/100: 611870:620610 [874139] in 88.36s elapsed: 6060.73s est Remain: 2475.51s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-09066b2f4be5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pos_tag'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'txt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-7773f9ec19c6>\u001b[0m in \u001b[0;36mapply_pos_tag\u001b[1;34m(input_series, slices)\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Begin Part of Speech tagging\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m     \u001b[0minput_series\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_with_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_series\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Part of Speech Tagging Time: {round(time.time() - start_time,2)}s\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-7773f9ec19c6>\u001b[0m in \u001b[0;36mapply_with_progress\u001b[1;34m(input_series, input_function, slices)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;31m#//*** Apply the input function to a slice of the input_series\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[1;31m#//*** This part does all the actual 'work'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m         \u001b[0minput_series\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbegin_dex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend_dex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_series\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbegin_dex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend_dex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m         \u001b[1;31m#//*** Get the time after the slice of work is done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4198\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4200\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \"\"\"\n\u001b[0;32m    160\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[0;32m    116\u001b[0m         )\n\u001b[0;32m    117\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtagged_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Maps to the specified tagset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"eng\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m                 \u001b[0mtag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_conf\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, features, return_conf)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m# Do a secondary alphabetic sort, for stability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['pos_tag'] = apply_pos_tag(df['txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Neural Network Classifier with Keras###\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using Keras. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classifying Images ###\n",
    "In chapter 20 of the Machine Learning with Python Cookbook, implement the code found in section 20.15 classify MSINT images using a convolutional neural network. Report the accuracy of your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
