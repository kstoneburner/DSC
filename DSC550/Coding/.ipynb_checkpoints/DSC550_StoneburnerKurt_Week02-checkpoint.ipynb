{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stoneburner, Kurt\n",
    "- ## DSC 550 - Week 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//**** Project imports.\n",
    "#//*** The nltk libraries involve additional downloads. The Try blocks automatically download the content if it's not\n",
    "#//*** present. This feels like good form and being a digital nomad, it should run on whichever workstation I happen\n",
    "#//*** to be on.\n",
    "import os\n",
    "import sys\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import time\n",
    "\n",
    "#//*** nltk - Natural Language toolkit\n",
    "import nltk\n",
    "\n",
    "#//**** Requires the punkt module. Download if it doesn't exist\n",
    "try:\n",
    "    type(nltk.punkt)\n",
    "except:\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#//*** Stopwords requires an additional download\n",
    "try:\n",
    "    type(stopwords)\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "#//pos_tag requires an additional download\n",
    "\n",
    "try:\n",
    "    pos_tag([\"the\",\"quick\",\"brown\",\"fox\"])\n",
    "except: \n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#//*** Convenience function to \n",
    "#//*** Take a time value and display the difference\n",
    "#//*** Return the difference\n",
    "def cum_time(input_time):\n",
    "    tot_time = round(time.time() - input_time,2)\n",
    "    \n",
    "    print(f\"Process Time: {int(tot_time/60)}m {tot_time % 60}s\")\n",
    "    \n",
    "    return tot_time\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Exercise: Build Your Text Classifiers ###\n",
    "\n",
    "**1. You can find the dataset controversial-comments.jsonl for this exercise in the Weekly Resources: Week 2 Data Files.**\n",
    "\n",
    "Pre-processing Text: For this part, you will start by reading the controversial-comments.jsonl file into a DataFrame. Then,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Time: 0m 5.79s\n"
     ]
    }
   ],
   "source": [
    "#//************************************************************************************************************************\n",
    "#//*** In hindisght a little research would have revealed I could have used pd.read_json() with the lines=True attribute.\n",
    "#//*** Instead, I borrowed code from my previous projects and parsed each line into a list dictionary.\n",
    "#//************************************************************************************************************************\n",
    "#//*** Read the file line by line\n",
    "#//*** Parse each line of JSON. Parse each Key / Value pair. Each value is appeneded to a list. The lists are managed\n",
    "#//*** with tdict[key]. As long as the input file has the same number of keys for each line, then this works.\n",
    "#//*** Not sure what the canonical method is for converting items into a dataframe. But this technique has worked well\n",
    "#//*** in DSC530 and DSC540.\n",
    "#//************************************************************************************************************************\n",
    "\n",
    "#//*** Temporary Dictionary\n",
    "tdict = {}\n",
    "\n",
    "#//*** Start Timing the process\n",
    "start_time = time.time()\n",
    "total_job_time = 0\n",
    "\n",
    "#//*** Read JSON into lists based on keys.\n",
    "with open('z_controversial-comments.jsonl', \"r\") as f:\n",
    "    \n",
    "    #//*** Initialize tdict. Each Key is used in both the JSON and tdict. This works on JSON of any length but is\n",
    "    #//*** limited to a flat construct. It works well for 2-D arrays.\n",
    "    #//*** 1.) Read the first line of the file\n",
    "    #//*** 2.) Convert the first line of JSON to a dictionary\n",
    "    #//*** 3.) Get each key/value in dictionary items\n",
    "    for key,value in json.loads(f.readline()).items():\n",
    "            #//*** Initialize a list of value, using tdict[key]\n",
    "            tdict[key] = [value]\n",
    "    \n",
    "    #//*** Process each remaining lines.\n",
    "    for line in f:\n",
    "        \n",
    "        #//*** 1.) Convert each line to a dictionary\n",
    "        #//*** 2.) get each key/value in dictionary\n",
    "        for key,value in json.loads(line).items():\n",
    "            \n",
    "            #//*** Add Value to the list associated with tdict[key]\n",
    "            tdict[key].append(value)\n",
    "            \n",
    "#//*** Initialize a new dataframe\n",
    "con_df = pd.DataFrame()\n",
    "\n",
    "#//*** Loop through tdict, add each key as a column with value as the column data\n",
    "for key,value in tdict.items():\n",
    "    con_df[key] = value\n",
    "\n",
    "#//*** Delete tdict. It is unused and a 200mb+ object\n",
    "del tdict\n",
    "\n",
    "#//*** Display process time and keep a running time of all step 1 processes\n",
    "total_job_time += cum_time(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Convert all text to lowercase letters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Time: 0m 0.83s\n"
     ]
    }
   ],
   "source": [
    "#//*** Start Timing the process\n",
    "start_time = time.time()\n",
    "\n",
    "#//*** Convert to lower case\n",
    "con_df['txt'] = con_df['txt'].str.lower()\n",
    "\n",
    "#//*** Display process time and keep a running time of all step 1 processes\n",
    "total_job_time += cum_time(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Remove all punctuation from the text.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Time: 0m 12.12s\n"
     ]
    }
   ],
   "source": [
    "#//*** Start Timing the process\n",
    "start_time = time.time()\n",
    "\n",
    "#//*** Remove new lines, I didn't see any samples of \\r\\n. But it is common enough. Replace it if it exists\n",
    "con_df['txt'] = con_df['txt'].str.replace(r'\\r?\\n',\"\")\n",
    "\n",
    "#//*** Remove plain ]n new lines\n",
    "con_df['txt'] = con_df['txt'].str.replace(r'\\n',\"\")\n",
    "\n",
    "#//*** Remove html entities, observed entities are &gt; and &lt;. All HTML entities begin with & and end with ;.\n",
    "#//*** Let's use regex to remove html entities\n",
    "con_df['txt'] = con_df['txt'].str.replace(r'&.*;',\"\")\n",
    "\n",
    "#//*** Remove elements flagged as [removed]\n",
    "con_df['txt'] = con_df['txt'].str.replace(r'\\[removed\\]',\"\")\n",
    "\n",
    "#//*** Remove elements flagged as [deleted]\n",
    "con_df['txt'] = con_df['txt'].str.replace(r'\\[deleted\\]',\"\")\n",
    "\n",
    "#//*** Some text should be empty with the removal of [removed] and [deleted]\n",
    "#//*** Remove the empty text\n",
    "con_df = con_df[ con_df['txt'].str.len() > 0]\n",
    "\n",
    "#//*** Remove punctuation using the example from the book\n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P') )\n",
    "con_df['txt'] = con_df['txt'].str.translate(punctuation)\n",
    "\n",
    "#//*** Display process time and keep a running time of all step 1 processes\n",
    "total_job_time += cum_time(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Remove stop words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Time: 2m 46.28s\n"
     ]
    }
   ],
   "source": [
    "#//*** Start Timing the process\n",
    "start_time = time.time()\n",
    "\n",
    "#//*** Tokenize conf_df['txt']\n",
    "#//*** This Takes a wee bit to run\n",
    "con_df['txt'] = con_df['txt'].apply(word_tokenize)\n",
    "\n",
    "#//*** Display process time and keep a running time of all step 1 processes\n",
    "total_job_time += cum_time(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Time: 0m 32.5s\n"
     ]
    }
   ],
   "source": [
    "#//*** Start Timing the process\n",
    "start_time = time.time()\n",
    "\n",
    "#//*** I'm not pythonic enough to do this on one line.\n",
    "#//*** This function removes stop_words from a list.\n",
    "#//*** Works with dataframe.apply()\n",
    "def remove_stop_words(input_list):\n",
    "    #//*** Load Stopwords   \n",
    "    for word in input_list:\n",
    "        if word in stop_words:\n",
    "            input_list.remove(word)\n",
    "    return input_list\n",
    "\n",
    "#//*** The stop_words include punctuation. Stop Word Contractions will not be filtered out.\n",
    "stop_words = []\n",
    "\n",
    "#//*** Remove apostrophies from the stop_words\n",
    "for stop in stopwords.words('english'):\n",
    "    stop_words.append(stop.replace(\"'\",\"\"))\n",
    "\n",
    "#//*** Remove Stop words from the tokenized strings in the 'process' column\n",
    "con_df['txt'] = con_df['txt'].apply(remove_stop_words)\n",
    "\n",
    "#//*** If I was cool, I'd do an additional filter pass and remove any Tokenized words with a length of 1\n",
    "#//*** Then do a second pass and remove rows that have no items.\n",
    "\n",
    "#//*** Display process time and keep a running time of all step 1 processes\n",
    "total_job_time += cum_time(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D. Apply NLTKâ€™s PorterStemmer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439    [bill, fuck, bagel, it, been, long, of, toaste...\n",
      "440                          [ucuteman, got, rekt, lulz]\n",
      "442    [dont, think, anyone, cares, his, business, ma...\n",
      "443    [mean, hes, going, take, away, spouses, health...\n",
      "444    [wanting, argue, something, did, past, make, d...\n",
      "445                       [long, we, get, keep, chicago]\n",
      "446    [everytime, say, doubt, that, actually, adult,...\n",
      "447    [working, momsnever, underestimate, stupid, le...\n",
      "448    [you, put, effort, your, first, response, conv...\n",
      "449    [1, dont, post, positive, trump, news, instead...\n",
      "450    [ive, saying, thinking, for, year, its, depres...\n",
      "452                                [subreddit, terrible]\n",
      "453    [mitt, romney, so, right, once, guy, in, offic...\n",
      "454    [you, dense, enough, be, taking, comments, sec...\n",
      "455    [think, boils, not, understanding, lobbysts, t...\n",
      "456    [that, true, cant, electors, vote, without, po...\n",
      "457    [lotta, research, that, done, thedonald, trump...\n",
      "458      [especially, it, a, 17, year, old, girl, asked]\n",
      "459    [here, see, racist, north, carolina, bigots, b...\n",
      "460    [thats, basically, it, came, do, live, texas, ...\n",
      "Name: txt, dtype: object\n",
      "439    [bill, fuck, bagel, it, been, long, of, toaste...\n",
      "440                          [ucuteman, got, rekt, lulz]\n",
      "442    [dont, think, anyon, care, hi, busi, manag, th...\n",
      "443    [mean, he, go, take, away, spous, healthcar, b...\n",
      "444    [want, argu, someth, did, past, make, disquali...\n",
      "445                       [long, we, get, keep, chicago]\n",
      "446    [everytim, say, doubt, that, actual, adult, so...\n",
      "447    [work, momsnev, underestim, stupid, leftist, r...\n",
      "448    [you, put, effort, your, first, respons, conve...\n",
      "449    [1, dont, post, posit, trump, news, instead, w...\n",
      "450    [ive, say, think, for, year, it, depress, me, ...\n",
      "452                                 [subreddit, terribl]\n",
      "453    [mitt, romney, so, right, onc, guy, in, offic,...\n",
      "454    [you, dens, enough, be, take, comment, section...\n",
      "455    [think, boil, not, understand, lobbyst, there,...\n",
      "456    [that, true, cant, elector, vote, without, pop...\n",
      "457    [lotta, research, that, done, thedonald, trump...\n",
      "458            [especi, it, a, 17, year, old, girl, ask]\n",
      "459    [here, see, racist, north, carolina, bigot, ba...\n",
      "460    [that, basic, it, came, do, live, texa, have, ...\n",
      "Name: txt, dtype: object\n",
      "Process Time: 7m 34.129999999999995s\n"
     ]
    }
   ],
   "source": [
    "#//*** Start Timing the process\n",
    "start_time = time.time()\n",
    "\n",
    "#/*** Create Stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "#//*** Pre stemming sample\n",
    "print(con_df['txt'][400:420])\n",
    "\n",
    "#//*** It's a pythonic answer\n",
    "#//*** 1.) Apply() an action to each row\n",
    "#//*** 2.) lambda word_list, each row is treated as word_list for the subsequent expression\n",
    "#//*** 3.) The base [ word for word in wordlist] would return each word in word_list as a list. \n",
    "#//*** 4.) [porter.stem(word) for word in word_list] - performs stemming on each word and returns a list\n",
    "con_df['txt'] = con_df['txt'].apply(lambda word_list: [porter.stem(word) for word in word_list] )\n",
    "\n",
    "#//*** post stemming sample\n",
    "print(con_df['txt'][400:420])\n",
    "\n",
    "#//*** Display process time and keep a running time of all step 1 processes\n",
    "total_job_time += cum_time(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Step One Job Time: 11m 11.649999999999977s \n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Step One Job Time: {int(total_job_time/60)}m {total_job_time % 60}s \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Save the Dataframe to a CSV to speed up later processing sessions.\n",
    "#//*** This ended up being a huge time sink. \n",
    "#//*** This is commented for safety. It is uncommented when needed\n",
    "\n",
    "#con_df.to_csv(\"z_wk02_controversial_words_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Now that the data is pre-processed, you will apply three different techniques to get it into a usable form for model-building. Apply each of the following steps (individually) to the pre-processed data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Process Time: 2m 16.03s\n"
     ]
    }
   ],
   "source": [
    "#//*** Start Timing the process\n",
    "start_time = time.time()\n",
    "total_job_time = 0\n",
    "\n",
    "#//*** For Step 2 We'll start from the CSV\n",
    "con_df = pd.read_csv(\"z_wk02_controversial_words_df.csv\")\n",
    "\n",
    "#//*** Delete the first column, For some reason CSV likes to make the index also the first column. \n",
    "con_df.pop(con_df.columns[0])\n",
    "\n",
    "#//*** In hindsight it is better to not move into and out of a CSV. It would have been a more efficient use of time\n",
    "#//*** To just reprocess the data for every session.\n",
    "#//*** The Tokenized words are stored to the CSV as a literal string representation of a list.\n",
    "#//*** remove the list syntax to get a string\n",
    "#//*** This process could have been improved by detokenizing before writing to CSV\n",
    "#//*** I kept with this process since I'm fully invested in the sunken cost fallacy.\n",
    "\n",
    "con_df['txt'] = con_df['txt'].str.replace(\"[\",\"\").str.replace(\"]\",\"\").str.replace(\",\",\"\").str.replace(\"'\",\"\")\n",
    "\n",
    "#//*** Create a tokenized columm for section 2B\n",
    "con_df['token'] = con_df['txt'].apply(word_tokenize)\n",
    "\n",
    "print()\n",
    "\n",
    "#//*** Display the Process Time\n",
    "cum_time(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Import Section 2 libraries.\n",
    "#//*** This is ill form for standard python. But feels appropriate for Notebooks.\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Convert each text entry into a word-count vector (see sections 5.3 & 6.8 in the Machine Learning with Python Cookbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           (0, 341165)\\t1\\n  (0, 127813)\\t1\\n  (0, 1342...\n",
      "1           (0, 341165)\\t1\\n  (0, 127813)\\t1\\n  (0, 1342...\n",
      "2           (0, 341165)\\t1\\n  (0, 127813)\\t1\\n  (0, 1342...\n",
      "3           (0, 341165)\\t1\\n  (0, 127813)\\t1\\n  (0, 1342...\n",
      "4           (0, 341165)\\t1\\n  (0, 127813)\\t1\\n  (0, 1342...\n",
      "                                ...                        \n",
      "874134      (0, 341165)\\t1\\n  (0, 127813)\\t1\\n  (0, 1342...\n",
      "874135      (0, 341165)\\t1\\n  (0, 127813)\\t1\\n  (0, 1342...\n",
      "874136      (0, 341165)\\t1\\n  (0, 127813)\\t1\\n  (0, 1342...\n",
      "874137      (0, 341165)\\t1\\n  (0, 127813)\\t1\\n  (0, 1342...\n",
      "874138      (0, 341165)\\t1\\n  (0, 127813)\\t1\\n  (0, 1342...\n",
      "Name: bow, Length: 874139, dtype: object\n",
      "Process Time: 0m 22.38s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#//*** Start Timing the process\n",
    "start_time = time.time()\n",
    "\n",
    "#//*** Create the bag of words feature matrix\n",
    "#//*** Initialize a new instance of the Counter Vectorizer\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "#//*** Vectorize the word count into a bag of words column\n",
    "#//*** This is awesome in the sense that so much work is abstracted behind a single line of code.\n",
    "#//*** I don't quite understand what is going on here. I'm assuming we will build upon this work and clarity\n",
    "#//*** (or confounding) will come later.\n",
    "\n",
    "con_df['bow'] = count_vector.fit_transform(con_df['txt'])\n",
    "\n",
    "print(con_df['bow'])\n",
    "\n",
    "#//*** Display the Process Time\n",
    "cum_time(start_time)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get frustrat reason want it way becaus foundat more complex problem they advanc grade can get decent grade an sat type test math i realli understand lot mathemat way get the right answer lot time can figur a common sens work around a lot the question i would ill prepar take colleg level math cours despit the abov averag math score theyr just tri to bust the kid ball\n",
      "  (0, 341165)\t1\n",
      "  (0, 127813)\t1\n",
      "  (0, 134216)\t3\n",
      "  (0, 283442)\t2\n",
      "  (0, 313271)\t1\n",
      "  (0, 42431)\t1\n",
      "  (0, 162200)\t1\n",
      "  (0, 217710)\t1\n",
      "  (0, 93569)\t1\n",
      "  (0, 320954)\t1\n",
      "  (0, 348493)\t1\n",
      "  (0, 113660)\t1\n",
      "  (0, 326408)\t1\n",
      "  (0, 170067)\t1\n",
      "  (0, 160770)\t1\n",
      "  (0, 253172)\t2\n",
      "  (0, 132834)\t1\n",
      "  (0, 215996)\t1\n",
      "  (0, 74674)\t1\n",
      "  (0, 337416)\t1\n",
      "  (1, 32283)\t1\n",
      "  (1, 261894)\t1\n",
      "  (1, 204312)\t1\n",
      "  (1, 240809)\t1\n",
      "  (2, 133807)\t2\n",
      "  :\t:\n",
      "  (874136, 88880)\t1\n",
      "  (874136, 344351)\t1\n",
      "  (874136, 248570)\t1\n",
      "  (874136, 135318)\t1\n",
      "  (874136, 213820)\t1\n",
      "  (874136, 39375)\t1\n",
      "  (874136, 326775)\t1\n",
      "  (874136, 257601)\t1\n",
      "  (874136, 162589)\t1\n",
      "  (874136, 225432)\t1\n",
      "  (874136, 27998)\t1\n",
      "  (874136, 230873)\t1\n",
      "  (874137, 30521)\t1\n",
      "  (874137, 107968)\t1\n",
      "  (874137, 222113)\t1\n",
      "  (874138, 261894)\t1\n",
      "  (874138, 112672)\t1\n",
      "  (874138, 352762)\t1\n",
      "  (874138, 119971)\t1\n",
      "  (874138, 237838)\t1\n",
      "  (874138, 274667)\t1\n",
      "  (874138, 58219)\t1\n",
      "  (874138, 350560)\t1\n",
      "  (874138, 177796)\t1\n",
      "  (874138, 282699)\t1\n"
     ]
    }
   ],
   "source": [
    "#//*** Pick a line for verification. \n",
    "#//*** We can verify one word get is used 3x times. Which matches the results. This test is by no means definitive\n",
    "#//*** but it feels 'right'. This is a cursory check opposed to a rigourous one.\n",
    "print(con_df['txt'].iloc[3])\n",
    "print(con_df['bow'].iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Convert each text entry into a part-of-speech tag vector (see section 6.7 in the Machine Learning with Python Cookbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         [(well, RB), (great, JJ), (he, PRP), (someth, ...\n",
      "1         [(are, VBP), (right, JJ), (mr, NN), (presid, NN)]\n",
      "2         [(have, VB), (given, VBN), (input, JJ), (apart...\n",
      "3         [(get, VB), (frustrat, JJ), (reason, NN), (wan...\n",
      "4         [(am, VBP), (far, RB), (expert, JJ), (tpp, NN)...\n",
      "                                ...                        \n",
      "874134               [(payer, NN), (immort, NN), (all, DT)]\n",
      "874135    [(genuin, NN), (cant, NN), (understand, NN), (...\n",
      "874136    [(remind, NN), (subreddit, NN), (for, IN), (ci...\n",
      "874137      [(k, NN), (explain, NN), (or, CC), (anyth, NN)]\n",
      "874138    [(ya, JJ), (sociopath, NN), (known, VBN), (cel...\n",
      "Name: pos_tag, Length: 874139, dtype: object\n",
      "Process Time: 33m 26.450000000000045s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2006.45"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#//*** Start Timing the process\n",
    "start_time = time.time()\n",
    "\n",
    "#//*** Applying the speech tag vector is easy to code, and really hard on the CPU cycles.\n",
    "#//*** Runs the part-of-speech tag against the tokenized data.\n",
    "con_df['pos_tag'] = con_df['token'].apply(pos_tag)\n",
    "\n",
    "print(con_df['pos_tag'])\n",
    "\n",
    "#//*** Display the Process Time\n",
    "cum_time(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Convert each entry into a term frequency-inverse document frequency (tfidf) vector (see section 6.9 in the Machine Learning with Python Cookbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Time: 0m 21.37s\n",
      "0           (0, 337416)\\t0.18676189328943824\\n  (0, 7467...\n",
      "1           (0, 337416)\\t0.18676189328943824\\n  (0, 7467...\n",
      "2           (0, 337416)\\t0.18676189328943824\\n  (0, 7467...\n",
      "3           (0, 337416)\\t0.18676189328943824\\n  (0, 7467...\n",
      "4           (0, 337416)\\t0.18676189328943824\\n  (0, 7467...\n",
      "                                ...                        \n",
      "874134      (0, 337416)\\t0.18676189328943824\\n  (0, 7467...\n",
      "874135      (0, 337416)\\t0.18676189328943824\\n  (0, 7467...\n",
      "874136      (0, 337416)\\t0.18676189328943824\\n  (0, 7467...\n",
      "874137      (0, 337416)\\t0.18676189328943824\\n  (0, 7467...\n",
      "874138      (0, 337416)\\t0.18676189328943824\\n  (0, 7467...\n",
      "Name: tfidf, Length: 874139, dtype: object\n",
      "  (0, 337416)\t0.18676189328943824\n",
      "  (0, 74674)\t0.4725873408381811\n",
      "  (0, 215996)\t0.15491479249290988\n",
      "  (0, 132834)\t0.21614319633286316\n",
      "  (0, 253172)\t0.27581616050694946\n",
      "  (0, 160770)\t0.12617802969160016\n",
      "  (0, 170067)\t0.10295162508673708\n",
      "  (0, 326408)\t0.2751599031144539\n",
      "  (0, 113660)\t0.1965960672694154\n",
      "  (0, 348493)\t0.11347019939682441\n",
      "  (0, 320954)\t0.09599694376739182\n",
      "  (0, 93569)\t0.20383253491015463\n",
      "  (0, 217710)\t0.18344565581383973\n",
      "  (0, 162200)\t0.13167670433152084\n",
      "  (0, 42431)\t0.2230731566232884\n",
      "  (0, 313271)\t0.16545061835384603\n",
      "  (0, 283442)\t0.30600388486849617\n",
      "  (0, 134216)\t0.34432453770792865\n",
      "  (0, 127813)\t0.17471500717274766\n",
      "  (0, 341165)\t0.1400676621683276\n",
      "  (1, 240809)\t0.42961657813509385\n",
      "  (1, 204312)\t0.7162811502004598\n",
      "  (1, 261894)\t0.39740014761761466\n",
      "  (1, 32283)\t0.3800579328635268\n",
      "  (2, 64158)\t0.31493848954064496\n",
      "  :\t:\n",
      "  (874136, 221303)\t0.08019780430767935\n",
      "  (874136, 260029)\t0.0815576428167252\n",
      "  (874136, 41018)\t0.16168274662455753\n",
      "  (874136, 194262)\t0.07773822764378334\n",
      "  (874136, 213252)\t0.2866749548515633\n",
      "  (874136, 331028)\t0.06926565064654412\n",
      "  (874136, 330730)\t0.06733479088998563\n",
      "  (874136, 272762)\t0.06624349465024981\n",
      "  (874136, 23516)\t0.0758642330451524\n",
      "  (874136, 316059)\t0.05696223315413456\n",
      "  (874136, 248817)\t0.0738606278431161\n",
      "  (874136, 307807)\t0.06857670565917398\n",
      "  (874137, 222113)\t0.611153062441362\n",
      "  (874137, 107968)\t0.654617212123772\n",
      "  (874137, 30521)\t0.444936219990961\n",
      "  (874138, 282699)\t0.46377114093771576\n",
      "  (874138, 177796)\t0.3457619836738018\n",
      "  (874138, 350560)\t0.38851247101440634\n",
      "  (874138, 58219)\t0.385285042154117\n",
      "  (874138, 274667)\t0.3314855560467065\n",
      "  (874138, 237838)\t0.2850150785132556\n",
      "  (874138, 119971)\t0.22027838912605435\n",
      "  (874138, 352762)\t0.1579031481153566\n",
      "  (874138, 112672)\t0.2445269568543952\n",
      "  (874138, 261894)\t0.20399215980320187\n"
     ]
    }
   ],
   "source": [
    "#//*** Start Timing the process\n",
    "start_time = time.time()\n",
    "\n",
    "#//*** Initialize the Vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "#//*** Build the feature matrix, which is a weighted sparse matrix\n",
    "con_df['tfidf'] = tfidf.fit_transform(con_df['txt'])\n",
    "\n",
    "\n",
    "#//*** Display the Process Time\n",
    "cum_time(start_time)\n",
    "\n",
    "#//*** Print the output to check the results.\n",
    "print(con_df['tfidf'])\n",
    "\n",
    "print(con_df['tfidf'].iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Follow-Up Question**\n",
    "\n",
    "For the three techniques in problem (2) above, give an example where each would be useful.\n",
    "\n",
    "NOTE\n",
    "\n",
    "Running these steps on all of the data can take a while, so feel free to cut down on the number of texts (maybe 50,000) if your program takes too long to run. But be sure to select the text entries randomly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Case: Bag of Words ####\n",
    "Bag of words can used to build a quick baseline model using a few lines of code and relatively few CPU cycles. A big  downside of teh bag of words method is it removes word order which is vital for providing context. In situations where the context is domain specific, bag of words can be very effective.\n",
    "\n",
    "Use cases include:<br>\n",
    "- **Spam Filtering** - Text classification which filters email as spam or legitimate\n",
    "- **Sentiment Analysis** - Classification of people's opinion expressed in a piece of text\n",
    "- **Intention Mining** - Determine a future decision of a person based on the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Case: Part of Speech Tag Vector ####\n",
    "Part-of-speech tagging in the process of breaking down language in to key categories on a word by word basis. Parts-of-Speech Tagging is used to help identify the context of a word, which involve identifying what type of word is used in its context. Common tags are: Pronouns, Verbs, Nouns, Prepositions, Conjunctions, Adverbs, and Adjectives.\n",
    "\n",
    "A typical use case involves identifying nouns and adjectives in a sentence to classify the intent of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Case:  Term Frequency-inverse Document Frequency (tfidf) Vector ####\n",
    "TF-IDF is a statistical measure to evaluate how relevant a word to a document in a collection of documents.\n",
    "Use case:<br>\n",
    "- **Information Retrieval**: TF-IDF was invented for document search and can be used to for words that are relevant to a search term\n",
    "- **Keyword Extraction**: TF-IDF can be used determine document keywords. The highest scoring document words are the most releveant to that document and can be considered keywords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
