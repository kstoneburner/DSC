{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stoneburner, Kurt\n",
    "- ## DSC 550 - Week 04\n",
    "\n",
    "A large portion of this code is taking my text cleaning code from Week 02 - Step1 and refactoring into multiple functions. As we continue our study into natural language processing it makes sense to build a framework to consistently\n",
    "clean data. These steps appear to be consistent (at this point) and definitely repeatable. I'm an inveterate reductionist, streamlining production workflows into a standarized toolkits is a process I'm constantly refining out in the deflationary world of televsion news production.\n",
    "\n",
    "I started with the provided lexigraphical classifier. It works by counting the instances of the words: good, special, bad. The total positive words are subtracted from the total negative words to provide a basic (if not bespoke and overfitted) sentiment analysis. One interesting shortcoming of the model is the statement: 'Today is neither a good day or a bad day!' Which is an interesting example that includes the negation word neither. Which in this context translates to not good and not bad which is a neutral sentiment. If classifier trains for negation words, it is likely that neither would only be applied to 'neither a good day' which beomes not good, which is combined with bad, making the sentiment appear negative instead of neutral.\n",
    "\n",
    "I applied the VADER Sentiment Analyzer since it was included in the nltk library. VADER is a pre-trained model designed to evaluated sentiment in social media posts. This seemed like a good candidate for this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\family\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#//**** Project imports.\n",
    "#//*** The nltk libraries involve additional downloads. The Try blocks automatically download the content if it's not\n",
    "#//*** present. This feels like good form and being a digital nomad, it should run on whichever workstation I happen\n",
    "#//*** to be on.\n",
    "import os\n",
    "import sys\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import time\n",
    "import textblob\n",
    "\n",
    "#//*** nltk - Natural Language toolkit\n",
    "import nltk\n",
    "\n",
    "#//**** Requires the punkt module. Download if it doesn't exist\n",
    "try:\n",
    "    type(nltk.punkt)\n",
    "except:\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "#//*** Check for Vader Lexicon\n",
    "try:\n",
    "    nltk.sentiment.vader.SentimentIntensityAnalyzer()\n",
    "except:\n",
    "    nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "#//pos_tag requires an additional download\n",
    "\n",
    "try:\n",
    "    pos_tag([\"the\",\"quick\",\"brown\",\"fox\"])\n",
    "except: \n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#//*** Convenience function to \n",
    "#//*** Take a time value and display the difference\n",
    "#//*** Return the difference\n",
    "def cum_time(input_time):\n",
    "    tot_time = round(time.time() - input_time,2)\n",
    "    \n",
    "    print(f\"Process Time: {int(tot_time/60)}m {tot_time % 60}s\")\n",
    "    \n",
    "    return tot_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Day of Week                                        comments  actual\n",
      "0      Monday                             Hello, how are you?       0\n",
      "1     Tuesday                            Today is a good day!       1\n",
      "2   Wednesday  It's my birthday so it's a really special day!       1\n",
      "3    Thursday       Today is neither a good day or a bad day!       0\n",
      "4      Friday                           I'm having a bad day.      -1\n",
      "5    Saturday       There' s nothing special happening today.       0\n",
      "6      Sunday                      Today is a SUPER good day!       1\n"
     ]
    }
   ],
   "source": [
    "#//*** Read the raw data into a Series\n",
    "\n",
    "#//****************************************************\n",
    "#//*** It's a Dataframe, not a Text File you ninny!\n",
    "#//****************************************************\n",
    "#with open(\"z_wk04_DailyComments.csv\", \"r\") as file:\n",
    "#    raw_text = pd.Series(file.readlines())\n",
    "\n",
    "#//*** Load the CSV file into a dataframe\n",
    "this_df = pd.read_csv(\"z_wk04_DailyComments.csv\")\n",
    "\n",
    "#//*** Manually classify each statement to check our results\n",
    "#//*** -1 = Negative\n",
    "#//***  0 = Neutral\n",
    "#//***  1 - Positive\n",
    "\n",
    "this_df['actual'] = [0,1,1,0,-1,0,1]\n",
    "\n",
    "print(this_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Apply Common Cleanup operations\n",
    "#//*** In anticpation that I'll be re-using text cleanup code. I'm adding some robustness to the function.\n",
    "#//*** Adding kwargs to disable features that default to true.\n",
    "#//*** Whether an action is skipped or executed is based on a boolean value stored in action_dict.\n",
    "#//*** Key values will default to true. If code needs to be defaulted to False, a default_false list can be added later\n",
    "\n",
    "#//*** All Boolean kwarg keya are stored in kwarg list. This speeds up the coding of the action_dict.\n",
    "#//*** As Kwargs are added \n",
    "def mr_clean_text(input_series, input_options={}):\n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "    \n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "    #//*** Add some data validation. I'm preparing this function for additional use. I'm checking if future users (ie future me)\n",
    "    #//*** may throw some garbage at this function. Experience has taught me to fail safely wherever possible.\n",
    "\n",
    "    #//*** All kwargs are listed here. These initialize TRUE by default.\n",
    "    key_list = [ \"lower\", \"newline\", \"html\", \"remove_empty\", \"punctuation\" ]\n",
    "    \n",
    "    #//*** Build Action Dictionary\n",
    "    action_dict = { } \n",
    "    \n",
    "    #//*** Build the keys from kwarg_list and default them to TRUE\n",
    "    for key in key_list:\n",
    "        action_dict[key] = True\n",
    "        \n",
    "    #//*** Loop through the input kwargs (if any). Assign the action_dict values based on the kwargs:\n",
    "    for key,value in input_options.items():\n",
    "        print(key,value)\n",
    "        action_dict[key] = value\n",
    "    \n",
    "    \n",
    "    #//*************************************************************************\n",
    "    #//*** The Cleanup/Processing code is a straight lift from DSC550 - Week02\n",
    "    #//*************************************************************************\n",
    "    #//*** Convert to Lower Case, Default to True\n",
    "    if action_dict[\"lower\"]:\n",
    "        input_series = input_series.str.lower()\n",
    "    \n",
    "   \n",
    "    #//*** Remove New Lines\n",
    "    if action_dict[\"newline\"]:\n",
    "        #//*** Rmove \\r\\n\n",
    "        input_series = input_series.str.replace(r'\\r?\\n',\"\")\n",
    "\n",
    "        #//*** Remove \\n new lines\n",
    "        input_series = input_series.str.replace(r'\\n',\"\")\n",
    "\n",
    "    #//*** Remove html entities, observed entities are &gt; and &lt;. All HTML entities begin with & and end with ;.\n",
    "    #//*** Let's use regex to remove html entities\n",
    "    if action_dict[\"html\"]:\n",
    "        input_series = input_series.str.replace(r'&.*;',\"\")\n",
    "\n",
    "    #//*** Remove the empty lines\n",
    "    if action_dict[\"remove_empty\"]:\n",
    "        input_series = input_series[ input_series.str.len() > 0]\n",
    "\n",
    "    #//*** Remove punctuation\n",
    "    if action_dict[\"punctuation\"]:\n",
    "        #//*** Load libraries for punctuation if not already loaded.\n",
    "        #//*** Wrapping these in a try, no sense in importing libraries that already exist.\n",
    "        #//*** Unsure of the cost of reimporting libraries (if any). But testing if library is already loaded feels\n",
    "        #//*** like a good practice\n",
    "        try:\n",
    "            type(sys)\n",
    "        except:\n",
    "            import sys\n",
    "\n",
    "        try:\n",
    "            type(unicodedata)\n",
    "        except:\n",
    "            import unicodedata\n",
    "        \n",
    "        #//*** replace Comma and Period with a space.\n",
    "        for punct in [\",\",\".\"]:\n",
    "            input_series = input_series.str.replace(punct,\" \")\n",
    "\n",
    "        #//*** Remove punctuation using the example from the book\n",
    "        punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P') )\n",
    "        input_series = input_series.str.translate(punctuation)\n",
    "\n",
    "    print(f\"Text Cleaning Time: {time.time() - start_time}\")\n",
    "\n",
    "    return input_series\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Tokenize a Series containing Strings.\n",
    "#//*** Breaking this out into it's own function for later reuse.\n",
    "#//*** Not a lot of code here, but it helps to keep the libraries localized. This creates standarization for future\n",
    "#//*** Stoneburner projects. Also has the ability to add functionality as needed.\n",
    "\n",
    "def tokenize_series(input_series):\n",
    "    \n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "    \n",
    "    word_tokenize = nltk.tokenize.word_tokenize \n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "    \n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    input_series = input_series.apply(word_tokenize)\n",
    "    \n",
    "    print(f\"Tokenize Time: {time.time() - start_time}\")\n",
    "    \n",
    "    return input_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Remove Stop words from the input list\n",
    "def remove_stop_words(input_series):\n",
    "    \n",
    "    #//*** This function removes stop_words from a series.\n",
    "    #//*** Works with series.apply()\n",
    "    def apply_stop_words(input_list):\n",
    "\n",
    "        #//*** Load Stopwords   \n",
    "        for word in input_list:\n",
    "            if word in stop_words:\n",
    "                input_list.remove(word)\n",
    "        return input_list\n",
    "\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "        \n",
    "    stopwords = nltk.corpus.stopwords\n",
    "\n",
    "    #//*** Stopwords requires an additional download\n",
    "    try:\n",
    "        type(stopwords)\n",
    "    except:\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "\n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    #//*** The stop_words include punctuation. Stop Word Contractions will not be filtered out.\n",
    "    stop_words = []\n",
    "\n",
    "    #//*** Remove apostrophies from the stop_words\n",
    "    for stop in stopwords.words('english'):\n",
    "        stop_words.append(stop.replace(\"'\",\"\"))\n",
    "\n",
    "    \n",
    "    #//*** Remove Stop words from the tokenized strings in the 'process' column\n",
    "    #input_series = input_series.apply(remove_stop_words,stop_words)\n",
    "    \n",
    "    input_series = input_series.apply(apply_stop_words)\n",
    "\n",
    "    print(f\"Stop Words Time: {time.time() - start_time}\")\n",
    "    \n",
    "    return input_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stemmer(input_series,trim_single_words = True):\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "\n",
    "    #//*** Instantiate the Stemmer\n",
    "    porter = nltk.stem.porter.PorterStemmer()\n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** 1.) Apply() an action to each row\n",
    "    #//*** 2.) lambda word_list, each row is treated as word_list for the subsequent expression\n",
    "    #//*** 3.) The base [ word for word in wordlist] would return each word in word_list as a list. \n",
    "    #//*** 4.) [porter.stem(word) for word in word_list] - performs stemming on each word and returns a list\n",
    "    input_series = input_series.apply(lambda word_list: [porter.stem(word) for word in word_list] )\n",
    "    \n",
    "    #//*** Remove Single letter words after stemming\n",
    "    if trim_single_words:\n",
    "        for word_list in input_series:\n",
    "            for word in word_list:\n",
    "                if len(word) < 2:\n",
    "                    word_list.remove(word)\n",
    "\n",
    "    print(f\"Apply Stemmer Time: {time.time() - start_time}\")\n",
    "    return input_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Cleaning Time: 0.3500540256500244\n",
      "Tokenize Time: 0.01500248908996582\n",
      "Stop Words Time: 0.008817911148071289\n",
      "Apply Stemmer Time: 0.00099945068359375\n",
      "  Day of Week                                        comments  actual  \\\n",
      "0      Monday                             Hello, how are you?       0   \n",
      "1     Tuesday                            Today is a good day!       1   \n",
      "2   Wednesday  It's my birthday so it's a really special day!       1   \n",
      "3    Thursday       Today is neither a good day or a bad day!       0   \n",
      "4      Friday                           I'm having a bad day.      -1   \n",
      "5    Saturday       There' s nothing special happening today.       0   \n",
      "6      Sunday                      Today is a SUPER good day!       1   \n",
      "\n",
      "                                     tokens                          processed  \n",
      "0                              [hello, are]                          hello are  \n",
      "1                        [today, good, day]                     today good day  \n",
      "2  [my, birthday, it, realli, special, day]  my birthday it realli special day  \n",
      "3     [today, neither, good, day, bad, day]     today neither good day bad day  \n",
      "4                            [im, bad, day]                         im bad day  \n",
      "5            [noth, special, happen, today]          noth special happen today  \n",
      "6                 [today, super, good, day]               today super good day  \n"
     ]
    }
   ],
   "source": [
    "#//*** Clean text: Remove punctuation, convert to lowercase, remove blank lines, new lines and html objects.\n",
    "#//*** Tokenize\n",
    "#//*** Remove Stop Words\n",
    "#//*** Stem Words\n",
    "#//*** Considering adding a lemnization routine.\n",
    "\n",
    "#//*** Cleaned and processed text is stored in Token Form\n",
    "this_df['tokens'] = apply_stemmer(remove_stop_words(tokenize_series(mr_clean_text(this_df['comments']))))\n",
    "\n",
    "#//*** Covert the tokenized words into a string\n",
    "this_df['processed'] = this_df['tokens'].apply(lambda word_list: ' '.join(word_list)  )\n",
    "print(this_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vectorized words\n",
      "\n",
      "['are', 'bad', 'birthday', 'day', 'good', 'happen', 'hello', 'im', 'it', 'my', 'neither', 'noth', 'realli', 'special', 'super', 'today']\n",
      "\n",
      "Identify Feature Words - Matrix View\n",
      "\n",
      "[[1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0]\n",
      " [0 1 0 2 1 0 0 0 0 0 1 0 0 0 0 1]\n",
      " [0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1]\n",
      " [0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1]]\n",
      "\n",
      "                                text  positive1  positive2  negative  TotScore\n",
      "0                          hello are          0          0         0         0\n",
      "1                     today good day          1          0         0         1\n",
      "2  my birthday it realli special day          0          1         0         1\n",
      "3     today neither good day bad day          1          0         1         0\n",
      "4                         im bad day          0          0         1        -1\n",
      "5          noth special happen today          0          1         0         1\n",
      "6               today super good day          1          0         0         1\n",
      "\n",
      "Overall Score:   3\n"
     ]
    }
   ],
   "source": [
    "# Basic Text analyzer included in this week's materials for reference\n",
    "# Analyzing text for whether comments are positive or negative\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "corpus = this_df['processed']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(\"\")\n",
    "print(\"vectorized words\")\n",
    "print(\"\")\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"\")\n",
    "print(\"Identify Feature Words - Matrix View\")\n",
    "print(\"\")\n",
    "print( X.toarray())\n",
    "\n",
    "df = pd.DataFrame({'text' : corpus})\n",
    "\n",
    "#check for positive words and negative words\n",
    "df['positive1'] = df.text.str.count('good')\n",
    "df['positive2']= df.text.str.count('special')\n",
    "df['negative'] = df.text.str.count('bad')\n",
    "df['TotScore'] = df.positive1 + df.positive2 - df.negative\n",
    "\n",
    "print(\"\")\n",
    "print(df)\n",
    "\n",
    "Z = sum(df['TotScore'])\n",
    "print(\"\")\n",
    "print(\"Overall Score:  \",Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   actual  vader\n",
      "0       0      0\n",
      "1       1      1\n",
      "2       1      1\n",
      "3       0     -1\n",
      "4      -1     -1\n",
      "5       0      0\n",
      "6       1      1\n",
      "Vader Scored 85.7%  [ 6 of 7 ] on unprocessed text\n"
     ]
    }
   ],
   "source": [
    "#//*** Return a categorical value based on the vader score\n",
    "def categorize_vader(input_score):\n",
    "\n",
    "    #//*** Less than -.33 Sentiment is negative\n",
    "    if  input_score < -.33:\n",
    "        return -1\n",
    "\n",
    "    #//*** Greater than .33 Sentiment is positive\n",
    "    if  input_score > .33:\n",
    "        return 1\n",
    "\n",
    "    #//*** Everything else is neutral\n",
    "    return 0\n",
    "\n",
    "\n",
    "#//*** Use NLTK Vader SentimentIntensityAnalyzer to build a sentiment score\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "#//************************************************************************\n",
    "#//*** Generate a categorical value based on the Vader Sentiment Score.\n",
    "#//************************************************************************\n",
    "#//*** Just because I can do this with one line of code, doesn't mean I should.\n",
    "#//*** this pythonic style can get a bit ridiculous.\n",
    "#//*** Let's try and unpack my monstrosity which I simplified and managed to remove a generator statement. Yay!\n",
    "#//**************************************************************************************************************\n",
    "#//*** Apply Vader to this_df['comments'], retrieve just the compound score: \n",
    "#//***         this_df['comments'].apply( lambda words : analyzer.polarity_scores(words)['compound'] )\n",
    "#//*** analyzer.polarity_scores(words) returns a dictionary containing postive, negative, neutral \n",
    "#//*** and compound (combined) scores. The ['compound'] value retrieves only the compound values.\n",
    "#//*** The second lambda function converts the Vader Compound score to a categorical sentiment value.\n",
    "#//***         ...apply(lambda score : categorize_vader(score))\n",
    "#//*** The second lambda pipes each Vader score through the custom function categorize_vader() which returns -1,0,1 for\n",
    "#//*** each value.\n",
    "\n",
    "#//*** Version 1: This includes an uneeded generator to filter out the compound value.\n",
    "#//***            It's a nice Rube Goldberg touch.\n",
    "#this_df['vader'] = pd.Series( [score['compound'] for score in this_df['comments'].apply( lambda words : analyzer.polarity_scores(words) )]).apply(lambda score : categorize_vader(score))\n",
    "\n",
    "#//*** This is more readable as a compound lambda functions. \n",
    "this_df['vader'] = this_df['comments'].apply( lambda words : analyzer.polarity_scores(words)['compound'] ).apply(lambda score : categorize_vader(score))\n",
    "\n",
    "#//*** Display the Actual Categorical Values vs the Vader Model Values\n",
    "print(this_df[ ['actual','vader'] ])\n",
    "\n",
    "print(f\"Vader Scored {round(len(this_df[this_df['actual'] == this_df['vader'] ]) / len(this_df['vader']),3)*100}%  [ {len(this_df[this_df['actual'] == this_df['vader'] ])} of {len(this_df)} ] on unprocessed text\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   actual  vader\n",
      "0       0      0\n",
      "1       1      1\n",
      "2       1      1\n",
      "3       0      0\n",
      "4      -1     -1\n",
      "5       0      1\n",
      "6       1      1\n",
      "Vader Scored 85.7%  [ 6 of 7 ] on processed text\n"
     ]
    }
   ],
   "source": [
    "#//*** Run vader again on the Processed text, just to see if there is a difference.\n",
    "#//*** And let's break apart the compound lambda statement to make it more legible.\n",
    "this_df['vader'] = this_df['processed'].apply( lambda words : analyzer.polarity_scores(words)['compound'])\n",
    "this_df['vader'] = this_df['vader'].apply(lambda score : categorize_vader(score))\n",
    "\n",
    "print(this_df[ ['actual','vader'] ])\n",
    "\n",
    "print(f\"Vader Scored {round(len(this_df[this_df['actual'] == this_df['vader'] ]) / len(this_df['vader']),3)*100}%  [ {len(this_df[this_df['actual'] == this_df['vader'] ])} of {len(this_df)} ] on processed text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Time: 0m 6.11s\n",
      "    Unnamed: 0  con                                                txt\n",
      "10          11    0  meaningless word keep fire contain he power ca...\n",
      "11          12    0  obama declar dictat life honestli would be ups...\n",
      "12          13    0  classic case us govern depart interior give a ...\n",
      "13          14    0  he a commun organ support redistribut wealth w...\n",
      "14          15    0                                 stop cri unattract\n",
      "15          16    0  believ is good time invok thishttpswwwredditco...\n",
      "16          17    0     you explain there death threat obama got elect\n",
      "17          18    0  cours doe do think is poor person onli give ha...\n",
      "18          19    0  submiss been automat remov it either link shor...\n",
      "19          21    0  would be a pretti common lay man term suppli s...\n"
     ]
    }
   ],
   "source": [
    "#//*** Convenience function to \n",
    "#//*** Take a time value and display the difference\n",
    "#//*** Return the difference\n",
    "def cum_time(input_time):\n",
    "    tot_time = round(time.time() - input_time,2)\n",
    "    \n",
    "    print(f\"Process Time: {int(tot_time/60)}m {tot_time % 60}s\")\n",
    "    \n",
    "    return tot_time\n",
    "\n",
    "#//*** Start Timing the process\n",
    "start_time = time.time()\n",
    "total_job_time = 0\n",
    "\n",
    "#//*** For Step 2 We'll start from the CSV\n",
    "con_df = pd.read_csv(\"z_wk02_controversial_words_df.csv\")\n",
    "\n",
    "\n",
    "\n",
    "#//*** Controversial Words was quite the lengthy project. Cleaning the corpus took over twelve minutes. \n",
    "\n",
    "con_df['txt'] = con_df['txt'].str.replace(\"[\",\"\").str.replace(\"]\",\"\").str.replace(\",\",\"\").str.replace(\"'\",\"\")\n",
    "\n",
    "#//*** Create a tokenized columm for section 2B\n",
    "#con_df['token'] = con_df['txt'].apply(nltk.tokenize.word_tokenize)\n",
    "\n",
    "\n",
    "#//*** Display the Process Time\n",
    "cum_time(start_time)\n",
    "print(con_df[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Vader Scoring Time:\n",
      "Process Time: 4m 3.819999999999993s\n"
     ]
    }
   ],
   "source": [
    "#//*** Start Timing the process\n",
    "start_time = time.time()\n",
    "#//*** Build the VADER dictionary attribute for each line\n",
    "vader_dict = con_df['txt'].apply( lambda words : analyzer.polarity_scores(words) )\n",
    "\n",
    "print(\"Raw Vader Scoring Time:\")\n",
    "#//*** Display the Process Time\n",
    "x = cum_time(start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0  con                                                txt  \\\n",
      "0                0    0  well great he someth those belief he in offic ...   \n",
      "1                1    0                                are right mr presid   \n",
      "2                2    0  have given input apart say am wrong have argum...   \n",
      "3                3    0  get frustrat reason want it way becaus foundat...   \n",
      "4                4    0  am far expert tpp would tend agre lot problem ...   \n",
      "...            ...  ...                                                ...   \n",
      "874134      949994    0                                   payer immort all   \n",
      "874135      949995    0  genuin cant understand anyon support at point ...   \n",
      "874136      949996    0  remind subreddit for civil discussionhttpswwwr...   \n",
      "874137      949997    0                                 k explain or anyth   \n",
      "874138      949999    0  ya sociopath known celebr posit feel you fuck ...   \n",
      "\n",
      "          neg    neu    pos  compound  \n",
      "0       0.163  0.639  0.198    0.2732  \n",
      "1       0.000  1.000  0.000    0.0000  \n",
      "2       0.412  0.588  0.000   -0.6808  \n",
      "3       0.084  0.898  0.019   -0.6682  \n",
      "4       0.029  0.826  0.145    0.9674  \n",
      "...       ...    ...    ...       ...  \n",
      "874134  0.000  1.000  0.000    0.0000  \n",
      "874135  0.116  0.771  0.113   -0.0150  \n",
      "874136  0.166  0.808  0.026   -0.9442  \n",
      "874137  0.000  1.000  0.000    0.0000  \n",
      "874138  0.280  0.720  0.000   -0.5423  \n",
      "\n",
      "[874139 rows x 7 columns]\n",
      "Process Time: 0m 1.7s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "list_dict = {}\n",
    "\n",
    "#//*** Initialize the list_dict with empty arrays\n",
    "for key in vader_dict[0].keys():\n",
    "    list_dict[key] = [ ]\n",
    "\n",
    "#//*** Convert each dictionary value to an array\n",
    "for row in vader_dict:\n",
    "    for key,value in row.items():\n",
    "        list_dict[key].append(value)\n",
    "\n",
    "for key,value in list_dict.items():\n",
    "    con_df[key] = list_dict[key]\n",
    "\n",
    "print(con_df)\n",
    "\n",
    "#//*** Display the Process Time\n",
    "x = cum_time(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'vader_raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'vader_raw'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ecb53138a1d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#//*** Start Timing the process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcon_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vader'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcon_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vader_raw'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mcategorize_vader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Categorize Vader Scoring Time:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#//*** Display the Process Time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'vader_raw'"
     ]
    }
   ],
   "source": [
    "list_dict#//*** Start Timing the process\n",
    "start_time = time.time()\n",
    "con_df['categorical'] = con_df['compound'].apply(lambda score : categorize_vader(score))\n",
    "print(\"Categorize Vader Scoring Time:\")\n",
    "#//*** Display the Process Time\n",
    "x = cum_time(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Start Timing the process\n",
    "start_time = time.time()\n",
    "\n",
    "total_cat_negative = len(con_df[con_df['vader'] == -1])\n",
    "total_cat_neutral = len(con_df[con_df['vader'] == 0])\n",
    "total_cat_positive = len(con_df[con_df['vader'] == 1])\n",
    "print(f\"Total Positive Posts: {total_cat_positive} [{round(total_cat_positive/len(con_df),4)*100}%]\" )\n",
    "print(f\"Total Neutral  Posts: {total_cat_neutral} [{round(total_cat_neutral/len(con_df),4)*100}%]\" )\n",
    "print(f\"Total Negative Posts: {total_cat_negative} [{round(total_cat_negative/len(con_df),4)*100}%]\" )\n",
    "print(f\"Corpus VADER Categorical Sentiment Sum: {con_df['vader'].sum()}\")\n",
    "\n",
    "print(f\"VADER Corpus Compound Score: {con_df['vader_raw'].sum()}\")\n",
    "#//*** Display the Process Time\n",
    "x = cum_time(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con_df[ -.33 < con_df['vader_raw'] < .33 ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#//*** I'm still wrapping my head around tfidf.\n",
    "#//*** This is here for contemplative reference\n",
    "\n",
    "countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english')\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n",
    "\n",
    "# convert th documents into a matrixcount_wm = countvectorizer.fit_transform(train)\n",
    "count_wm = countvectorizer.fit_transform(this_df['processed'])\n",
    "tfidf_wm = tfidfvectorizer.fit_transform(this_df['processed'])\n",
    "\n",
    "#retrieve the terms found in the corpora\n",
    "# if we take same parameters on both Classes(CountVectorizer and TfidfVectorizer) , it will give same output of get_feature_names() methods)#count_tokens = tfidfvectorizer.get_feature_names() # no difference\n",
    "count_tokens = countvectorizer.get_feature_names()\n",
    "tfidf_tokens = tfidfvectorizer.get_feature_names()\n",
    "df_countvect = pd.DataFrame(data = count_wm.toarray(),columns = count_tokens)\n",
    "df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),columns = tfidf_tokens)\n",
    "print(\"Count Vectorizer\\n\")\n",
    "print(df_countvect)\n",
    "print(\"\\nTD-IDF Vectorizer\\n\")\n",
    "print(df_tfidfvect)\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
