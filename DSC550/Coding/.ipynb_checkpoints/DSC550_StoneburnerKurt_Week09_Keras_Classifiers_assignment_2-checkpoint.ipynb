{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stoneburner, Kurt\n",
    "- ## DSC 550 - Week 09/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Neural Network Classifier with Keras###\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using Keras. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://oindrilasen.com/2021/02/how-to-install-and-import-keras-in-anaconda-jupyter-notebooks/\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, accuracy_score, recall_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"z_wk09_categorized_comments_processed.zip\")\n",
    "\n",
    "df['processed'] = df['lema_stem_tokens'].apply(lambda word_list: ' '.join(word_list)) \n",
    "\n",
    "#//*** Convert categorical string to categorical int\n",
    "#//*** Only run once to prevent iPython issues\n",
    "if (df.dtypes['cat'] == object):\n",
    "    cat_dict = dict(tuple(enumerate(df['cat'].unique())))\n",
    "    #//*** Build sexcat Categorical column\n",
    "    df['intcat'] = df['cat'].copy()\n",
    "    \n",
    "    #//*** replace values using the sex_dict dictionary\n",
    "    for key,value in cat_dict.items():\n",
    "        df['intcat'] = df['intcat'].replace(value,key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First pass Vectorizing\n",
      "Re-Vectorizing: max_features=9659 [38639*0.25]\n",
      "Epoch 1/5\n",
      "474/474 [==============================] - 177s 374ms/step - loss: 0.4596 - accuracy: 0.8162\n",
      "Epoch 2/5\n",
      "474/474 [==============================] - 177s 374ms/step - loss: 0.3045 - accuracy: 0.8747\n",
      "Epoch 3/5\n",
      "474/474 [==============================] - 178s 375ms/step - loss: 0.1728 - accuracy: 0.9295\n",
      "Epoch 4/5\n",
      "474/474 [==============================] - 177s 374ms/step - loss: 0.1050 - accuracy: 0.9593\n",
      "Epoch 5/5\n",
      "474/474 [==============================] - 177s 374ms/step - loss: 0.0862 - accuracy: 0.9652\n",
      "Scoring...\n",
      "Scoring Time 97.3s\n",
      "========\n",
      "Precision: 0.822503520300399\n",
      "Accuracy: 0.822503520300399\n",
      "Recall: 0.822503520300399\n",
      "cm: [[ 82434    447  48213]\n",
      " [  1531  10475  10631]\n",
      " [ 33115   2871 355691]]\n",
      "Epoch 1/5\n",
      "474/474 [==============================] - 211s 445ms/step - loss: 0.4595 - accuracy: 0.8153\n",
      "Epoch 2/5\n",
      "474/474 [==============================] - 211s 446ms/step - loss: 0.3034 - accuracy: 0.8764\n",
      "Epoch 3/5\n",
      "474/474 [==============================] - 211s 445ms/step - loss: 0.1728 - accuracy: 0.9295\n",
      "Epoch 4/5\n",
      "474/474 [==============================] - 212s 446ms/step - loss: 0.1048 - accuracy: 0.9590\n",
      "Epoch 5/5\n",
      "474/474 [==============================] - 211s 446ms/step - loss: 0.0857 - accuracy: 0.9652\n",
      "Scoring...\n",
      "Scoring Time 160.0s\n",
      "========\n",
      "Precision: 0.8274246068997888\n",
      "Accuracy: 0.8274246068997888\n",
      "Recall: 0.8274246068997888\n",
      "cm: [[ 78637    363  52094]\n",
      " [  1261  10009  11367]\n",
      " [ 26786   2253 362638]]\n",
      "   acc_per_s    time   size     tf    fp  features  sls  epochs  accuracy  \\\n",
      "0   0.000781  1059.0  60600  38639  0.25      9659  0.5       5  0.827425   \n",
      "\n",
      "   precision    recall  \n",
      "0   0.827425  0.827425  \n",
      "Epoch 1/5\n",
      "474/474 [==============================] - 241s 509ms/step - loss: 0.4618 - accuracy: 0.8138\n",
      "Epoch 2/5\n",
      "474/474 [==============================] - 242s 510ms/step - loss: 0.3006 - accuracy: 0.8766\n",
      "Epoch 3/5\n",
      "474/474 [==============================] - 242s 510ms/step - loss: 0.1658 - accuracy: 0.9321\n",
      "Epoch 4/5\n",
      "474/474 [==============================] - 242s 511ms/step - loss: 0.1027 - accuracy: 0.9588\n",
      "Epoch 5/5\n",
      "474/474 [==============================] - 242s 511ms/step - loss: 0.0856 - accuracy: 0.9651\n",
      "Scoring...\n",
      "Scoring Time 225.99s\n",
      "========\n",
      "Precision: 0.8250997418446374\n",
      "Accuracy: 0.8250997418446374\n",
      "Recall: 0.8250997418446374\n",
      "cm: [[ 81138    514  49442]\n",
      " [  1522  10512  10603]\n",
      " [ 30429   2882 358366]]\n",
      "   acc_per_s    time   size     tf    fp  features   sls  epochs  accuracy  \\\n",
      "0    0.00068  1213.0  60600  38639  0.25      9659  0.75       5    0.8251   \n",
      "\n",
      "   precision  recall  \n",
      "0     0.8251  0.8251  \n",
      "Epoch 1/5\n",
      "474/474 [==============================] - 262s 553ms/step - loss: 0.4600 - accuracy: 0.8145\n",
      "Epoch 2/5\n",
      "474/474 [==============================] - 264s 556ms/step - loss: 0.2998 - accuracy: 0.8770\n",
      "Epoch 3/5\n",
      "474/474 [==============================] - 264s 557ms/step - loss: 0.1684 - accuracy: 0.9306\n",
      "Epoch 4/5\n",
      "474/474 [==============================] - 263s 555ms/step - loss: 0.1051 - accuracy: 0.9577\n",
      "Epoch 5/5\n",
      "474/474 [==============================] - 263s 555ms/step - loss: 0.0863 - accuracy: 0.9653\n",
      "Scoring...\n",
      "Scoring Time 295.56s\n",
      "========\n",
      "Precision: 0.8265848685754518\n",
      "Accuracy: 0.8265848685754518\n",
      "Recall: 0.8265848685754518\n",
      "cm: [[ 79029    502  51563]\n",
      " [  1401  10542  10694]\n",
      " [ 27496   2926 361255]]\n",
      "   acc_per_s    time   size     tf    fp  features  sls  epochs  accuracy  \\\n",
      "0   0.000626  1321.0  60600  38639  0.25      9659    1       5  0.826585   \n",
      "\n",
      "   precision    recall  \n",
      "0   0.826585  0.826585  \n",
      "Re-Vectorizing: max_features=19319 [38639*0.5]\n",
      "Epoch 1/5\n",
      "474/474 [==============================] - 670s 1s/step - loss: 0.4585 - accuracy: 0.8173\n",
      "Epoch 2/5\n",
      "474/474 [==============================] - 670s 1s/step - loss: 0.2833 - accuracy: 0.8860\n",
      "Epoch 3/5\n",
      "474/474 [==============================] - 670s 1s/step - loss: 0.1542 - accuracy: 0.9378\n",
      "Epoch 4/5\n",
      "474/474 [==============================] - 670s 1s/step - loss: 0.0938 - accuracy: 0.9635\n",
      "Epoch 5/5\n",
      "474/474 [==============================] - 671s 1s/step - loss: 0.0783 - accuracy: 0.9690\n",
      "Scoring...\n",
      "Scoring Time 315.93s\n",
      "========\n",
      "Precision: 0.8241133243370101\n",
      "Accuracy: 0.8241133243370101\n",
      "Recall: 0.8241133243370101\n",
      "cm: [[ 85441    596  45057]\n",
      " [  1816  10456  10365]\n",
      " [ 34925   3171 353581]]\n",
      "   acc_per_s    time   size     tf   fp  features   sls  epochs  accuracy  \\\n",
      "0   0.000245  3361.0  60600  38639  0.5     19319  0.25       5  0.824113   \n",
      "\n",
      "   precision    recall  \n",
      "0   0.824113  0.824113  \n",
      "Epoch 1/5\n",
      "474/474 [==============================] - 846s 2s/step - loss: 0.4617 - accuracy: 0.8154\n",
      "Epoch 2/5\n",
      "474/474 [==============================] - 846s 2s/step - loss: 0.2820 - accuracy: 0.8869\n",
      "Epoch 3/5\n",
      "474/474 [==============================] - 848s 2s/step - loss: 0.1490 - accuracy: 0.9404\n",
      "Epoch 4/5\n",
      "474/474 [==============================] - 847s 2s/step - loss: 0.0936 - accuracy: 0.9632\n",
      "Epoch 5/5\n",
      "474/474 [==============================] - 845s 2s/step - loss: 0.0785 - accuracy: 0.9687\n",
      "Scoring...\n",
      "Scoring Time 564.48s\n",
      "========\n",
      "Precision: 0.8208643804271298\n",
      "Accuracy: 0.8208643804271298\n",
      "Recall: 0.8208643804271298\n",
      "cm: [[ 86225    470  44399]\n",
      " [  2023  10736   9878]\n",
      " [ 37795   3137 350745]]\n",
      "   acc_per_s    time   size     tf   fp  features  sls  epochs  accuracy  \\\n",
      "0   0.000193  4244.0  60600  38639  0.5     19319  0.5       5  0.820864   \n",
      "\n",
      "   precision    recall  \n",
      "0   0.820864  0.820864  \n",
      "Epoch 1/5\n",
      "474/474 [==============================] - 902s 2s/step - loss: 0.4588 - accuracy: 0.8181\n",
      "Epoch 2/5\n",
      "474/474 [==============================] - 902s 2s/step - loss: 0.2828 - accuracy: 0.8876\n",
      "Epoch 3/5\n",
      "474/474 [==============================] - 903s 2s/step - loss: 0.1538 - accuracy: 0.9383\n",
      "Epoch 4/5\n",
      "474/474 [==============================] - 903s 2s/step - loss: 0.0946 - accuracy: 0.9624\n",
      "Epoch 5/5\n",
      "474/474 [==============================] - 902s 2s/step - loss: 0.0785 - accuracy: 0.9688\n",
      "Scoring...\n",
      "Scoring Time 883.36s\n",
      "========\n",
      "Precision: 0.8223348392396151\n",
      "Accuracy: 0.8223348392396151\n",
      "Recall: 0.8223348392396151\n",
      "cm: [[ 85258    740  45096]\n",
      " [  1786  11352   9499]\n",
      " [ 35423   4356 351898]]\n",
      "   acc_per_s    time   size     tf   fp  features   sls  epochs  accuracy  \\\n",
      "0   0.000182  4525.0  60600  38639  0.5     19319  0.75       5  0.822335   \n",
      "\n",
      "   precision    recall  \n",
      "0   0.822335  0.822335  \n",
      "Epoch 1/5\n",
      "474/474 [==============================] - 1027s 2s/step - loss: 0.4636 - accuracy: 0.8133\n",
      "Epoch 2/5\n",
      "474/474 [==============================] - 1031s 2s/step - loss: 0.2805 - accuracy: 0.8880\n",
      "Epoch 3/5\n",
      "474/474 [==============================] - 1035s 2s/step - loss: 0.1457 - accuracy: 0.9421\n",
      "Epoch 4/5\n",
      "474/474 [==============================] - 1036s 2s/step - loss: 0.0920 - accuracy: 0.9639\n",
      "Epoch 5/5\n",
      "474/474 [==============================] - 1037s 2s/step - loss: 0.0769 - accuracy: 0.9691\n",
      "Scoring...\n",
      "Scoring Time 1120.01s\n",
      "========\n",
      "Precision: 0.8276152898380662\n",
      "Accuracy: 0.8276152898380662\n",
      "Recall: 0.8276152898380662\n",
      "cm: [[ 81343    510  49241]\n",
      " [  1383  10598  10656]\n",
      " [ 29333   2897 359447]]\n",
      "   acc_per_s    time   size     tf   fp  features  sls  epochs  accuracy  \\\n",
      "0    0.00016  5182.0  60600  38639  0.5     19319    1       5  0.827615   \n",
      "\n",
      "   precision    recall  \n",
      "0   0.827615  0.827615  \n",
      "Re-Vectorizing: max_features=28979 [38639*0.75]\n",
      "Epoch 1/5\n",
      "474/474 [==============================] - 1521s 3s/step - loss: 0.4617 - accuracy: 0.8173\n",
      "Epoch 2/5\n",
      "474/474 [==============================] - 1530s 3s/step - loss: 0.2709 - accuracy: 0.8923\n",
      "Epoch 3/5\n",
      "474/474 [==============================] - 1540s 3s/step - loss: 0.1389 - accuracy: 0.9449\n",
      "Epoch 4/5\n",
      "474/474 [==============================] - 1542s 3s/step - loss: 0.0881 - accuracy: 0.9662\n",
      "Epoch 5/5\n",
      "474/474 [==============================] - 1550s 3s/step - loss: 0.0725 - accuracy: 0.9712\n",
      "Scoring...\n",
      "Scoring Time 693.16s\n",
      "========\n",
      "Precision: 0.813794443792537\n",
      "Accuracy: 0.813794443792537\n",
      "Recall: 0.813794443792537\n",
      "cm: [[ 88858    573  41663]\n",
      " [  2117  10857   9663]\n",
      " [ 44049   3493 344135]]\n",
      "   acc_per_s    time   size     tf    fp  features   sls  epochs  accuracy  \\\n",
      "0   0.000106  7705.0  60600  38639  0.75     28979  0.25       5  0.813794   \n",
      "\n",
      "   precision    recall  \n",
      "0   0.813794  0.813794  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "474/474 [==============================] - 1749s 4s/step - loss: 0.4649 - accuracy: 0.8144\n",
      "Epoch 2/5\n",
      "474/474 [==============================] - 1745s 4s/step - loss: 0.2730 - accuracy: 0.8907\n",
      "Epoch 3/5\n",
      "474/474 [==============================] - 1746s 4s/step - loss: 0.1386 - accuracy: 0.9446\n",
      "Epoch 4/5\n",
      "474/474 [==============================] - 1746s 4s/step - loss: 0.0855 - accuracy: 0.9661\n",
      "Epoch 5/5\n",
      "474/474 [==============================] - 1774s 4s/step - loss: 0.0725 - accuracy: 0.9711\n",
      "Scoring...\n",
      "Scoring Time 1273.62s\n",
      "========\n",
      "Precision: 0.8345330468199953\n",
      "Accuracy: 0.8345330468199953\n",
      "Recall: 0.8345330468199953\n",
      "cm: [[ 73373    544  57177]\n",
      " [   917  10844  10876]\n",
      " [ 17198   3535 370944]]\n",
      "   acc_per_s    time   size     tf    fp  features  sls  epochs  accuracy  \\\n",
      "0   0.000095  8791.0  60600  38639  0.75     28979  0.5       5  0.834533   \n",
      "\n",
      "   precision    recall  \n",
      "0   0.834533  0.834533  \n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "in user code:\n\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:756 train_step\n        _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2743 _minimize\n        optimizer.apply_gradients(\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:519 apply_gradients\n        self._create_all_weights(var_list)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:704 _create_all_weights\n        self._create_slots(var_list)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adam.py:127 _create_slots\n        self.add_slot(var, 'm')\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:760 add_slot\n        weight = tf_variables.Variable(\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:262 __call__\n        return cls._variable_v2_call(*args, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:244 _variable_v2_call\n        return previous_getter(\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2857 creator\n        return next_creator(**kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2857 creator\n        return next_creator(**kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2857 creator\n        return next_creator(**kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:683 variable_capturing_scope\n        v = UnliftedInitializerVariable(\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:264 __call__\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:226 __init__\n        initial_value() if init_from_fn else initial_value,\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\initializers\\initializers_v2.py:137 __call__\n        return super(Zeros, self).__call__(shape, dtype=_get_dtype(dtype))\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops_v2.py:132 __call__\n        return array_ops.zeros(shape, dtype)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2747 wrapped\n        tensor = fun(*args, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2806 zeros\n        output = fill(shape, constant(zero, dtype=dtype), name=name)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:239 fill\n        result = gen_array_ops.fill(dims, value, name=name)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:3401 fill\n        _ops.raise_from_not_ok_status(e, name)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6843 raise_from_not_ok_status\n        six.raise_from(core._status_to_exception(e.code, message), None)\n    <string>:3 raise_from\n        \n\n    ResourceExhaustedError: OOM when allocating tensor with shape[28979,21734] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Fill]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-76b8bdf37a69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m                 \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[0mmodel_run_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Invalid shape for y: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: in user code:\n\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:756 train_step\n        _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2743 _minimize\n        optimizer.apply_gradients(\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:519 apply_gradients\n        self._create_all_weights(var_list)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:704 _create_all_weights\n        self._create_slots(var_list)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adam.py:127 _create_slots\n        self.add_slot(var, 'm')\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:760 add_slot\n        weight = tf_variables.Variable(\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:262 __call__\n        return cls._variable_v2_call(*args, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:244 _variable_v2_call\n        return previous_getter(\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2857 creator\n        return next_creator(**kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2857 creator\n        return next_creator(**kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2857 creator\n        return next_creator(**kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:683 variable_capturing_scope\n        v = UnliftedInitializerVariable(\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:264 __call__\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:226 __init__\n        initial_value() if init_from_fn else initial_value,\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\keras\\initializers\\initializers_v2.py:137 __call__\n        return super(Zeros, self).__call__(shape, dtype=_get_dtype(dtype))\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops_v2.py:132 __call__\n        return array_ops.zeros(shape, dtype)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2747 wrapped\n        tensor = fun(*args, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2806 zeros\n        output = fill(shape, constant(zero, dtype=dtype), name=name)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:239 fill\n        result = gen_array_ops.fill(dims, value, name=name)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:3401 fill\n        _ops.raise_from_not_ok_status(e, name)\n    C:\\Users\\stonk013\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6843 raise_from_not_ok_status\n        six.raise_from(core._status_to_exception(e.code, message), None)\n    <string>:3 raise_from\n        \n\n    ResourceExhaustedError: OOM when allocating tensor with shape[28979,21734] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Fill]\n"
     ]
    }
   ],
   "source": [
    "#//*** Reference Code: Applied Text Analysys with Python p282.\n",
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.sparse.reorder(tf.SparseTensor(indices, coo.data, coo.shape))\n",
    "\n",
    "def build_network():\n",
    "    \"\"\"\n",
    "    Create a function that returns a compiled neural network\n",
    "    \"\"\"\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(N_FEATURES, activation='relu', input_shape=(N_FEATURES,)))\n",
    "    \n",
    "    nn.add(Dense(N_FEATURES*SECOND_LAYER_SIZE, activation='relu'))\n",
    "    nn.add(Dense(N_CLASSES, activation='softmax'))\n",
    "    nn.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return nn\n",
    "\n",
    "score_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "#//*** Theoretically, this number should be return as the second value of the tfidf sparse matrix\n",
    "N_FEATURES = 1000\n",
    "\n",
    "#//*** X is Post Processed Data to evaluate\n",
    "data_model_x = df['processed']\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=N_FEATURES)\n",
    "\n",
    "data_model_y = df['intcat']\n",
    "\n",
    "test_size=.5\n",
    "test_size=.98\n",
    "#continuous scoring model\n",
    "#scoring = 'r2_score'\n",
    "\n",
    "#data_model_x = tfidf.fit_transform(data_model_x)\n",
    "\n",
    "#//*** N_CLASSES the number of categories to solve for\n",
    "N_CLASSES = len(df['intcat'].unique())\n",
    "\n",
    "FEATURE_PERCENT = .25\n",
    "SECOND_LAYER_SIZE = .25\n",
    "epochs = 2\n",
    "\n",
    "for test_size in [.9]:\n",
    "    # split the data randomly into test/train sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_model_x, data_model_y, test_size =test_size, random_state=0)\n",
    "\n",
    "\n",
    "    #vectorizer = TfidfVectorizer(max_features=N_FEATURES)\n",
    "\n",
    "    #//*** Initialize the Vectorizer, get all the features\n",
    "    tfidf = TfidfVectorizer()\n",
    "\n",
    "    print(\"First pass Vectorizing\")\n",
    "    total_features = tfidf.fit_transform(x_train).shape[1]\n",
    "\n",
    "\n",
    "    for FEATURE_PERCENT in [.25,.5,.75,1]:\n",
    "        # split the data randomly into test/train sets\n",
    "        x_train, x_test, y_train, y_test = train_test_split(data_model_x, data_model_y, test_size =test_size, random_state=0)\n",
    "\n",
    "        N_FEATURES = int(total_features * FEATURE_PERCENT)\n",
    "\n",
    "        print(f\"Re-Vectorizing: max_features={N_FEATURES} [{total_features}*{FEATURE_PERCENT}]\")\n",
    "\n",
    "        vectorizer = TfidfVectorizer(max_features=N_FEATURES)\n",
    "\n",
    "\n",
    "        x_train = vectorizer.fit_transform(x_train)\n",
    "        x_test = vectorizer.transform(x_test)\n",
    "\n",
    "        x_train = convert_sparse_matrix_to_sparse_tensor(x_train)\n",
    "        x_test = convert_sparse_matrix_to_sparse_tensor(x_test)\n",
    "\n",
    "        for SECOND_LAYER_SIZE in [.25,.5,.75,1]:\n",
    "\n",
    "            for epochs in [5]:\n",
    "\n",
    "                pipeline = Pipeline([\n",
    "                #  ('norm', TextNormalizer()),\n",
    "                #  ('vect', TfidfVectorizer(max_features=N_FEATURES)),\n",
    "                 ('nn', KerasClassifier(build_fn=build_network,\n",
    "                 epochs=epochs,\n",
    "                batch_size=128))\n",
    "                 ])\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                pipeline.fit(x_train,y_train)\n",
    "\n",
    "                model_run_time = time.time() - start_time \n",
    "                #print(f\"Model Run Time: {model_run_time}s\")\n",
    "                precision = 1\n",
    "                accuracy = 1\n",
    "                recall = 1\n",
    "\n",
    "                print('Scoring...')\n",
    "                start_score_time = time.time()\n",
    "                y_predicted = pipeline.predict(x_test)\n",
    "                print(f\"Scoring Time {round(time.time()-start_score_time,2)}s\")\n",
    "\n",
    "                precision = precision_score(y_test, y_predicted, average='micro')\n",
    "                accuracy = accuracy_score(y_test, y_predicted)\n",
    "                recall = recall_score(y_test, y_predicted, average ='micro')\n",
    "                cm = confusion_matrix(y_test, y_predicted)\n",
    "\n",
    "                print(\"========\")\n",
    "                print(f\"Precision: {precision}\")\n",
    "                print(f\"Accuracy: {accuracy}\")\n",
    "                print(f\"Recall: {recall}\")\n",
    "                print(f\"cm: {cm}\")\n",
    "\n",
    "                if len(score_df) == 0:\n",
    "                    score_df = pd.DataFrame()\n",
    "                    score_df['acc_per_s'] = [accuracy/model_run_time]    \n",
    "                    score_df['time'] = [round(model_run_time,0)]  \n",
    "                    score_df['size'] = [int((1-test_size)*len(data_model_x))]\n",
    "                    score_df['tf'] = [total_features]    \n",
    "                    score_df['fp'] = [FEATURE_PERCENT]    \n",
    "                    score_df['features'] = [N_FEATURES]    \n",
    "                    score_df['sls'] = [SECOND_LAYER_SIZE]    \n",
    "                    score_df['epochs'] = [epochs]    \n",
    "                    score_df['accuracy'] = [accuracy]    \n",
    "                    score_df['precision'] = [precision]    \n",
    "                    score_df['recall'] = [recall]    \n",
    "\n",
    "                else:\n",
    "                    temp_df = pd.DataFrame()\n",
    "                    temp_df['acc_per_s'] = [accuracy/model_run_time]    \n",
    "                    temp_df['time'] = [round(model_run_time,0)]    \n",
    "                    temp_df['size'] = [int((1-test_size)*len(data_model_x))]\n",
    "                    temp_df['tf'] = [total_features]    \n",
    "                    temp_df['fp'] = [FEATURE_PERCENT]    \n",
    "                    temp_df['features'] = [N_FEATURES]    \n",
    "                    temp_df['sls'] = [SECOND_LAYER_SIZE]    \n",
    "                    temp_df['epochs'] = [epochs]    \n",
    "                    temp_df['accuracy'] = [accuracy]    \n",
    "                    temp_df['precision'] = [precision]    \n",
    "                    temp_df['recall'] = [recall] \n",
    "\n",
    "                    score_df = pd.concat([score_df,temp_df])\n",
    "                    print(temp_df)\n",
    "    \n",
    "\n",
    "print(score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   acc_per_s    time   size     tf    fp  features   sls  epochs  accuracy  \\\n",
      "0   0.000924   890.0  60600  38639  0.25      9659  0.25       5  0.822504   \n",
      "0   0.000781  1059.0  60600  38639  0.25      9659  0.50       5  0.827425   \n",
      "0   0.000680  1213.0  60600  38639  0.25      9659  0.75       5  0.825100   \n",
      "0   0.000626  1321.0  60600  38639  0.25      9659  1.00       5  0.826585   \n",
      "0   0.000245  3361.0  60600  38639  0.50     19319  0.25       5  0.824113   \n",
      "0   0.000193  4244.0  60600  38639  0.50     19319  0.50       5  0.820864   \n",
      "0   0.000182  4525.0  60600  38639  0.50     19319  0.75       5  0.822335   \n",
      "0   0.000160  5182.0  60600  38639  0.50     19319  1.00       5  0.827615   \n",
      "0   0.000106  7705.0  60600  38639  0.75     28979  0.25       5  0.813794   \n",
      "0   0.000095  8791.0  60600  38639  0.75     28979  0.50       5  0.834533   \n",
      "\n",
      "   precision    recall  \n",
      "0   0.822504  0.822504  \n",
      "0   0.827425  0.827425  \n",
      "0   0.825100  0.825100  \n",
      "0   0.826585  0.826585  \n",
      "0   0.824113  0.824113  \n",
      "0   0.820864  0.820864  \n",
      "0   0.822335  0.822335  \n",
      "0   0.827615  0.827615  \n",
      "0   0.813794  0.813794  \n",
      "0   0.834533  0.834533  \n"
     ]
    }
   ],
   "source": [
    "print(score_df)\n",
    "pd.to_pickle(score_df,\"z_wk09_keras_v1.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799031130448839\n",
      "[[ 68691    624  73491]\n",
      " [  1168   7849  15566]\n",
      " [ 25777   2727 397995]]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = pipeline.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, accuracy_score, recall_score, confusion_matrix\n",
    "\n",
    "#You'll get a warning but just ignore it\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision = precision_score(y_test, y_predicted, average='micro')\n",
    "accuracy = accuracy_score(y_test, y_predicted)\n",
    "recall = recall_score(y_test, y_predicted, average ='micro')\n",
    "cm = confusion_matrix(y_test, y_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4640/4640 [==============================] - 47s 10ms/step - loss: 0.9633 - accuracy: 0.7990\n"
     ]
    }
   ],
   "source": [
    "score = pipeline.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Samplewise metrics are not available outside of multilabel classification.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-ffbea4a067c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'samples'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1464\u001b[0m     \u001b[1;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1465\u001b[0m     \u001b[0msamplewise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maverage\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'samples'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1466\u001b[1;33m     MCM = multilabel_confusion_matrix(y_true, y_pred,\n\u001b[0m\u001b[0;32m   1467\u001b[0m                                       \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1468\u001b[0m                                       labels=labels, samplewise=samplewise)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mmultilabel_confusion_matrix\u001b[1;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msamplewise\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m             raise ValueError(\"Samplewise metrics are not available outside of \"\n\u001b[0m\u001b[0;32m    476\u001b[0m                              \"multilabel classification.\")\n\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Samplewise metrics are not available outside of multilabel classification."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test, y_predicted,average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'saveto' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-92b8f0bb3e66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaveto\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keras_model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaveto\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sklearn_pipe'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'saveto' is not defined"
     ]
    }
   ],
   "source": [
    "pipeline.steps[-1][1].model.save(saveto['keras_model']) \n",
    "pipeline.steps.pop(-1)\n",
    "joblib.dump(model, saveto['sklearn_pipe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8205883990948976\n",
      "0.8205883990948976\n",
      "0.8205883990948976\n",
      "0.8205883990948976\n",
      "[[ 64247    620  51632]\n",
      " [   985   8901  10133]\n",
      " [ 20564   3046 324679]]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = pipeline.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, accuracy_score, recall_score, confusion_matrix\n",
    "\n",
    "#You'll get a warning but just ignore it\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "print(f1_score(y_test, y_predicted, average = 'micro'))\n",
    "print(precision_score(y_test, y_predicted, average = 'micro'))\n",
    "print(accuracy_score(y_test, y_predicted))\n",
    "print(recall_score(y_test, y_predicted, average = 'micro'))\n",
    "print(confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-f23549e117f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"processed\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"processed_text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_sparse_matrix_to_sparse_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_sparse_matrix_to_sparse_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "N_FEATURES = 5000\n",
    "N_CLASSES = 3\n",
    "def build_network():\n",
    "    \"\"\"\n",
    "    Create a function that returns a compiled neural network\n",
    "    \"\"\"\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(500, activation='relu', input_shape=(N_FEATURES,)))\n",
    "    nn.add(Dense(150, activation='relu'))\n",
    "    nn.add(Dense(N_CLASSES, activation='softmax'))\n",
    "    nn.compile(\n",
    "         loss='categorical_crossentropy',\n",
    "         optimizer='adam',\n",
    "         metrics=['accuracy']\n",
    "    )\n",
    "    return nn\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.sparse.reorder(tf.SparseTensor(indices, coo.data, coo.shape))\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    " \n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=N_FEATURES)\n",
    "\n",
    " \n",
    "\n",
    "X_train = vectorizer.fit_transform(df[\"processed\"])\n",
    "X_test = vectorizer.transform(test[\"processed_text\"])\n",
    "X_train = convert_sparse_matrix_to_sparse_tensor(X_train)\n",
    "X_test = convert_sparse_matrix_to_sparse_tensor(X_test)\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from transformer import TextNormalizer\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "pipeline = Pipeline([\n",
    "#  ('norm', TextNormalizer()),\n",
    "#  ('vect', TfidfVectorizer(max_features=N_FEATURES)),\n",
    " ('nn', KerasClassifier(build_fn=build_network,\n",
    " epochs=200,\n",
    "batch_size=128))\n",
    " ])\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def train_model(X, y, model, saveto=None, cv=12): \n",
    "    \"\"\"\n",
    "    Trains model from corpus at specified path and fits on full data.\n",
    "    If a saveto dictionary is specified, writes Keras and Sklearn\n",
    "    pipeline components to disk separately. Returns the scores.\n",
    "    \"\"\"\n",
    "\n",
    " \n",
    "\n",
    "    model.fit(X, y)\n",
    "    if saveto: \n",
    "        model.steps[-1][1].model.save(saveto['keras_model']) \n",
    "        model.steps.pop(-1)\n",
    "        joblib.dump(model, saveto['sklearn_pipe'])\n",
    "    return model\n",
    "\n",
    " \n",
    "\n",
    "    y_train = train[\"cat\"]\n",
    "y_test = test[\"cat\"]\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "TModel =train_model(X_train, y_train, model = pipeline)\n",
    "\n",
    " \n",
    "\n",
    "#scores being returned are the \"loss scores\" \n",
    "\n",
    " \n",
    "\n",
    "y_predicted = TModel.predict(X_test)\n",
    "\n",
    " \n",
    "\n",
    "#You'll get a warning but just ignore it\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "f1_score(ytest, y_predicted, average = 'micro')\n",
    "precision_score(ytest, y_predicted, average = 'micro')\n",
    "accuracy_score(ytest, y_predicted)\n",
    "recall_score(ytest, y_predicted, average = 'micro')\n",
    "confusion_matrix(ytest, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#//*** Initialize the Vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "\n",
    "#//*** Build the feature matrix, which is a weighted sparse matrix\n",
    "bwarg = tfidf.fit_transform(df['processed'][:10000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bwarg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classifying Images ###\n",
    "In chapter 20 of the Machine Learning with Python Cookbook, implement the code found in section 20.15 classify MSINT images using a convolutional neural network. Report the accuracy of your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
