{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stoneburner, Kurt\n",
    "- ## DSC 550 - Week 09/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Neural Network Classifier with Scikit ### \n",
    "\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using scikit-learn. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//***************************************\n",
    "#//*** Apply Common Cleanup operations\n",
    "#//***************************************\n",
    "#//*** These cleanup functions are based on Week 02 cleanup code, and rebuilt for Week 04\n",
    "\n",
    "#//*****************************************\n",
    "#//*** Functions:\n",
    "#//*****************************************\n",
    "#//*** Mr_clean_text: Converts to lowercase, removes punctuation, newlines and html markup\n",
    "#//****************************************************************************************************\n",
    "#//*** Tokenize_series: Converts a Series containing strings, to a series containing tokenized lists\n",
    "#//****************************************************************************************************\n",
    "#//*** Remove_stop_words: Removes Stop words based on nltk stopwords 'english' dictionary\n",
    "#//****************************************************************************************************\n",
    "#//*** Apply_stemmer: Stem tokenized words using nltk.stem.porter.PorterStemme\n",
    "#//****************************************************************************************************\n",
    "#//*** apply_pos_tag: Builds Part of Speech Tagging from tokeninzed text\n",
    "#//****************************************************************************************************\n",
    "\n",
    "#//****************************************************************************************************\n",
    "\n",
    "#//****************************************************************************************************\n",
    "#//*** Key values will default to true. If code needs to be defaulted to False, a default_false list can be added later\n",
    "#//*** All Boolean kwarg keya are stored in kwarg list. This speeds up the coding of the action_dict.\n",
    "#//*** As Kwargs are added \n",
    "def mr_clean_text(input_series, input_options={}):\n",
    "    \n",
    "    def clean_text(input_string):\n",
    "        clean1 = re.sub(r'['+string.punctuation + '’—”'+']', \"\", input_string.lower())\n",
    "        return re.sub(r'\\W+', ' ', clean1)\n",
    "\n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "    #//*** Add some data validation. I'm preparing this function for additional use. I'm checking if future users (ie future me)\n",
    "    #//*** may throw some garbage at this function. Experience has taught me to fail safely wherever possible.\n",
    "\n",
    "    #//*** All kwargs are listed here. These initialize TRUE by default.\n",
    "    key_list = [ \"lower\", \"newline\", \"html\", \"punctuation\" ]\n",
    "    \n",
    "    default_false = [\"remove_empty\"]\n",
    "    \n",
    "    #//*** Build Action Dictionary\n",
    "    action_dict = { } \n",
    "    \n",
    "    #//*** Build the keys from kwarg_list and default them to TRUE\n",
    "    for key in key_list:\n",
    "        action_dict[key] = True\n",
    "    \n",
    "    for key in default_false:\n",
    "        action_dict[key] = False\n",
    "        \n",
    "    #//*** Loop through the input kwargs (if any). Assign the action_dict values based on the kwargs:\n",
    "    for key,value in input_options.items():\n",
    "        print(key,value)\n",
    "        action_dict[key] = value\n",
    "    \n",
    "    \n",
    "    #//*************************************************************************\n",
    "    #//*** The Cleanup/Processing code is a straight lift from DSC550 - Week02\n",
    "    #//*************************************************************************\n",
    "    #//*** Convert to Lower Case, Default to True\n",
    "    if action_dict[\"lower\"]:\n",
    "        input_series = input_series.str.lower()\n",
    "    \n",
    "   \n",
    "    #//*** Remove New Lines\n",
    "    if action_dict[\"newline\"]:\n",
    "        #//*** Rmove \\r\\n\n",
    "        input_series = input_series.str.replace('\\r?\\n',\"\")\n",
    "\n",
    "        #//*** Remove \\n new lines\n",
    "        input_series = input_series.str.replace('\\n',\"\")\n",
    "    \n",
    "    \n",
    "    input_series = input_series.str.replace(\"\\\\(http.+\\\\)\",\"\")\n",
    "    \n",
    "    #//*** Print Elements between brackets\n",
    "    #print(input_series[ input_series == input_series.str.match('[.*]')])\n",
    "\n",
    "     \n",
    "    #//*** Remove html entities, observed entities are &gt; and &lt;. All HTML entities begin with & and end with ;.\n",
    "    #//*** Let's use regex to remove html entities\n",
    "    if action_dict[\"html\"]:\n",
    "        input_series = input_series.str.replace(r'&.*;',\"\")\n",
    "\n",
    "    #//*** Remove the empty lines\n",
    "    if action_dict[\"remove_empty\"]:\n",
    "        input_series = input_series[ input_series.str.len() > 0]\n",
    "\n",
    "    #//*** Remove punctuation\n",
    "    if action_dict[\"punctuation\"]:\n",
    "        #//*** Load libraries for punctuation if not already loaded.\n",
    "        #//*** Wrapping these in a try, no sense in importing libraries that already exist.\n",
    "        #//*** Unsure of the cost of reimporting libraries (if any). But testing if library is already loaded feels\n",
    "        #//*** like a good practice\n",
    "\n",
    "        #input_series = input_series.apply(lambda x: clean_text(x))\n",
    "\n",
    "        try:\n",
    "            type(sys)\n",
    "        except:\n",
    "            import sys\n",
    "\n",
    "        try:\n",
    "            type(unicodedata)\n",
    "        except:\n",
    "            import unicodedata\n",
    "\n",
    "        #//*** replace Comma and Period with a space.\n",
    "        for punct in [\",\",\".\"]:\n",
    "            input_series = input_series.str.replace(punct,\" \")\n",
    "\n",
    "        #//*** Remove punctuation using the example from the book\n",
    "        punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P') )\n",
    "        input_series = input_series.str.translate(punctuation)\n",
    "\n",
    "        #table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}\n",
    "        #print(table )\n",
    "        #input_series = input_series.str.translate(table)\n",
    "\n",
    "    print(f\"Text Cleaning Time: {time.time() - start_time}\")\n",
    "\n",
    "    return input_series\n",
    "\n",
    "                                          \n",
    "#//*** Tokenize a Series containing Strings.\n",
    "#//*** Breaking this out into it's own function for later reuse.\n",
    "#//*** Not a lot of code here, but it helps to keep the libraries localized. This creates standarization for future\n",
    "#//*** Stoneburner projects. Also has the ability to add functionality as needed.\n",
    "\n",
    "def tokenize_series(input_series,slices=20,input_options={}):\n",
    "    \n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "    \n",
    "    word_tokenize = nltk.tokenize.word_tokenize \n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "        \n",
    "    #//*** All kwargs are listed here. These initialize False by default.\n",
    "    key_list = [ \"fast\", \"quiet\" ]\n",
    "    \n",
    "    #//*** Build Action Dictionary\n",
    "    action_dict = { } \n",
    "    \n",
    "    #//*** Build the keys from kwarg_list and default them to False\n",
    "    for key in key_list:\n",
    "        action_dict[key] = False\n",
    "        \n",
    "    #//*** Loop through the input kwargs (if any). Assign the action_dict values based on the kwargs:\n",
    "    for key,value in input_options.items():\n",
    "        print(key,value)\n",
    "        action_dict[key] = value\n",
    "    \n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "            \n",
    "    #input_series = input_series.apply(word_tokenize)\n",
    "    \n",
    "    if action_dict['fast'] == False:\n",
    "        print(\"Processing Tokens with NLTK Word Tokenize\")\n",
    "        input_series = apply_with_progress(input_series,word_tokenize,slices)\n",
    "    else:\n",
    "        print(\"Process Tokens with Split()\")\n",
    "        input_series = apply_with_progress(input_series,lambda x: x.split(),slices)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"Tokenize Time: {time.time() - start_time}\")\n",
    "    \n",
    "    return input_series\n",
    "\n",
    "#//*** Remove Stop words from the input list\n",
    "def remove_stop_words(input_series):\n",
    "    \n",
    "    #//*** This function removes stop_words from a series.\n",
    "    #//*** Works with series.apply()\n",
    "    def apply_stop_words(input_list):\n",
    "\n",
    "        #//*** Load Stopwords   \n",
    "        for word in input_list:\n",
    "            if word in stop_words:\n",
    "                input_list.remove(word)\n",
    "                #print(f\"Removing: {word}\")\n",
    "        return input_list\n",
    "\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "        \n",
    "    stopwords = nltk.corpus.stopwords\n",
    "\n",
    "    #//*** Stopwords requires an additional download\n",
    "    try:\n",
    "        type(stopwords)\n",
    "    except:\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "\n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "\n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    #//*** The stop_words include punctuation. Stop Word Contractions will not be filtered out.\n",
    "    #//*** Manually adding word the\n",
    "    stop_words = []\n",
    "    \n",
    "    #//*** Remove apostrophies from the stop_words\n",
    "    for stop in stopwords.words('english'):\n",
    "        stop_words.append(stop.replace(\"'\",\"\"))\n",
    "\n",
    "    #print(\"Stop Words: \")\n",
    "    print(stop_words)\n",
    "    print (\"Processing Stop Words\")\n",
    "    input_series = apply_with_progress(input_series, apply_stop_words)\n",
    "    \n",
    "    print(f\"Stop Words Time: {time.time() - start_time}\")\n",
    "    \n",
    "    return input_series\n",
    "\n",
    "def apply_stemmer(input_series,trim_single_words = True,slices=100):\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "\n",
    "    #//*** Instantiate the Stemmer\n",
    "    porter = nltk.stem.porter.PorterStemmer()\n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    #//*** 1.) Apply() an action to each row\n",
    "    #//*** 2.) lambda word_list, each row is treated as word_list for the subsequent expression\n",
    "    #//*** 3.) The base [ word for word in wordlist] would return each word in word_list as a list. \n",
    "    #//*** 4.) [porter.stem(word) for word in word_list] - performs stemming on each word and returns a list\n",
    "    #input_series = input_series.apply(lambda word_list: [porter.stem(word) for word in word_list] )\n",
    "    print(\"Begin: Apply Stemmer\")\n",
    "    input_series = apply_with_progress(input_series, lambda word_list: [porter.stem(word) for word in word_list],slices)\n",
    "    \n",
    "    #//*** Remove Single letter words after stemming\n",
    "    \n",
    "    \"\"\"\n",
    "    if trim_single_words:\n",
    "        for word_list in input_series:\n",
    "            for word in word_list:\n",
    "                if len(word) < 2:\n",
    "                    word_list.remove(word)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Apply Stemmer Time: {time.time() - start_time}\")\n",
    "    return input_series\n",
    "\n",
    "def apply_pos_tag(input_series,slices=100):\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "\n",
    "    from nltk import pos_tag\n",
    "\n",
    "    #//pos_tag requires an additional download\n",
    "    try:\n",
    "        pos_tag([\"the\",\"quick\",\"brown\",\"fox\"])\n",
    "    except: \n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    print(\"Begin Part of Speech tagging\")\n",
    "    \n",
    "    input_series = apply_with_progress(input_series,pos_tag,slices)\n",
    "    \n",
    "    print(f\"Part of Speech Tagging Time: {round(time.time() - start_time,2)}s\")\n",
    "    \n",
    "    return input_series\n",
    "    \n",
    "def apply_lemmatization(input_series,slices=20):\n",
    "            \n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    from nltk.corpus import wordnet    \n",
    "    \n",
    "    #nltk.download('wordnet')\n",
    "    \n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Force creation of a new Series instead of using a copy\n",
    "    input_series = input_series.copy()\n",
    "    \n",
    "    # Initialize the Lemmatizer instance\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    \n",
    "    #//*** 1.) Apply() an action to each row\n",
    "    #//*** 2.) lambda word_list, each row is treated as word_list for the subsequent expression\n",
    "    #//*** 3.) The base [ word for word in wordlist] would return each word in word_list as a list. \n",
    "    #//*** 4.) [lemmatizer.lemmatize(word) for word in word_list] - performs lemmtization on each word and returns a list\n",
    "    #lemmatized = input_series.apply(lambda word_list: [lemmatizer.lemmatize(*word) for word in word_list] )\n",
    "    \n",
    "    print(\"Begin Lemmatization...\")\n",
    "    \n",
    "    input_series = apply_with_progress(input_series,lambda word_list: [lemmatizer.lemmatize(word) for word in word_list],20)\n",
    "    \n",
    "    print(f\"Lemmatization Time: {time.time() - start_time}\")\n",
    "    \n",
    "    #if detoken:\n",
    "    #    return tokenize_series(input_series,5,{\"fast\":True})\n",
    "\n",
    "    return input_series\n",
    "\n",
    "#//*** Apply a function to a Series and display processing progress\n",
    "#//*** Slices is the total number of intervals to report progress.\n",
    "#//*** Slices = 20 displays processing after every 5% is processed\n",
    "#//*** Slices = 100 displays processing after every 1% is processed\n",
    "def apply_with_progress(input_series,input_function,slices=20):\n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Get the time at the start of the loop, used for elapsed time.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** The interval is the number of elements to process in each Loop. The default is 20.\n",
    "    #//*** Which displays results at 5% intervals.\n",
    "    interval = int(len(input_series)/slices)\n",
    "    \n",
    "    #//*** Total number of items to process\n",
    "    total = len(input_series)\n",
    "    \n",
    "\n",
    "    #//*** Loop through slice times and display processing statistics for each slice.\n",
    "    for x in range(0, slices ):\n",
    "        #//*** Get time at the start of the slice.\n",
    "        loop_time = time.time()\n",
    "        \n",
    "        #//*** Set the start index\n",
    "        begin_dex = interval*x\n",
    "        \n",
    "        #//*** Set the end index\n",
    "        end_dex = interval*x+interval-1\n",
    "        \n",
    "        #//*** Apply the input function to a slice of the input_series\n",
    "        #//*** This part does all the actual 'work'\n",
    "        input_series[begin_dex:end_dex] = input_series[begin_dex:end_dex].apply(input_function)\n",
    "        \n",
    "        #//*** Get the time after the slice of work is done\n",
    "        now = time.time()\n",
    "        \n",
    "        #//*** Compute the estimated remaining time\n",
    "        #//*** Total elapsed time / % of completed slices = Estimated total time\n",
    "        #//*** Estimated total time - elaped time = Remaining time\n",
    "        est_remain = round( ( ( now - start_time ) /  ( (x+1)/slices ) - (now-start_time)),2)\n",
    "\n",
    "        #//*** Display Results so we know how much time is left (so we can effectively multi-task: ie comments, research and Valheim)\n",
    "        print(f\"Processed {x}/{slices}: {begin_dex}:{end_dex} [{total}] in {round(now-loop_time,2)}s elapsed: {round(now-start_time,2)}s est Remain: {est_remain}s\")\n",
    "    \n",
    "    #//*** END For Slice Loop\n",
    "    \n",
    "    #//*** Process the remaining values (Since interval is an int there should be a remainder)\n",
    "    loop_time = time.time()\n",
    "    begin_dex = end_dex+1\n",
    "    if begin_dex < len(input_series):\n",
    "        print(f\"Processing Remaining values: {begin_dex} : {total} \")\n",
    "        #print(input_series[begin_dex:])\n",
    "        input_series[begin_dex:] = input_series[begin_dex:].apply(input_function)\n",
    "    \n",
    "    #//*** Display Final output\n",
    "    print(f\"Processed {slices}/{slices}: {begin_dex}:{end_dex} [{total}] in {round(time.time()-loop_time,2)}s elapsed: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "    #//*** return Series\n",
    "    return input_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#//*** Process categorized-comments.jsonl\n",
    "#//*** Processed comments are stored in a local pickled file. \n",
    "#//*** Only run processing if a local copy if needed. Local builds are required since github has a 100 mb file size limit.\n",
    "#//*** If it's too annoying I should manually split the dataframe into multiple files, or see if pickle supports multi-part zip files.\n",
    "if False:\n",
    "    df = pd.read_json(\"z_wk09_categorized-comments.jsonl\", lines=True)\n",
    "\n",
    "    #//*** Clean the Text.\n",
    "    df['processed'] = mr_clean_text(df['txt'],{\"lower\": True, \"newline\": True, \"html\": True, \"remove_empty\" : False, \"punctuation\" : True})\n",
    "\n",
    "    print(f\"Articles of length with 0 characters: {len(df[ df['processed'].str.len() == 0 ])}\")\n",
    "\n",
    "    #//Remove Items with an Arbitrary length of 0\n",
    "\n",
    "    print(f\"Articles of length with 0 characters: {len(df[ df['processed'].str.len() == 0 ])}\")\n",
    "    print(\"Remove these articles\")\n",
    "    print(f\"Article Count Before: {len(df)}\")\n",
    "    df = df[ df['processed'].str.len() > 0 ]\n",
    "    print(f\"Article Count After: {len(df)}\")\n",
    "\n",
    "    #//************************\n",
    "    #//*** Tokenize the Text\n",
    "    #//************************\n",
    "    #//*** The custom function displays progress while it's working\n",
    "    df['processed'] = tokenize_series(df['processed'],20,{\"fast\":True})\n",
    "\n",
    "    #//************************\n",
    "    #//*** Remove Stop Words\n",
    "    #//************************\n",
    "    #//*** The custom function displays progress while it's working\n",
    "    df['processed'] = remove_stop_words(df['processed'])\n",
    "\n",
    "    #//************************\n",
    "    #//*** Apply Lematization\n",
    "    #//************************\n",
    "    df['lema_stem_tokens'] = apply_lemmatization(df['processed']) \n",
    "\n",
    "    print( f\"Total Corpus Word Count: {df['lema_stem_tokens'].apply(lambda x: len(x)).sum()}\" )\n",
    "    #//*** Eliminate words with length of 0,1 or 2. This is an arbitrary value to help with feature reduction\n",
    "    #df['tokens'] = df['tokens'].apply(lambda word_list : list(filter(lambda word : len(word) >= 3, word_list))) \n",
    "    df['lema_stem_tokens'] = df['lema_stem_tokens'].apply(lambda word_list : list(filter(lambda word : len(word) >= 3, word_list))) \n",
    "\n",
    "    #//*** Build Word Count\n",
    "    df['num_wds'] = df['lema_stem_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "    print( f\"Total Corpus Word Count: {df['lema_stem_tokens'].apply(lambda x: len(x)).sum()}\" )\n",
    "\n",
    "    #//************************\n",
    "    #//*** Stem the lemma's\n",
    "    #//***********************************************************************\n",
    "    #//*** We are trying to reduce the feature set\n",
    "    #//***********************************************************************\n",
    "    df['lema_stem_tokens'] = apply_stemmer(df['lema_stem_tokens'])\n",
    "\n",
    "    #//**********************************\n",
    "    #//*** Apply Part of Speech Tagging\n",
    "    #//**********************************\n",
    "    df['pos_tag'] = apply_pos_tag(df['lema_stem_tokens'])\n",
    "    \n",
    "    #//**********************************\n",
    "    #//*** Save the Dataframe to file\n",
    "    #//**********************************\n",
    "    pd.to_pickle(df,\"z_wk09_categorized_comments_processed.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"z_wk09_categorized_comments_processed.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                cat                                                txt  \\\n",
      "0            sports  Barely better than Gabbert? He was significant...   \n",
      "1            sports  Fuck the ducks and the Angels! But welcome to ...   \n",
      "2            sports  Should have drafted more WRs.\\n\\n- Matt Millen...   \n",
      "3            sports            [Done](https://i.imgur.com/2YZ90pm.jpg)   \n",
      "4            sports                                      No!! NOO!!!!!   \n",
      "...             ...                                                ...   \n",
      "606471  video_games             No. It's probably only happened to you   \n",
      "606472  video_games  I think most of the disappointment came from t...   \n",
      "606473  video_games  dishonored 1/2 looked like arse, so what the h...   \n",
      "606474  video_games                                          [removed]   \n",
      "606475  video_games  I wish more games provided options like Rise o...   \n",
      "\n",
      "                                                processed  \\\n",
      "0       [barely, better, gabbert, significantly, bette...   \n",
      "1       [fuck, ducks, the, angels, welcome, all, new, ...   \n",
      "2            [have, drafted, wrs, matt, millen, probably]   \n",
      "3                                                  [done]   \n",
      "4                                                   [noo]   \n",
      "...                                                   ...   \n",
      "606471                     [its, probably, happened, you]   \n",
      "606472  [think, disappointment, came, delay, of, the, ...   \n",
      "606473  [dishonored, 12, looked, like, arse, what, hel...   \n",
      "606474                                          [removed]   \n",
      "606475  [wish, games, provided, options, like, rise, t...   \n",
      "\n",
      "                                         lema_stem_tokens  num_wds  \\\n",
      "0       [bare, better, gabbert, significantli, better,...       56   \n",
      "1       [fuck, duck, the, angel, welcom, all, new, nin...        9   \n",
      "2                [have, draft, wr, matt, millen, probabl]        6   \n",
      "3                                                  [done]        1   \n",
      "4                                                   [noo]        1   \n",
      "...                                                   ...      ...   \n",
      "606471                             [probabl, happen, you]        3   \n",
      "606472  [think, disappoint, came, delay, the, ps+, ver...       34   \n",
      "606473  [dishonor, look, like, ars, what, hell, they, ...       14   \n",
      "606474                                            [remov]        1   \n",
      "606475  [wish, game, provid, option, like, rise, the, ...       13   \n",
      "\n",
      "                                                  pos_tag  intcat  \n",
      "0       [(bare, NN), (better, RBR), (gabbert, NN), (si...       0  \n",
      "1       [(fuck, JJ), (duck, VBD), (the, DT), (angel, N...       0  \n",
      "2       [(have, VB), (draft, NN), (wr, NN), (matt, NN)...       0  \n",
      "3                                           [(done, VBN)]       0  \n",
      "4                                             [(noo, NN)]       0  \n",
      "...                                                   ...     ...  \n",
      "606471          [(probabl, NN), (happen, VB), (you, PRP)]       2  \n",
      "606472  [(think, VB), (disappoint, NN), (came, VBD), (...       2  \n",
      "606473  [(dishonor, JJ), (look, NN), (like, IN), (ars,...       2  \n",
      "606474                                      [(remov, NN)]       2  \n",
      "606475  [(wish, JJ), (game, NN), (provid, NN), (option...       2  \n",
      "\n",
      "[606008 rows x 7 columns]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-2b7ac55a8d8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vectorized words\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Basic Text analyzer included in this week's materials for reference\n",
    "# Analyzing text for whether comments are positive or negative\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(df)\n",
    "corpus = df['processed']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(\"\")\n",
    "print(\"vectorized words\")\n",
    "print(\"\")\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"\")\n",
    "print(\"Identify Feature Words - Matrix View\")\n",
    "print(\"\")\n",
    "print( X.toarray())\n",
    "\n",
    "\"\"\"\n",
    "df = pd.DataFrame({'text' : corpus})\n",
    "\n",
    "#check for positive words and negative words\n",
    "df['positive1'] = df.text.str.count('good')\n",
    "df['positive2']= df.text.str.count('special')\n",
    "df['negative'] = df.text.str.count('bad')\n",
    "df['TotScore'] = df.positive1 + df.positive2 - df.negative\n",
    "\n",
    "print(\"\")\n",
    "print(df)\n",
    "\n",
    "Z = sum(df['TotScore'])\n",
    "print(\"\")\n",
    "print(\"Overall Score:  \",Z)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                cat                                                txt  \\\n",
      "0            sports  Barely better than Gabbert? He was significant...   \n",
      "1            sports  Fuck the ducks and the Angels! But welcome to ...   \n",
      "2            sports  Should have drafted more WRs.\\n\\n- Matt Millen...   \n",
      "3            sports            [Done](https://i.imgur.com/2YZ90pm.jpg)   \n",
      "4            sports                                      No!! NOO!!!!!   \n",
      "...             ...                                                ...   \n",
      "606471  video_games             No. It's probably only happened to you   \n",
      "606472  video_games  I think most of the disappointment came from t...   \n",
      "606473  video_games  dishonored 1/2 looked like arse, so what the h...   \n",
      "606474  video_games                                          [removed]   \n",
      "606475  video_games  I wish more games provided options like Rise o...   \n",
      "\n",
      "                                                processed  \\\n",
      "0       [barely, better, gabbert, significantly, bette...   \n",
      "1       [fuck, ducks, the, angels, welcome, all, new, ...   \n",
      "2            [have, drafted, wrs, matt, millen, probably]   \n",
      "3                                                  [done]   \n",
      "4                                                   [noo]   \n",
      "...                                                   ...   \n",
      "606471                     [its, probably, happened, you]   \n",
      "606472  [think, disappointment, came, delay, of, the, ...   \n",
      "606473  [dishonored, 12, looked, like, arse, what, hel...   \n",
      "606474                                          [removed]   \n",
      "606475  [wish, games, provided, options, like, rise, t...   \n",
      "\n",
      "                                         lema_stem_tokens  num_wds  \\\n",
      "0       [bare, better, gabbert, significantli, better,...       56   \n",
      "1       [fuck, duck, the, angel, welcom, all, new, nin...        9   \n",
      "2                [have, draft, wr, matt, millen, probabl]        6   \n",
      "3                                                  [done]        1   \n",
      "4                                                   [noo]        1   \n",
      "...                                                   ...      ...   \n",
      "606471                             [probabl, happen, you]        3   \n",
      "606472  [think, disappoint, came, delay, the, ps+, ver...       34   \n",
      "606473  [dishonor, look, like, ars, what, hell, they, ...       14   \n",
      "606474                                            [remov]        1   \n",
      "606475  [wish, game, provid, option, like, rise, the, ...       13   \n",
      "\n",
      "                                                  pos_tag  \n",
      "0       [(bare, NN), (better, RBR), (gabbert, NN), (si...  \n",
      "1       [(fuck, JJ), (duck, VBD), (the, DT), (angel, N...  \n",
      "2       [(have, VB), (draft, NN), (wr, NN), (matt, NN)...  \n",
      "3                                           [(done, VBN)]  \n",
      "4                                             [(noo, NN)]  \n",
      "...                                                   ...  \n",
      "606471          [(probabl, NN), (happen, VB), (you, PRP)]  \n",
      "606472  [(think, VB), (disappoint, NN), (came, VBD), (...  \n",
      "606473  [(dishonor, JJ), (look, NN), (like, IN), (ars,...  \n",
      "606474                                      [(remov, NN)]  \n",
      "606475  [(wish, JJ), (game, NN), (provid, NN), (option...  \n",
      "\n",
      "[606008 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge transformers \n",
    "\n",
    "#from sklearn.externals import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "#//*** Convert categorical string to categorical int\n",
    "#//*** Only run once to prevent iPython issues\n",
    "if (df.dtypes['cat'] == object):\n",
    "    cat_dict = dict(tuple(enumerate(df['cat'].unique())))\n",
    "    #//*** Build sexcat Categorical column\n",
    "    df['intcat'] = df['cat'].copy()\n",
    "    \n",
    "    #//*** replace values using the sex_dict dictionary\n",
    "    for key,value in cat_dict.items():\n",
    "        df['intcat'] = df['intcat'].replace(value,key)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12120.16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['processed'] = df['lema_stem_tokens'].apply(lambda word_list: ' '.join(word_list)) \n",
    "print(len(df)*.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformer import TextNormalizer\n",
    "#//*** https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "#https://analyticsindiamag.com/a-beginners-guide-to-scikit-learns-mlpclassifier/\n",
    "#https://duckduckgo.com/?q=sklearn+cross_val_score+&t=ffab&ia=web\n",
    "#https://towardsdatascience.com/sentiment-analysis-using-classification-e73da5b4159f\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.87880367\n",
      "Iteration 2, loss = 0.62474422\n",
      "Iteration 3, loss = 0.48838717\n",
      "Iteration 4, loss = 0.37664260\n",
      "Iteration 5, loss = 0.29745798\n",
      "Iteration 6, loss = 0.24323493\n",
      "Iteration 7, loss = 0.20585773\n",
      "Iteration 8, loss = 0.17960307\n",
      "Iteration 9, loss = 0.16101218\n",
      "Iteration 10, loss = 0.14631470\n",
      "Iteration 11, loss = 0.13494408\n",
      "Iteration 12, loss = 0.12578378\n",
      "Iteration 13, loss = 0.11826513\n",
      "Iteration 14, loss = 0.11186075\n",
      "Iteration 15, loss = 0.10689121\n",
      "Iteration 16, loss = 0.10231371\n",
      "Iteration 17, loss = 0.09841441\n",
      "Iteration 18, loss = 0.09518308\n",
      "Iteration 19, loss = 0.09199913\n",
      "Iteration 20, loss = 0.08893460\n",
      "Iteration 21, loss = 0.08602783\n",
      "Iteration 22, loss = 0.08463393\n",
      "Iteration 23, loss = 0.08229871\n",
      "Iteration 24, loss = 0.08044378\n",
      "Iteration 25, loss = 0.07899397\n",
      "Iteration 26, loss = 0.07717090\n",
      "Iteration 27, loss = 0.07644547\n",
      "Iteration 28, loss = 0.07479978\n",
      "Iteration 29, loss = 0.07340799\n",
      "Iteration 30, loss = 0.07238928\n",
      "Iteration 31, loss = 0.07135999\n",
      "Iteration 32, loss = 0.07034889\n",
      "Iteration 33, loss = 0.06967310\n",
      "Iteration 34, loss = 0.06881461\n",
      "Iteration 35, loss = 0.06809356\n",
      "Iteration 36, loss = 0.06765109\n",
      "Iteration 37, loss = 0.06685547\n",
      "Iteration 38, loss = 0.06666089\n",
      "Iteration 39, loss = 0.06622086\n",
      "Iteration 40, loss = 0.06484345\n",
      "Iteration 41, loss = 0.06469277\n",
      "Iteration 42, loss = 0.06391245\n",
      "Iteration 43, loss = 0.06376536\n",
      "Iteration 44, loss = 0.06364254\n",
      "Iteration 45, loss = 0.06295502\n",
      "Iteration 46, loss = 0.06266937\n",
      "Iteration 47, loss = 0.06199984\n",
      "Iteration 48, loss = 0.06242918\n",
      "Iteration 49, loss = 0.06148689\n",
      "Iteration 50, loss = 0.06158106\n",
      "Iteration 51, loss = 0.06110732\n",
      "Iteration 52, loss = 0.06059938\n",
      "Iteration 53, loss = 0.06059216\n",
      "Iteration 54, loss = 0.06125610\n",
      "Iteration 55, loss = 0.05989757\n",
      "Iteration 56, loss = 0.05946823\n",
      "Iteration 57, loss = 0.05946920\n",
      "Iteration 58, loss = 0.05961843\n",
      "Iteration 59, loss = 0.05904706\n",
      "Iteration 60, loss = 0.05872713\n",
      "Iteration 61, loss = 0.05905834\n",
      "Iteration 62, loss = 0.05870406\n",
      "Iteration 63, loss = 0.05846412\n",
      "Iteration 64, loss = 0.05821360\n",
      "Iteration 65, loss = 0.05840460\n",
      "Iteration 66, loss = 0.05795313\n",
      "Iteration 67, loss = 0.05784326\n",
      "Iteration 68, loss = 0.05786643\n",
      "Iteration 69, loss = 0.05748413\n",
      "Iteration 70, loss = 0.05760379\n",
      "Iteration 71, loss = 0.05727032\n",
      "Iteration 72, loss = 0.05757754\n",
      "Iteration 73, loss = 0.05756647\n",
      "Iteration 74, loss = 0.05697154\n",
      "Iteration 75, loss = 0.05666879\n",
      "Iteration 76, loss = 0.05682108\n",
      "Iteration 77, loss = 0.05702287\n",
      "Iteration 78, loss = 0.05629235\n",
      "Iteration 79, loss = 0.05620665\n",
      "Iteration 80, loss = 0.05608952\n",
      "Iteration 81, loss = 0.05701204\n",
      "Iteration 82, loss = 0.05571367\n",
      "Iteration 83, loss = 0.05634838\n",
      "Iteration 84, loss = 0.05620628\n",
      "Iteration 85, loss = 0.05594085\n",
      "Iteration 86, loss = 0.05538445\n",
      "Iteration 87, loss = 0.05611768\n",
      "Iteration 88, loss = 0.05583560\n",
      "Iteration 89, loss = 0.05650580\n",
      "Iteration 90, loss = 0.05583866\n",
      "Iteration 91, loss = 0.05584986\n",
      "Iteration 92, loss = 0.05563922\n",
      "Iteration 93, loss = 0.05557064\n",
      "Iteration 94, loss = 0.05520912\n",
      "Iteration 95, loss = 0.05507282\n",
      "Iteration 96, loss = 0.05552680\n",
      "Iteration 97, loss = 0.05541304\n",
      "Iteration 98, loss = 0.05481179\n",
      "Iteration 99, loss = 0.05518855\n",
      "Iteration 100, loss = 0.05521053\n",
      "Iteration 101, loss = 0.05536917\n",
      "Iteration 102, loss = 0.05502288\n",
      "Iteration 103, loss = 0.05506283\n",
      "Iteration 104, loss = 0.05522400\n",
      "Iteration 105, loss = 0.05516358\n",
      "Iteration 106, loss = 0.05517439\n",
      "Iteration 107, loss = 0.05496345\n",
      "Iteration 108, loss = 0.05454794\n",
      "Iteration 109, loss = 0.05482060\n",
      "Iteration 110, loss = 0.05449245\n",
      "Iteration 111, loss = 0.05437514\n",
      "Iteration 112, loss = 0.05471105\n",
      "Iteration 113, loss = 0.05496745\n",
      "Iteration 114, loss = 0.05508995\n",
      "Iteration 115, loss = 0.05490554\n",
      "Iteration 116, loss = 0.05439048\n",
      "Iteration 117, loss = 0.05414912\n",
      "Iteration 118, loss = 0.05489678\n",
      "Iteration 119, loss = 0.05420519\n",
      "Iteration 120, loss = 0.05403306\n",
      "Iteration 121, loss = 0.05427650\n",
      "Iteration 122, loss = 0.05459082\n",
      "Iteration 123, loss = 0.05451266\n",
      "Iteration 124, loss = 0.05459827\n",
      "Iteration 125, loss = 0.05398846\n",
      "Iteration 126, loss = 0.05406282\n",
      "Iteration 127, loss = 0.05395717\n",
      "Iteration 128, loss = 0.05409328\n",
      "Iteration 129, loss = 0.05432201\n",
      "Iteration 130, loss = 0.05396582\n",
      "Iteration 131, loss = 0.05408013\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Test Size 0.98 - 12120 cm:\n",
      "[[ 73838   2876  48242]\n",
      " [  2296   8599   8283]\n",
      " [ 66618  13132 370004]]\n",
      "Test Size 0.98 - 12120: 607s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#//*** Path to postprocessed, part of speech tagges review corpus\n",
    "#cpath = df['lema_stem_tokens']\n",
    "\n",
    "classifier = Pipeline([\n",
    "    #'norm', TextNormalizer(),\n",
    "    ('tfidf',TfidfVectorizer()),\n",
    "    #('ann',MLPRegressor(hidden_layer_sizes=[500,150], verbose=True))\n",
    "    ('ann',MLPClassifier( verbose=True))\n",
    "])\n",
    "\n",
    "import time\n",
    "\n",
    "#//*** Start Timing the process\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#//*** X is Post Processed Data to evaluate\n",
    "data_model_x = df['processed']\n",
    "\n",
    "\n",
    "data_model_y = df['intcat']\n",
    "\n",
    "test_size=.98\n",
    "#continuous scoring model\n",
    "#scoring = 'r2_score'\n",
    "\n",
    "# split the data randomly into test/train sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_model_x, data_model_y, test_size =test_size)\n",
    "\n",
    "#categorical scoring model\n",
    "scoring = 'f1'\n",
    "\n",
    "print(f\"Calculating Model: {len(x_train)} elements\")\n",
    "\n",
    "classifier.fit(x_train,y_train)\n",
    "#scores = cross_val_score(regressor,x,y,cv=12,scoring=scoring)\n",
    "#print(scores)\n",
    "y_pred = classifier.predict(x_test)\n",
    "cm = confusion_matrix(y_pred, y_test)\n",
    "\n",
    "#Printing the accuracy\n",
    "print(f\"Test Size {test_size} - {len(x_train)} cm:\")\n",
    "print(cm)\n",
    "\n",
    "print(f\"Test Size {test_size} - {len(x_train)}: {int(time.time() - start_time)}s\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7618288296783232\n"
     ]
    }
   ],
   "source": [
    "#pickle model\n",
    "#https://stackabuse.com/scikit-learn-save-and-restore-models/\n",
    "print(classifier.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Neural Network Classifier with Keras###\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using Keras. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://oindrilasen.com/2021/02/how-to-install-and-import-keras-in-anaconda-jupyter-notebooks/\n",
    "\n",
    "#from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classifying Images ###\n",
    "In chapter 20 of the Machine Learning with Python Cookbook, implement the code found in section 20.15 classify MSINT images using a convolutional neural network. Report the accuracy of your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
