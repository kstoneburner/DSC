{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stoneburner, Kurt\n",
    "- ## DSC 550 - Week 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //****************************************************************************************\n",
    "# //*** Set Working Directory to thinkstats folder.\n",
    "# //*** This pseudo-relative path call should work on all Stoneburner localized projects. \n",
    "# //****************************************************************************************\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json \n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "#//*** nltk - Natural Language toolkit\n",
    "import nltk\n",
    "\n",
    "#//**** Requires the punkt module. Download if it doesn't exist\n",
    "try:\n",
    "    type(nltk.punkt)\n",
    "except:\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#//*** Stopwords requires an additional download\n",
    "try:\n",
    "    type(stopwords)\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Exercise: Build Your Text Classifiers ###\n",
    "\n",
    "**1. You can find the dataset controversial-comments.jsonl for this exercise in the Weekly Resources: Week 2 Data Files.**\n",
    "\n",
    "Pre-processing Text: For this part, you will start by reading the controversial-comments.jsonl file into a DataFrame. Then,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Temporary dictionary holds lists of JSON objects. pd.read_json generated an error. Likely due to the file\n",
    "#//*** Not being a complete JSON object. Each line is its on JSON object. \n",
    "#//*** Read the file line by line\n",
    "#//*** Parse each line of JSON. Parse each Key / Value pair. Each value is appeneded to a list. The lists are managed\n",
    "#//*** with tdict[key]. As long as the input file has the same number of keys for each line, then this works.\n",
    "#//*** Not sure what the canonical method is for converting items into a dataframe. But this technique has worked well\n",
    "#//*** in DSC530 and DSC540.\n",
    "\n",
    "#//*** Temporary Dictionary\n",
    "tdict = {}\n",
    "\n",
    "#//*** Read JSON into lists based on keys.\n",
    "with open('z_controversial-comments.jsonl', \"r\") as f:\n",
    "    \n",
    "    #//*** Initialize tdict. Each Key is used in both the JSON and tdict. This works on JSON of any length but is\n",
    "    #//*** limited to a flat construct which is fine for 2-D arrays.\n",
    "    #//*** 1.) Read the first line of the file\n",
    "    #//*** 2.) Convert the first line of JSON to a dictionary\n",
    "    #//*** 3.) Get each key/value in dictionary items\n",
    "    for key,value in json.loads(f.readline()).items():\n",
    "            #//*** Initialize a list of value, using tdict[key]\n",
    "            tdict[key] = [value]\n",
    "    \n",
    "    #//*** Process each remaining lines.\n",
    "    for line in f:\n",
    "        \n",
    "        #//*** 1.) Convert each line to a dictionary\n",
    "        #//*** 2.) get each key/value in dictionary\n",
    "        for key,value in json.loads(line).items():\n",
    "            \n",
    "            #//*** Add Value to the list associated with tdict[key]\n",
    "            tdict[key].append(value)\n",
    "#//*** Initialize a new dataframe\n",
    "con_df = pd.DataFrame()\n",
    "\n",
    "#//*** Loop through tdict, add each key as a column with value as the column data\n",
    "for key,value in tdict.items():\n",
    "    con_df[key] = value\n",
    "\n",
    "#//*** Delete tdict. It is unused and a 200mb+ object\n",
    "del tdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Convert all text to lowercase letters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Convert to lower case\n",
    "con_df['txt'] = con_df['txt'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Remove all punctuation from the text.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Remove new lines, I didn't see any samples of \\r\\n. But it is common enough. Replace it if it exists\n",
    "con_df['txt'] = con_df['txt'].str.replace(r'\\r?\\n',\"\")\n",
    "#//*** Remove plain ]n new lines\n",
    "con_df['txt'] = con_df['txt'].str.replace(r'\\n',\"\")\n",
    "\n",
    "#//*** Remove html entities, observed entities are &gt; and &lt;. All HTML entities begin with & and end with ;.\n",
    "#//*** Let's use regex to remove html entities\n",
    "con_df['txt'] = con_df['txt'].str.replace(r'&.*;',\"\")\n",
    "\n",
    "#//*** Remove elements flagged as [removed]\n",
    "con_df['txt'] = con_df['txt'].str.replace(r'\\[removed\\]',\"\")\n",
    "\n",
    "#//*** Remove elements flagged as [deleted]\n",
    "con_df['txt'] = con_df['txt'].str.replace(r'\\[deleted\\]',\"\")\n",
    "\n",
    "#//*** Some text should be empty with the removal of [removed] and [deleted]\n",
    "#//*** Remove the empty text\n",
    "con_df = con_df[ con_df['txt'].str.len() > 0]\n",
    "\n",
    "#//*** Remove punctuation using the example from the book\n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P') )\n",
    "con_df['txt'] = con_df['txt'].str.translate(punctuation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Remove stop words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Tokenize conf_df['txt']\n",
    "#//*** This Takes a wee bit to run\n",
    "con_df['process'] = con_df['txt'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** I'm not pythonic enough to do this on one line.\n",
    "#//*** This function removes stop_words from a list.\n",
    "#//*** Works with dataframe.apply()\n",
    "def remove_stop_words(input_list):\n",
    "    #//*** Load Stopwords\n",
    "    \n",
    "    \n",
    "    for word in input_list:\n",
    "        if word in stop_words:\n",
    "            input_list.remove(word)\n",
    "    return input_list\n",
    "\n",
    "#//*** The stop_words include punctuation. Stop Word Contractions will not be filtered out.\n",
    "stop_words = []\n",
    "\n",
    "#//*** Remove apostrophies from the stop_words\n",
    "for stop in stopwords.words('english'):\n",
    "    stop_words.append(stop.replace(\"'\",\"\"))\n",
    "\n",
    "#//*** Remove Stop words from the tokenized strings in the 'process' column\n",
    "con_df['process'] = con_df['process'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D. Apply NLTKâ€™s PorterStemmer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439    [bill, fuck, bagel, long, toaster, yet, hillar...\n",
      "440                          [ucuteman, got, rekt, lulz]\n",
      "442    [think, anyone, cares, business, management, p...\n",
      "443    [mean, hes, going, take, away, spouses, health...\n",
      "444    [wanting, argue, something, past, make, disqua...\n",
      "445                           [long, get, keep, chicago]\n",
      "446    [everytime, say, doubt, actually, adult, sound...\n",
      "447    [working, momsnever, underestimate, stupid, le...\n",
      "448    [put, effort, first, response, conversation, w...\n",
      "449    [1, post, positive, trump, news, instead, whin...\n",
      "450    [ive, saying, thinking, year, depressing, amer...\n",
      "452                                [subreddit, terrible]\n",
      "453    [mitt, romney, right, guy, office, positive, s...\n",
      "454    [dense, enough, taking, comments, section, rpo...\n",
      "455    [think, boils, understanding, lobbysts, first,...\n",
      "456    [true, cant, electors, vote, without, popular,...\n",
      "457    [lotta, research, done, thedonald, trump, univ...\n",
      "458             [especially, 17, year, old, girl, asked]\n",
      "459    [see, racist, north, carolina, bigots, bad, en...\n",
      "460    [thats, basically, came, live, texas, exercise...\n",
      "Name: process, dtype: object\n",
      "439    [bill, fuck, bagel, long, toaster, yet, hillar...\n",
      "440                          [ucuteman, got, rekt, lulz]\n",
      "442    [think, anyon, care, busi, manag, point, he, r...\n",
      "443    [mean, he, go, take, away, spous, healthcar, h...\n",
      "444    [want, argu, someth, past, make, disqualifi, r...\n",
      "445                           [long, get, keep, chicago]\n",
      "446    [everytim, say, doubt, actual, adult, sound, l...\n",
      "447    [work, momsnev, underestim, stupid, leftist, r...\n",
      "448    [put, effort, first, respons, convers, would, ...\n",
      "449    [1, post, posit, trump, news, instead, whine, ...\n",
      "450    [ive, say, think, year, depress, american, kno...\n",
      "452                                 [subreddit, terribl]\n",
      "453    [mitt, romney, right, guy, offic, posit, send,...\n",
      "454    [dens, enough, take, comment, section, rpolit,...\n",
      "455    [think, boil, understand, lobbyst, first, plac...\n",
      "456    [true, cant, elector, vote, without, popular, ...\n",
      "457    [lotta, research, done, thedonald, trump, univ...\n",
      "458                   [especi, 17, year, old, girl, ask]\n",
      "459    [see, racist, north, carolina, bigot, bad, eno...\n",
      "460    [that, basic, came, live, texa, exercis, cauti...\n",
      "Name: process, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#/*** Create Stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "#//*** Pre stemming sample\n",
    "print(con_df['process'][400:420])\n",
    "\n",
    "#//*** It's a pythonic answer\n",
    "#//*** 1.) Apply() an action to each row\n",
    "#//*** 2.) lambda word_list, each row is treated as word_list for the subsequent expression\n",
    "#//*** 3.) The base [ word for word in wordlist] would return each word in word_list as a list. \n",
    "#//*** 4.) [porter.stem(word) for word in word_list] - performs stemming on each word and returns a list\n",
    "con_df['process'] = con_df['process'].apply(lambda word_list: [porter.stem(word) for word in word_list] )\n",
    "\n",
    "#//*** post stemming sample\n",
    "print(con_df['process'][400:420])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//******************************\n",
    "#//*** Break out the Konami Code\n",
    "#//******************************\n",
    "#//*** Up, up, Down, Down, Left, Right, Left, Right, Select Start\n",
    "\n",
    "#//*** It's a lot of processing to get here. Save the dataframe to make this easier to pickup later\n",
    "con_df.to_csv(\"z_wk02_controversial_words_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Now that the data is pre-processed, you will apply three different techniques to get it into a usable form for model-building. Apply each of the following steps (individually) to the pre-processed data.**\n",
    "\n",
    "A. Convert each text entry into a word-count vector (see sections 5.3 & 6.8 in the Machine Learning with Python Cookbook).\n",
    "\n",
    "B. Convert each text entry into a part-of-speech tag vector (see section 6.7 in the Machine Learning with Python Cookbook).\n",
    "\n",
    "C. Convert each entry into a term frequency-inverse document frequency (tfidf) vector (see section 6.9 in the Machine Learning with Python Cookbook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Follow-Up Question**\n",
    "\n",
    "For the three techniques in problem (2) above, give an example where each would be useful.\n",
    "\n",
    "NOTE\n",
    "\n",
    "Running these steps on all of the data can take a while, so feel free to cut down on the number of texts (maybe 50,000) if your program takes too long to run. But be sure to select the text entries randomly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //*** CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //*** CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
