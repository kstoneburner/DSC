{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stoneburner, Kurt\n",
    "- ## DSC 550 - Week 09/10\n",
    "\n",
    "The code below is mostly supplied by the book. I adapted in metrics for recall, precision and f1 scores. This assignment was only able to compile on an Intel based processor, using the following command: **pip install intel-tensorflow**. This assignment was not compatible with my home AMD based PC. It required considerable effort and frustration to work through this. Because this project needed to be built on a separate PC, I broke this assignment out from the others. Assignment 1 and 2 were hugely CPU intensive, I took advantage of letting them run on separate PCs. Hence this assignment is a bit spread out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classifying Images ###\n",
    "In chapter 20 of the Machine Learning with Python Cookbook, implement the code found in section 20.15 classify MSINT images using a convolutional neural network. Report the accuracy of your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//**************************************************************************************************************************************************************\n",
    "#//*** code adpated: from https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "#//*** Functions to manually calculate recall, precision and f1 score\n",
    "#//**************************************************************************************************************************************************************\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60/60 [==============================] - 14s 218ms/step - loss: 0.9570 - accuracy: 0.6882 - f1_m: 0.6162 - precision_m: 0.8114 - recall_m: 0.5483 - val_loss: 0.1528 - val_accuracy: 0.9564 - val_f1_m: 0.9566 - val_precision_m: 0.9673 - val_recall_m: 0.9462\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - 13s 212ms/step - loss: 0.2002 - accuracy: 0.9408 - f1_m: 0.9405 - precision_m: 0.9560 - recall_m: 0.9255 - val_loss: 0.0914 - val_accuracy: 0.9724 - val_f1_m: 0.9726 - val_precision_m: 0.9772 - val_recall_m: 0.9681\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - 13s 217ms/step - loss: 0.1304 - accuracy: 0.9622 - f1_m: 0.9624 - precision_m: 0.9699 - recall_m: 0.9550 - val_loss: 0.0585 - val_accuracy: 0.9813 - val_f1_m: 0.9817 - val_precision_m: 0.9848 - val_recall_m: 0.9786\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - 14s 225ms/step - loss: 0.0985 - accuracy: 0.9720 - f1_m: 0.9719 - precision_m: 0.9775 - recall_m: 0.9664 - val_loss: 0.0546 - val_accuracy: 0.9814 - val_f1_m: 0.9813 - val_precision_m: 0.9834 - val_recall_m: 0.9791\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - 13s 225ms/step - loss: 0.0878 - accuracy: 0.9738 - f1_m: 0.9742 - precision_m: 0.9787 - recall_m: 0.9697 - val_loss: 0.0409 - val_accuracy: 0.9865 - val_f1_m: 0.9866 - val_precision_m: 0.9882 - val_recall_m: 0.9851\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - 14s 227ms/step - loss: 0.0704 - accuracy: 0.9786 - f1_m: 0.9787 - precision_m: 0.9818 - recall_m: 0.9756 - val_loss: 0.0412 - val_accuracy: 0.9863 - val_f1_m: 0.9864 - val_precision_m: 0.9881 - val_recall_m: 0.9847\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - 14s 226ms/step - loss: 0.0647 - accuracy: 0.9804 - f1_m: 0.9812 - precision_m: 0.9840 - recall_m: 0.9783 - val_loss: 0.0348 - val_accuracy: 0.9876 - val_f1_m: 0.9880 - val_precision_m: 0.9896 - val_recall_m: 0.9864\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - 14s 226ms/step - loss: 0.0600 - accuracy: 0.9810 - f1_m: 0.9811 - precision_m: 0.9835 - recall_m: 0.9787 - val_loss: 0.0333 - val_accuracy: 0.9892 - val_f1_m: 0.9890 - val_precision_m: 0.9903 - val_recall_m: 0.9878\n",
      "Epoch 9/10\n",
      "60/60 [==============================] - 13s 225ms/step - loss: 0.0514 - accuracy: 0.9841 - f1_m: 0.9844 - precision_m: 0.9863 - recall_m: 0.9824 - val_loss: 0.0296 - val_accuracy: 0.9907 - val_f1_m: 0.9904 - val_precision_m: 0.9914 - val_recall_m: 0.9895\n",
      "Epoch 10/10\n",
      "60/60 [==============================] - 13s 224ms/step - loss: 0.0477 - accuracy: 0.9859 - f1_m: 0.9859 - precision_m: 0.9877 - recall_m: 0.9841 - val_loss: 0.0339 - val_accuracy: 0.9888 - val_f1_m: 0.9890 - val_precision_m: 0.9900 - val_recall_m: 0.9880\n",
      "Loss: 0.03385162353515625\n",
      "Accuracy: 0.9887999892234802\n",
      "F1 Score: 0.9889889359474182\n",
      "Precision: 0.989990234375\n",
      "Recall: 0.9880191683769226\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "# Set that the color channel value will be first\n",
    "K.set_image_data_format(\"channels_first\")\n",
    "#K.set_image_data_format(\"channels_last\")\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set image information\n",
    "channels = 1\n",
    "height = 28\n",
    "width = 28\n",
    "\n",
    "# Load data and target from MNIST data\n",
    "(data_train, target_train), (data_test, target_test) = mnist.load_data()\n",
    "\n",
    "# Reshape training image data into features\n",
    "data_train = data_train.reshape(data_train.shape[0], channels, height, width)\n",
    "\n",
    "# Reshape test image data into features\n",
    "data_test = data_test.reshape(data_test.shape[0], channels, height, width)\n",
    "\n",
    "# Rescale pixel intensity to between 0 and 1\n",
    "features_train = data_train / 255\n",
    "features_test = data_test / 255\n",
    "\n",
    "# One-hot encode target\n",
    "target_train = np_utils.to_categorical(target_train)\n",
    "target_test = np_utils.to_categorical(target_test)\n",
    "number_of_classes = target_test.shape[1]\n",
    "\n",
    "# Start neural network\n",
    "network = Sequential()\n",
    "\n",
    "# Add convolutional layer with 64 filters, a 5x5 window, and ReLU activation function\n",
    "network.add(Conv2D(filters=64,\n",
    "                   kernel_size=(5, 5),\n",
    "                   #kernel_size=(3, 3),\n",
    "                   input_shape=(channels, width, height),\n",
    "                   activation='relu'))\n",
    "\n",
    "# Add max pooling layer with a 2x2 window\n",
    "network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "# Add layer to flatten input\n",
    "network.add(Flatten())\n",
    "\n",
    "# # Add fully connected layer of 128 units with a ReLU activation function\n",
    "network.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "# Add fully connected layer with a softmax activation function\n",
    "network.add(Dense(number_of_classes, activation=\"softmax\"))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\",f1_m,precision_m, recall_m]\n",
    "               ) # Accuracy performance metric\n",
    "\n",
    "# Train neural network\n",
    "network.fit(features_train, # Features\n",
    "            target_train, # Target\n",
    "            epochs=10, # Number of epochs\n",
    "            verbose=1, # Don't print description after each epoch\n",
    "            batch_size=1000, # Number of observations per batch\n",
    "            validation_data=(features_test, target_test)) # Data for evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.03385162353515625\n",
      "Accuracy: 0.9887999892234802\n",
      "F1 Score: 0.9889889359474182\n",
      "Precision: 0.989990234375\n",
      "Recall: 0.9880191683769226\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy, f1_score, precision, recall = network.evaluate(features_test, target_test, verbose=0)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {f1_score}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
