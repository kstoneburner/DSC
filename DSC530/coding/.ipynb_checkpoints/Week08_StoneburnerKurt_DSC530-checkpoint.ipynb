{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stoneburner, Kurt\n",
    "- ## DSC 530 - Week 08\n",
    "- ## Chapter 9, Exercise 1\n",
    "\n",
    "**Exercise:** As sample size increases, the power of a hypothesis test increases, which means it is more likely to be positive if the effect is real. Conversely, as sample size decreases, the test is less likely to be positive even if the effect is real.\n",
    "\n",
    "To investigate this behavior, run the tests in this chapter with different subsets of the NSFG data. You can use `thinkstats2.SampleRows` to select a random subset of the rows in a DataFrame.\n",
    "\n",
    "What happens to the p-values of these tests as sample size decreases? What is the smallest sample size that yields a positive test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //****************************************************************************************\n",
    "# //*** Set Working Directory to thinkstats folder.\n",
    "# //*** This pseudo-relative path call should work on all Stoneburner localized projects. \n",
    "# //****************************************************************************************\n",
    "\n",
    "import os\n",
    "import sys\n",
    "workingPath = os.getcwd().replace(\"coding\", \"ThinkStats2\\\\code\")\n",
    "sys.path.insert(1, workingPath)\n",
    "os.chdir(workingPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //*** Imports and Load Data\n",
    "import nsfg\n",
    "import thinkstats2\n",
    "import thinkplot\n",
    "import first\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "resp = nsfg.ReadFemResp()\n",
    "preg = nsfg.ReadFemPreg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_stat(input_ts_type, data1, data2):\n",
    "    \n",
    "    if input_ts_type ==  'mean_diff':\n",
    "        return abs( np.mean(data1) - np.mean(data2) )  \n",
    "    \n",
    "    if input_ts_type ==  'mean_diff_one_sided':\n",
    "        return np.mean(data1) - np.mean(data2)\n",
    "    \n",
    "    if input_ts_type ==  'std_diff':\n",
    "        return  np.std(data1) - np.std(data2)\n",
    "\n",
    "    if input_ts_type ==  'cor_pearson':\n",
    "        return  scipy.stats.pearsonr(data1,data2)\n",
    "    \n",
    "    # //*** Chi Squared Reference\n",
    "    # https://www.geeksforgeeks.org/python-pearsons-chi-square-test/\n",
    "    if input_ts_type == 'chi-squared':\n",
    "        \n",
    "        if isinstance(data1, pd.Series) == False:\n",
    "            data1 = pd.Series(data=data1)\n",
    "        if isinstance(data2, pd.Series) == False:\n",
    "            data2 = pd.Series(data=data2)\n",
    "            \n",
    "        # Chi squared is sum( (observed - expected) ** 2 / expected )\n",
    "        # observed = histogram of values\n",
    "        # expected is a PMF from histogram of data1 and data2 combined with each value multipled by the total number of values.\n",
    "        # perform chi on each data set and add them together for the result\n",
    "        # //*** Observed is a histgram of the data\n",
    "        observed1 = data1.value_counts().sort_index()\n",
    "        # //*** Expectation:\n",
    "        # //*** 1. Concatenate both data sets.\n",
    "        # //*** 2. Convert to pd.Series\n",
    "        # //*** 3. Build histogram of both data sets\n",
    "        # //*** 4. build a PMF from pd.Series\n",
    "        # //*** 5. Multiply each Series value by length of data1\n",
    "        expectation_pmf1 = build_pmf(pd.Series(data=np.hstack( (data1,data2) ) ).value_counts().sort_index()) * len(data1)\n",
    "        expectation_pmf2 = build_pmf(pd.Series(data=np.hstack( (data1,data2) ) ).value_counts().sort_index()) * len(data2)\n",
    "\n",
    "        # //*** Build a list of the difference of observed - expectations\n",
    "        # //*** Since expectations and observed are of different lengths, on the values with the same index are compared\n",
    "        difference1 = np.array ( [ observed1[x] - expectation_pmf1[x] for x in observed1.index ] )\n",
    "        # //*** Sampe process to build the expectations\n",
    "        expectation1 = np.array( [ expectation_pmf1[x] for x in observed1.index ] )\n",
    "\n",
    "        observed2 = data2.value_counts().sort_index()\n",
    "        difference2 = np.array ( [ observed2[x] - expectation_pmf2[x] for x in observed2.index ] )\n",
    "        expectation2 = np.array( [ expectation_pmf2[x] for x in observed2.index ] )\n",
    "\n",
    "        # //*** Once the differences and expectations are of identical length, they can be plugged into the formula\n",
    "        stat1 = sum( (difference1**2)/expectation1 )\n",
    "        stat2 = sum( (difference2**2)/expectation2 )\n",
    "        \n",
    "        # //*** Return the sum of the stats\n",
    "        return stat1 + stat2\n",
    "\n",
    "        \n",
    "\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis_test(input_dict):\n",
    "    \n",
    "    if 'type' not in input_dict.keys():\n",
    "        print(f\"hypothesis test requires a type value\")\n",
    "        return \"\"\n",
    "    \n",
    "    if 'data' not in input_dict.keys():\n",
    "        print(f\"Need valid data\")\n",
    "        return \"\"\n",
    "    \n",
    "    # //*** Assign the total number of tests to run.\n",
    "    # //*** Defaults to 1000\n",
    "    if 'count' in input_dict.keys():\n",
    "        max_test_count = input_dict['count']\n",
    "    else:\n",
    "        max_test_count = 1000\n",
    "    \n",
    "    test_statistic = -1\n",
    "    \n",
    "\n",
    "    # //*** Convert data to Lists\n",
    "    data1 = input_dict['data'][0]\n",
    "    data2 = input_dict['data'][1]\n",
    "\n",
    "    # //*** Sample a random subset of data\n",
    "    # //*** Defaults to all data\n",
    "    if 'sample' in input_dict.keys():\n",
    "        # //*** Get random sample and convert to list\n",
    "        data1 = getSample_from_series(data1,input_dict['sample'])[0]\n",
    "        data2 = getSample_from_series(data2,input_dict['sample'])[0]\n",
    "        \n",
    "        \n",
    "    \n",
    "    #data1 = list(data1)\n",
    "    #data2 = list(data2)\n",
    "\n",
    "    # //*** Concatinate the lists\n",
    "    combined_data = np.hstack( (data1,data2) )\n",
    "    \n",
    "    n = int( len(combined_data) / 2) \n",
    "\n",
    "    \n",
    "    # //*** get the test statistic. Function performs calculation based on type\n",
    "    # //*** Assume data has been properly validated at this point.\n",
    "    test_statistic = get_test_stat(input_dict['type'],data1,data2)\n",
    "    \n",
    "    null_count = 0\n",
    "    # //*** Build random permutations\n",
    "    for loop_counter in range(max_test_count):\n",
    "        \n",
    "        # //*** randomly shuffle the combined data\n",
    "        np.random.shuffle(combined_data)\n",
    "        \n",
    "        \n",
    "        # //*** Split shuffled data evenly\n",
    "        data1,data2 = combined_data[0:n],combined_data[n:len(combined_data)]\n",
    "        \n",
    "        loop_test_statistic = get_test_stat(input_dict['type'],data1,data2)\n",
    "        \n",
    "        # //*** If loop test statistic greater than test statistic. Then add to null count\n",
    "        if loop_test_statistic > test_statistic:\n",
    "            null_count = null_count + 1\n",
    "        \n",
    "    return (null_count / max_test_count)\n",
    "\n",
    "#sample_df,remainder_df = getSample_from_series(total_weight,.1)\n",
    "\n",
    "#total_weight['totalwgt_lb']\n",
    "#total_weight['agepreg']\n",
    "\n",
    "#Permutation Test\n",
    "#Difference in standard deviation\n",
    "#print(np.std( total_weight['totalwgt_lb']))\n",
    "#print(np.std( total_weight['agepreg']))\n",
    "#abs(np.std( total_weight['totalwgt_lb']) - np.std( total_weight['agepreg']))\n",
    "\n",
    "#get_p_scores(total_weight['totalwgt_lb'])\n",
    "\n",
    "#print(f\"{scipy.stats.pearsonr(total_weight['totalwgt_lb'],total_weight['agepreg'])}\")\n",
    "#correlation Testing\n",
    "#Testing Proportions\n",
    "#Chi Squared test\n",
    "\n",
    "#Difference of Means Permutation test\n",
    "# Generate a test statistic for reference.\n",
    "# This is the difference of the means\n",
    "# test statistic or t-value is the abs(difference of means)\n",
    "# combine both data sets.\n",
    "# 1000 thousand times:\n",
    "#      Randomly split combined data in half.\n",
    "#      Find the difference of the means for the random samples, for a random sample test statistic\n",
    "#      count the random test statistics that are greater than the base line test statistic\n",
    "# The P-value is the count / total tests run (1000)\n",
    "# The P-value represents the chance of the outcome occuring randomly.\n",
    "# Reference: https://www.ohbmbrainmappingblog.com/blog/a-brief-overview-of-permutation-testing-with-examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# //*** Combine the Hypothesis tests into a single function for display. To keep things tidy\n",
    "def display_hypothesis(input_type, input_data1, input_data2, input_iter=50,input_percentages=[.05,.1,.2,.5,.75]):\n",
    "    \n",
    "    test_statistic = get_test_stat(input_type,input_data1,input_data2)\n",
    "        \n",
    "    p_vals_test_statistic = [ hypothesis_test({ 'type':input_type,'data':(input_data1,input_data2) }) for i in range(pval_iter) ]\n",
    "    test_name = \"Test: XXXX\"\n",
    "    if input_type == 'mean_diff':\n",
    "        test_name = \"Mean Difference\"\n",
    "        \n",
    "    elif input_type == 'mean_diff_one_sided':\n",
    "        test_name = \"One Sided Mean Difference\"\n",
    "\n",
    "    elif input_type == 'cor_pearson':\n",
    "        test_name = \"Pearson's Correlation\"\n",
    "\n",
    "    elif input_type == 'std_diff':\n",
    "        test_name = \"Standard Deviation Difference\"\n",
    "        \n",
    "    elif input_type == 'chi-squared':\n",
    "        test_name = \"Chi-Squared\"\n",
    "        \n",
    "    print(f\"=======================================================\")\n",
    "    print(f\"{test_name}: {test_statistic}\")\n",
    "    print(f\"p-val: {np.mean(p_vals_test_statistic)} [ \\u03C3: {np.std(p_vals_test_statistic)} ]\")\n",
    "    print(f\"=======================================================\")\n",
    "\n",
    "    for x in input_percentages:\n",
    "        pvals = [ hypothesis_test({ 'type':'mean_diff','data':(first_preglen,other_preglen),'sample':x }) for i in range(input_iter) ]\n",
    "        pvals = np.array(pvals)\n",
    "        pstd = np.std(pvals)\n",
    "        #print(f\"================================================================================================\")\n",
    "        if x == 1:\n",
    "            x = x*10\n",
    "        print(f\"Sample size {round(x*100,2)}%: p-val: {round(np.mean(pvals),4)} [ \\u03C3: {pstd} ]\")\n",
    "        print(f\"================================================================================================\")\n",
    "\n",
    "    # //*** END display_hypothesis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //*** I initially built a random sampler to perform permutations.\n",
    "# //*** In hidsight I could have used np.random.shuffle and grabbed smaller sampling slices \n",
    "def getSample_from_series(input_df,input_percentage=.1):\n",
    "    #//*** Returns a random sampling of the input_df or Series\n",
    "    # //*** Series type conversion to dataFrame\n",
    "    \n",
    "    is_series = False\n",
    "    \n",
    "    if isinstance(input_df, pd.Series):\n",
    "        input_df = pd.DataFrame(input_df)\n",
    "        is_series = True\n",
    "    \n",
    "    sample_size = int(len(input_df) * input_percentage)\n",
    "    remainder_size = len(input_df) - sample_size\n",
    "    \n",
    "    #print(f\"{sample_size} {remainder_size}\")\n",
    "    \n",
    "    # //*** Set Loop safety as a function of sample_size and input_percentage\n",
    "    loop_safe_max = (sample_size * 1/input_percentage)**2\n",
    "    #print(f\"Loop Safe Max: {loop_safe_max}\")\n",
    "    \n",
    "    loop_safe = 0\n",
    "    sample_index = []\n",
    "    sample_dict = {}\n",
    "    \n",
    "    # //*** Get a random integer between 0 and size of input_series -1\n",
    "    # //*** Build a list of unique random numbers equal to the sample size\n",
    "    # //*** A dictionary is used to keep track of unique values.\n",
    "    while len(sample_index) < sample_size:\n",
    "        # //*** Pick a random integer between - and len(input_df) -1 \n",
    "        random_int = np.random.randint( (len(input_df)-1) )\n",
    "        \n",
    "        #//*** Convert integer to index key\n",
    "        random_int = input_df.index[random_int]\n",
    "        \n",
    "               \n",
    "        \n",
    "        #//*** Check if we've used this number\n",
    "        if random_int not in sample_dict.keys():\n",
    "            sample_index.append(random_int)\n",
    "            sample_dict[random_int] = \"\"\n",
    "        \n",
    "        loop_safe = loop_safe + 1\n",
    "        \n",
    "        if loop_safe > loop_safe_max:\n",
    "            print(\"Loop Maximum exceeded! Quitting for Safety!\")\n",
    "            break;\n",
    "    \n",
    "    #//*** Sort the values\n",
    "    sample_index = np.sort(sample_index)\n",
    "    \n",
    "    #//*** Build a list of values, 1 is sample, 0 is not sample\n",
    "    #//*** This will be a column to add to the input_df\n",
    "    is_sample = []\n",
    "    for x in input_df.index:\n",
    "        if x in sample_index:\n",
    "            is_sample.append(1)\n",
    "        else:\n",
    "            is_sample.append(0)\n",
    "    \n",
    "    #//*** Add is_sample column\n",
    "    input_df = input_df.assign( is_sample = is_sample)\n",
    "    \n",
    "    # //*** get sample and remainder dataframes based on is_sample attribute\n",
    "    # //*** Get the sample data frame and the remainder dataframe\n",
    "    sample_df = input_df[input_df ['is_sample'] == 1]\n",
    "    remainder_df = input_df[input_df ['is_sample'] == 0 ]\n",
    "    \n",
    "    # //**** Remove is_Sample Parameter\n",
    "    del sample_df['is_sample']\n",
    "    del remainder_df['is_sample']\n",
    "    \n",
    "    #print(len(sample_index))\n",
    "    #print(len(sample_df))\n",
    "\n",
    "    \n",
    "    if is_series == True:\n",
    "        sample_df = pd.Series(index= sample_df.index, data= sample_df.iloc[:,0])\n",
    "        remainder_df = pd.Series(index= remainder_df.index, data= remainder_df.iloc[:,0])\n",
    "    return sample_df, remainder_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //*****************************************\n",
    "# //*** Build a probability mass function\n",
    "# //*****************************************\n",
    "# //*** Returns Series as a PMF\n",
    "# //*****************************************\n",
    "def build_pmf(input_series):\n",
    "    output_series = input_series.copy()\n",
    "    total_values = input_series.sum()\n",
    "    for value,freq in output_series.items():\n",
    "        #print(f\"{value} {freq} {total_values} {freq/total_values}\")\n",
    "        output_series.loc[value] = freq/total_values\n",
    "    return output_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare Pregnancy Lengths between first births and other births\n",
      "Hypothesis: first births have a longer pregnancy length\n",
      "Null Hypothesis: first births and other births are no different\n",
      "Sampled: p-vals are run 5 times. With the p-Val's averaged together. This helps even out the\n",
      "sampled selection variance.\n",
      "=======================================================\n",
      "Chi-Squared: 119.84093173385406\n",
      "p-val: 0.0 [ σ: 0.0 ]\n",
      "=======================================================\n",
      "Sample size 5.0%: p-val: 0.3804 [ σ: 0.1601331945600287 ]\n",
      "================================================================================================\n",
      "Sample size 10.0%: p-val: 0.5612 [ σ: 0.039473535438316124 ]\n",
      "================================================================================================\n",
      "Sample size 20.0%: p-val: 0.192 [ σ: 0.17747112441183213 ]\n",
      "================================================================================================\n",
      "Sample size 50.0%: p-val: 0.1592 [ σ: 0.1031220635945577 ]\n",
      "================================================================================================\n",
      "Sample size 75.0%: p-val: 0.3828 [ σ: 0.3027245612764184 ]\n",
      "================================================================================================\n",
      "=======================================================\n",
      "Mean Difference: 0.0839628095830065\n",
      "p-val: 0.14020000000000002 [ σ: 0.011160645142642962 ]\n",
      "=======================================================\n",
      "Sample size 5.0%: p-val: 0.7852 [ σ: 0.09415604069840662 ]\n",
      "================================================================================================\n",
      "Sample size 10.0%: p-val: 0.569 [ σ: 0.2989869562372245 ]\n",
      "================================================================================================\n",
      "Sample size 20.0%: p-val: 0.447 [ σ: 0.23583638396142356 ]\n",
      "================================================================================================\n",
      "Sample size 50.0%: p-val: 0.1778 [ σ: 0.12032688810070674 ]\n",
      "================================================================================================\n",
      "Sample size 75.0%: p-val: 0.3598 [ σ: 0.18325108458069217 ]\n",
      "================================================================================================\n",
      "=======================================================\n",
      "One Sided Mean Difference: 0.0839628095830065\n",
      "p-val: 0.06620000000000001 [ σ: 0.005670978751503131 ]\n",
      "=======================================================\n",
      "Sample size 5.0%: p-val: 0.4388 [ σ: 0.2083011281774537 ]\n",
      "================================================================================================\n",
      "Sample size 10.0%: p-val: 0.6932 [ σ: 0.12885868228412084 ]\n",
      "================================================================================================\n",
      "Sample size 20.0%: p-val: 0.3592 [ σ: 0.2910583446664947 ]\n",
      "================================================================================================\n",
      "Sample size 50.0%: p-val: 0.4686 [ σ: 0.19723752178528303 ]\n",
      "================================================================================================\n",
      "Sample size 75.0%: p-val: 0.2632 [ σ: 0.09288142979088984 ]\n",
      "================================================================================================\n",
      "=======================================================\n",
      "Standard Deviation Difference: 0.14967231279798243\n",
      "p-val: 0.09059999999999999 [ σ: 0.00440908153700972 ]\n",
      "=======================================================\n",
      "Sample size 5.0%: p-val: 0.3234 [ σ: 0.27154123075511016 ]\n",
      "================================================================================================\n",
      "Sample size 10.0%: p-val: 0.4004 [ σ: 0.2124980941091002 ]\n",
      "================================================================================================\n",
      "Sample size 20.0%: p-val: 0.7422 [ σ: 0.23993699172907876 ]\n",
      "================================================================================================\n",
      "Sample size 50.0%: p-val: 0.2558 [ σ: 0.18105181578763577 ]\n",
      "================================================================================================\n",
      "Sample size 75.0%: p-val: 0.3382 [ σ: 0.27324962945995 ]\n",
      "================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# //*** Compare birth order birth weights\n",
    "\n",
    "pval_iter = 5\n",
    "sample_percentages = [.05,.1,.2,.5,.75]\n",
    "\n",
    "# //*** Get the birthweight Series data from the preg dataframe\n",
    "print(f\"Compare Pregnancy Lengths between first births and other births\")\n",
    "print(f\"Hypothesis: first births have a longer pregnancy length\")\n",
    "print(f\"Null Hypothesis: first births and other births are no different\")\n",
    "print(f\"Sampled: p-vals are run {pval_iter} times. With the p-Val's averaged together. This helps even out the\")\n",
    "print(f\"sampled selection variance.\")\n",
    "    \n",
    "preg = preg.dropna(subset=['totalwgt_lb','agepreg','birthord'])\n",
    "\n",
    "firsts_df = preg[preg.birthord == 1]\n",
    "others_df = preg[preg.birthord != 1]\n",
    "\n",
    "first_preglen = firsts_df['prglngth'][firsts_df['prglngth'] > 0]\n",
    "other_preglen = others_df['prglngth']\n",
    "\n",
    "\n",
    "display_hypothesis('chi-squared',first_preglen,other_preglen,pval_iter,sample_percentages,)\n",
    "display_hypothesis('mean_diff',first_preglen,other_preglen,pval_iter,sample_percentages,)\n",
    "display_hypothesis('mean_diff_one_sided',first_preglen,other_preglen,pval_iter,sample_percentages,)\n",
    "display_hypothesis('std_diff',first_preglen,other_preglen,pval_iter,sample_percentages,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Chapter 10, Exercise 1\n",
    "\n",
    "**Exercise:** Using the data from the BRFSS, compute the linear least squares fit for log(weight) versus height. How would you best present the estimated parameters for a model like this where one of the variables is log-transformed? If you were trying to guess someone’s weight, how much would it help to know their height?\n",
    "\n",
    "Like the NSFG, the BRFSS oversamples some groups and provides a sampling weight for each respondent. In the BRFSS data, the variable name for these weights is totalwt. Use resampling, with and without weights, to estimate the mean height of respondents in the BRFSS, the standard error of the mean, and a 90% confidence interval. How much does correct weighting affect the estimates?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brfss\n",
    "\n",
    "df = brfss.ReadBrfss(nrows=None)\n",
    "df = df.dropna(subset=['htm3', 'wtkg2'])\n",
    "heights, weights = df.htm3, df.wtkg2\n",
    "log_weights = np.log10(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate intercept and slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a scatter plot of the data and show the fitted line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot percentiles of the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute coefficient of determination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that $R^2 = \\rho^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Std(ys), which is the RMSE of predictions that don't use height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Std(res), the RMSE of predictions that do use height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much does height information reduce RMSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use resampling to compute sampling distributions for inter and slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the sampling distribution of slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the p-value of the slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the 90% confidence interval of slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the mean of the sampling distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the standard deviation of the sampling distribution, which is the standard error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample rows without weights, compute mean height, and summarize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample rows with weights.  Note that the weight column in this dataset is called `finalwt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //*** CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //*** CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
