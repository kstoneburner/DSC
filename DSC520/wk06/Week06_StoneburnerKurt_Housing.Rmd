---
title: 'Week06: Housing Data Analysis'
author: "Kurt Stoneburner"
date: "7/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

### Set working Directory
setwd("C:\\Users\\newcomb\\DSCProjects\\DSC\\DSC520\\wk06")
#setwd("L:\\stonk\\projects\\DSC\\DSC\\DSC520\\wk06")
library(QuantPsyc)


removeColsFromDF <- function(input_df, removeCols){
  ###########################################################################
  ### Remove Columns from data frame the Hard Way!
  ## # I don't like the clever answers from the Internet that I don't quite
  ### Understand.
  ###########################################################################
  ### Purpose: Return a data frame without the columns listed in removeCols
  ###########################################################################
  ### Variables #############################################################
  ###########################################################################
  ### input_df: Data Frame that needs columns removed
  ### removeCols: Vector of columns names as strings to be removed:
  ###             Example: c("date","Location","col1","col2")
  ###########################################################################
  
  
  ### Build a new vector of names by excluding values in removeCols
  newColNames <- lapply(colnames(input_df), function(x){
    
    if ( (x %in% removeCols) == FALSE) {return(x)}
  })
  
  ### Initialize output data frame with the first column from input. 
  ### This allows us to cbind in the loop. The first column will be
  ### removed later
  output_df = data.frame(input_df[1])
  
  ### Build output data frame by adding in columns from newColNames
  ### For each new column name
  for (i in 1:length(newColNames)){
    
    ########################################################################################
    ### Build column name
    ########################################################################################
    ### Not exactly sure why I need to unlist.
    ### probably should use a different function from lapply. Maybe capply? But this works
    ########################################################################################
    thisColName <- unlist(newColNames[i])
    
    output_df <- cbind(output_df,input_df[thisColName])
    
  }### END Each New Column Name
  
  return(output_df)
  
}### END RemoveColsFromDF

computeCI <- function(input_data){
  ##############################################
  #### Compute Confidence Intervals 
  ##############################################
  #### The best way to learn it is to do it!
  #### Calculation confirmed with t.test()
  ##############################################
  # Calculate Standard Error in R 
  # Standard Deviation divided by the sqrt of the sample size
  # using the sd (input_data) / sqrt(length(input_data))
  ### Determine standard error
  
  se <- sd(input_data) / sqrt(length(input_data))
  thisMean <- mean(input_data)
  ##############################################################################
  ### The boundaries are a function of the standard error and 95% of zScores lie
  ### between -1.96 and 1.96
  ##############################################################################
  lowerBoundary <- thisMean - (1.96 * se)
  upperBoundary <- thisMean + (1.96 * se)
  
  return (data.frame(
    
    lowerBoundary = lowerBoundary,
    mean = thisMean,
    upperBoundary = upperBoundary,
    lowerValue = (thisMean - lowerBoundary),
    upperValue = (thisMean + upperBoundary)
    
  ))
}



## Load the housing data
## We'll keep the raw table for access to the unaltered data. 
## This helps to keep housing_df (more) svelte (or svelter)
raw_housing_df <- read.csv("week-7-housing.csv")

housing_df <- removeColsFromDF(raw_housing_df,c("lon",
                                                "lat",
                                                "addr_full",
                                                "ctyname",
                                                "postalctyn",
                                                "prop_type",
                                                "year_renovated",
                                                "current_zoning",
                                                "bath_full_count",
                                                "bath_half_count",
                                                "bath_3qtr_count",
                                                "sitetype",
                                                "sale_warning",
                                                "present_use",
                                                "sale_reason",
                                                "sale_instrument",
                                                "Sale.Date"))[,-1]

#####################################################
### Convert spaces to periods in column names
#####################################################
thisNames <- colnames(housing_df)
colnames(housing_df) <- gsub(" ",".",thisNames)

thisNames <- colnames(raw_housing_df)
colnames(raw_housing_df) <- gsub(" ",".",thisNames)


#### Combine the bathrooms - using weighted values
bath_total <- raw_housing_df$bath_full_count + (raw_housing_df$bath_half_count *.5) + (raw_housing_df$bath_3qtr_count *.75)


housing_df <- cbind(housing_df,bath_total)

##########################################################################################################
#### Build house_age. This is the age of the house at Sale. 
##########################################################################################################
#### Get Year build and coerce into a number
year_built <- as.numeric(raw_housing_df$year_built)

#### Coerce the Date into a Date value and return just the year
sale_year <- format(as.Date(raw_housing_df$Sale.Date, format="%m/%d/%Y"),"%Y")

#### Convert Date (year) into Date (number)
sale_year <- as.numeric(sale_year)

#### The difference between the sale_year and year_built. Is the dwellings age.
house_age <- (sale_year - year_built)

################################################################################################################################
#### There are negative number in house_age. Some sale_dates are listing as sold before the date of building. 
#### I'm going to assume this is a data entry issue. Although there could be an issue where houses were actually sold before
#### they were built. A sampling of the sale dates all reference 2006, with multi-year differences in some cases.
#### I'm treating this as a data entry issue and converting all negative house_age values to 0. 
################################################################################################################################
house_age <- vapply(house_age, function(x){
  if (x < 0 ) { return(0)}
  return(x)
},numeric(1))

housing_df <- cbind(housing_df,house_age)


```

#### a. Explain why you chose to remove data points from your ‘clean’ dataset.
As a currently active house hunter the primary factors that affect price are:

  1. Size
  1. Location
  1. Bedrooms
  1. Bathrooms
  1. House Quality

Other factors that may have a lessor effect on price

  - Age of the home - There may be a premium based on the age of the home. 
  - Sale Warning - May indicate other issues with the property or repairs, that may deflate the selling price.
  - Zoning variation - Is the location zoned differently than use? This implies a redevlopment scenario, where an older home on a larger lot is replaced with a higher density structure(s). Although it is a possibility I looked at the relationship between zoning and present use. A unique list of zones was generated. The current_zoning column was converted to a numeric vector based on the index value in the unique zone list. 
The numeric zones were correlated with the present_use column. 

The resulting value:
``` {r zoning, echo=FALSE }

## Convert current_zoning into a numeric value.
## Build a number vector based on the unique values in current_zoning
zoning_zones <- unique(raw_housing_df$current_zoning)

### Return the index Value from zoning_zones

 coded_zoning <- vapply(raw_housing_df$current_zoning, function(x){
 thisIndex <- match(x,zoning_zones)
  return(thisIndex)},numeric(1))

print(cor(coded_zoning,raw_housing_df$present_use))

```
portrays no significant link between these variables. 

All other categories unrelated categories were removed.

One other note on the data set. The three bathroom variables have been summed into a new variable bath_total. Using the following weighted values:
  bath_full_count + (bath_half_count \*.5) + (bath_3qtr_count \*.75)

The bath_total is more in line with current listing standards and makes the relationship of bathroom count to the overall price easier to interpret.


#### b. Create two variables; one that will contain the variables Sale Price and Square Foot of Lot (same variables used from previous assignment on simple regression) and one that will contain Sale Price and several additional predictors of your choice. Explain the basis for your additional predictor selections.

1. Location has a large affect on price. Denser urban locations, as well as high income zip codes increase price. A 1 acre farm 100 miles from the nearest city is priced different than a 1 acre farm in a city. In my city of Castro Valley, similar homes may vary by $100k depending on which school district the house lies in. 
1. Bedrooms and Bathrooms directly reflect the needs of the American buyer which assumes individual space demands a higher premium than the overall shared space of the dwelling. 
1. The an increase in total living space should increase the sale price since structure costs increase with size. 

``` {r build lm, echo=TRUE}

salePrice_base_lm <-  lm(Sale.Price ~ sq_ft_lot, data=housing_df)

salePrice_naieve_lm <-  lm(Sale.Price ~ zip5 + bedrooms + bath_total + square_feet_total_living, data=housing_df)

summary(salePrice_base_lm)
summary(salePrice_naieve_lm)

```
And this is where I stop and challenge my assumptions. The criteria I listed above are what **"I"** look for in a dwelling. Clearly my biases are showing. An Adjusted R-squared:  0.2095 means I need to rethink the process.

The first selling point of real estate is LOCATION! LOCATION! LOCATION! Let's test the veracity of that claim using the data.
``` {r its_all_about_location, echo=TRUE }

unique(housing_df$zip5)

```

There are only four zip codes in the data. Location may be important, but not in this data set. This variable is not significant and should be binned.

Let's turn to correlation to determine significant variables.
``` {r lets_correlate,echo=TRUE}

cor(housing_df)[1,]

```

Based on the correlation, variables with a correlation of greater than .20 are good candidates to work with. I was genuinely surprised at the correlation value of bathrooms to sale price. The improved linear model looks like this:
``` {r improved_linear_model, echo=TRUE}

salePrice_base_lm <-  lm(Sale.Price ~ sq_ft_lot, data=housing_df)
salePrice_house_age_lm <-    lm(Sale.Price ~ square_feet_total_living + building_grade + bedrooms + bath_total + house_age, data=housing_df)
salePrice_year_built_lm <-    lm(Sale.Price ~ square_feet_total_living + building_grade + bedrooms + bath_total + year_built, data=housing_df)

```

I included a third model using house_age instead of year_built. These variable vary similarly with year_built having a slightly higher correlation than house_age. But house age at the time of sale feels like a more relevant metric. The year built variable doesn't neccessarily take into account the age of the home since the sale date is not included. A affect of age on a house built in 1945 and sold in 1945 should be different than the same house built in 1945 and sold in 2006. 

#### c. Execute a summary() function on two variables defined in the previous step to compare the model results. What are the R2 and Adjusted R2 statistics? Explain what these results tell you about the overall model. Did the inclusion of the additional predictors help explain any large variations found in Sale Price?
``` {r lm summary, echo=FALSE}
summary(salePrice_base_lm)
summary(salePrice_house_age_lm)
summary(salePrice_year_built_lm)

```
Price by Lot size Model           - Multiple R-squared:  0.01435, Adjusted R-squared:  0.01428
Price by Living Space ~ House Age - Multiple R-squared:   0.22,   Adjusted R-squared:  0.2197 
Price by Living Space ~ Year Built- Multiple R-squared:  0.2219,  Adjusted R-squared:  0.2216

The Adjusted R-squared values significantly improved with both models using additional predictors. The adjusted R-squared value for using Lot size as a predictor had barely any significance on predicted price. As indicated by the correlation calculation the Building Grade accounted for the largest variability in predicting price. I am a little surprised that bedrooms and bath_total have a negative coefficient using the year_built mode. The House age model reflects negative coefficents for house age and bedrooms. A negative relationship with house_age makes sense, and older house would be expected to have some negative pricing affect.

#### d. Considering the parameters of the multiple regression model you have created. What are the standardized betas for each parameter and what do the values indicate?

+---------------+----------------------------+------------------+----------------+--------------+--------------+--------------+
|               | square_feet_total_living   | building_grade   | bedrooms       |bath_total    | house_age    | year_built   |
+===============+============================+==================+================+==============+==============+==============+
| house_age     | 0.373945132                |  0.087315978     | -0.022322776   | 0.004951635  | -0.084478319 |              |
+---------------+----------------------------+------------------+----------------+--------------+--------------+--------------+
| year_built    | 0.373103476                |  0.084926945     | -0.018628440   | -0.001705028 |              |  0.098504715 |
+---------------+----------------------------+------------------+----------------+--------------+--------------+--------------+

For every increase in standard deviation of a given variable, the sale price will change by a stand deviation of sale price * Beta. Assuming the other variables remain constant.

For Example:
Looking at the standard deviations for the variables:
``` {r sale_price_Std_Deviation, echo=FALSE}
std_deviation_df <- data.frame ( 
  variable = 
    c("Sale Price", 
      "sqft Living", 
      "Building Grade",
      "bedrooms",
      "bathrooms",
      "house age"),
  std_dev = 
    c( sd(raw_housing_df$Sale.Price),
       sd(housing_df$square_feet_total_living),
       sd(housing_df$building_grade),
       sd(housing_df$bedrooms),
       sd(housing_df$bath_total),
       sd(housing_df$house_age)
)) 
print(std_deviation_df)

```

 Using the house age model
  
    Predicted Sale Price increases **$151,216** per **989.817** increase in square_feet_total_living
      - 989.817 std deviation of square_feet_total_living.
      - 151,216 = 404,381.10 (sale price standard deviation) \* .373945132 (square foot living Beta)

    Predicted Sale Price increases **$35308.93** per **1.09** increase in building_grade
      - 1.09 std deviation of building_grade
      - 35308.93 = 404,381.10 (sale price standard deviation) * 0.087315978 (building_grade Beta)

    Predicted Sale Price decreases **$9026.90** per **.88** increase in bedrooms
      - .88 std deviation of building_grade
      - $9026.90 = 404,381.10 (sale price standard deviation) * -0.022322776 (bedrooms Beta)

    Predicted Sale Price increases **$2002.35** per **.7** increase in bathrooms
      - .7 std deviation of building_grade
      - $2002.35 = 404,381.10 (sale price standard deviation) * 0.004951635 (bathrooms Beta)

    Predicted Sale Price decreases **$-34,212.12** per **17.5** increase in house_age
      - 17.5 std deviation of building_grade
      - $-34,212.12 = 404,381.10 (sale price standard deviation) * -0.084478319 (house_age Beta)
      
All of these predicted values assume the other variables remain constant. This also indicates the variable bath_total has < .001 influence on the predicted sale price. Making it a good candidate for elimination.

#### e. Calculate the confidence intervals for the parameters in your model and explain what the results indicate.

The 95% confidence intervals for Sales Price looks like this.

``` {r CIPrice, echo=FALSE }
price_CI <- computeCI(salePrice_house_age_lm$model$Sale.Price)
print(price_CI)
sqft_CI <- computeCI(salePrice_house_age_lm$model$square_feet_total_living)
grade_CI <- computeCI(salePrice_house_age_lm$model$building_grade)
bath_CI <- computeCI(salePrice_house_age_lm$model$bath_total)
bedroom_CI <- computeCI(salePrice_house_age_lm$model$bedrooms)
age_CI <- computeCI(salePrice_house_age_lm$model$house_age)
```
The lower and upper bounds indicate a range from the mean where 95% of the Sales Price values lie. 95% of the Sales Price data lies between \$6,987.83 and \$1,328,463. If this data set was from the Bay Area, $1.3m would not be an outlier

Looking at the 95% confidence interval for Square Feet Living Space
``` {r CISQft, echo=FALSE}
print(sqft_CI)
```
The range of values for Square feet living is notable that the lower bounds is 17sqft which is an impractical size. There may be data entry errors or properties that are lot only (0 Square feet living). These are outliers that should be taken into consideration. The upper value of 5096 sqft indicates there very well may be some very large properties for sale. Considering wealth and income distribution issues in America, these may indeed be outliers.

Looking at the 95% confidence interval for Building Grade
``` {r CI_BlgdGrade, echo=FALSE}
print(grade_CI)
```
A lower value of near zero likely indicates there are properties sold without buildings. I suspect this would align with properties with 0sq Ft. An upper bound of 16, indicates there are definitely some fancy homes in the top 5% of building grades.

Looking at the 95% confidence interval for bedrooms
``` {r CI_Bedrooms, echo=FALSE}
print(bedroom_CI)
```

Looking at the 95% confidence interval for bathrooms
``` {r CI_bath, echo=FALSE}
print(bath_CI)
```

Looking at the 95% confidence interval for House Age
``` {r age_CI, echo=FALSE}
print(age_CI)
```
95% of the homes are 0 - 36 years old.

#### f. Assess the improvement of the new model compared to your original model (simple regression model) by testing whether this change is significant by performing an analysis of variance.
Looking at the analysis of variance
``` {r anova_base, echo=FALSE}
print(anova(salePrice_base_lm,salePrice_house_age_lm))
print(anova(salePrice_base_lm,salePrice_year_built_lm))
```

Both models show an improved F score over the original model. The data continues to indicate that year_built is a slightly better indicator than house age, however I house_age continues to feel like an appropriate metric.


#### g. Perform casewise diagnostics to identify outliers and/or influential cases, storing each function's output in a dataframe assigned to a unique variable name.
``` {r setup_casewise, echo=TRUE}
casewise_df <- housing_df
casewise_df$cooks.distance <- cooks.distance(salePrice_house_age_lm)
casewise_df$leverage <- hatvalues(salePrice_house_age_lm)
casewise_df$covariance.ratios <- covratio(salePrice_house_age_lm)
```

#### h. Calculate the standardized residuals using the appropriate command, specifying those that are +-2, storing the results of large residuals in a variable you create.
``` {r generate_large_residuals, echo=TRUE}

large_residuals <- rstandard(salePrice_house_age_lm) > 2 | rstandard(salePrice_house_age_lm) < -2

```

#### i. Use the appropriate function to show the sum of large residuals.
There are 325 large residuals, which represents 0.02526234 of the data. In a normal distribution, having more than 5% of the data attributed to large residuals is an indicator of undue influence on the model.
``` {r large_residual_count, echo=TRUE}
sum(large_residuals)
sum(large_residuals) / nrow(housing_df)
```

#### j. Which specific variables have large residuals (only cases that evaluate as TRUE)?
Large_residuals is a logistical vector which can be used to select data that evaluates to TRUE. This method is used to create a data frame containing only the large outliers
``` {r 5percentCI, echo=TRUE}

large_residuals_df <- housing_df[large_residuals,c(
  "Sale.Price",
  "building_grade",
  "square_feet_total_living",
  "bedrooms",
  "bath_total",
  "house_age")]

```
Each parameter can be compared to the upper and lower bounds of the confidence interval to identify the specific outlier values.
``` {r the_other_5_percent, echo=FALSE}

sales_count_low <- nrow(large_residuals_df[which(large_residuals_df$Sale.Price < price_CI$lowerValue),])
sales_count_high <- nrow(large_residuals_df[which(large_residuals_df$Sale.Price > price_CI$upperValue),])

sqft_count_low <- nrow(large_residuals_df[which(large_residuals_df$square_feet_total_living < sqft_CI$lowerValue),])
sqft_count_high <- nrow(large_residuals_df[which(large_residuals_df$square_feet_total_living > sqft_CI$upperValue),])

bg_count_low <- nrow(large_residuals_df[which(large_residuals_df$building_grade < grade_CI$lowerValue),])
bg_count_high <- nrow(large_residuals_df[which(large_residuals_df$building_grade > grade_CI$upperValue),])

bath_count_low <- nrow(large_residuals_df[which(large_residuals_df$bath_total < bath_CI$lowerValue),])
bath_count_high <- nrow(large_residuals_df[which(large_residuals_df$bath_total > bath_CI$upperValue),])

bed_count_low <- nrow(large_residuals_df[which(large_residuals_df$bedrooms < bedroom_CI$lowerValue),])
bed_count_high <- nrow(large_residuals_df[which(large_residuals_df$bedrooms > bedroom_CI$upperValue),])

age_count_low <- nrow(large_residuals_df[which(large_residuals_df$house_age < age_CI$lowerValue),])
age_count_high <- nrow(large_residuals_df[which(large_residuals_df$house_age > age_CI$upperValue),])

print(paste0("Sales.Price - ",(sales_count_low + sales_count_high), " Total outliers"))
print(paste0("              ", sales_count_low, " contain values less than ",round(price_CI$lowerValue,0) ) )
print(paste0("              ", sales_count_high, " contain values more than ",round(price_CI$upperValue,0) ) ) 

print(paste0("Building Grade - ",(bg_count_low + bg_count_high), " Total outliers"))

print(paste0("Square Foot Living - ",(sqft_count_low + sqft_count_high), " Total outliers"))
print(paste0("              ", sqft_count_high, " contain values more than ",round(sqft_CI$upperValue,0) ) ) 

print(paste0("Bedrooms - ",(bed_count_low + bed_count_high), " Total outliers"))
print(paste0("              ", bed_count_low, " contain values less than ",round(bedroom_CI$lowerValue,2) ) )
print(paste0("              ", bed_count_high, " contain values more than ",round(bedroom_CI$upperValue,0) ) ) 

print(paste0("Bathrooms - ",(bath_count_low + bath_count_high), " Total outliers"))
print(paste0("              ", bath_count_low, " contain values less than ",round(bath_CI$lowerValue,2) ) )
print(paste0("              ", bath_count_high, " contain values more than ",round(bath_CI$upperValue,0) ) ) 

print(paste0("House Age - ",(age_count_low + age_count_high), " Total outliers"))
print(paste0("              ", age_count_low, " contain values less than ",round(age_CI$lowerValue,2) ) )
print(paste0("              ", age_count_high, " contain values more than ",round(age_CI$upperValue,0) ) ) 

```
The predominant outliers appear to be large expensive homes.

#### k. Investigate further by calculating the leverage, cooks distance, and covariance rations. Comment on all cases that are problematics.

#### l. Perform the necessary calculations to assess the assumption of independence and state if the condition is met or not.

#### m. Perform the necessary calculations to assess the assumption of no multicollinearity and state if the condition is met or not.

#### n. Visually check the assumptions related to the residuals using the plot() and hist() functions. Summarize what each graph is informing you of and if any anomalies are present.

#### o. Overall, is this regression model unbiased? If an unbiased regression model, what does this tell us about the sample vs. the entire population model?
