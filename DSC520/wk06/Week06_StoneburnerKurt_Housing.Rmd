---
title: 'Week06: Housing Data Analysis'
author: "Kurt Stoneburner"
date: "7/7/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

### Set working Directory
setwd("C:\\Users\\newcomb\\DSCProjects\\DSC\\DSC520\\wk06")
#setwd("L:\\stonk\\projects\\DSC\\DSC\\DSC520\\wk06")
library(readxl)
library(QuantPsyc)
library(car) #durbinwatsontest- dwt

removeColsFromDF <- function(input_df, removeCols){
  ###########################################################################
  ### Remove Columns from data frame the Hard Way!
  ## # I don't like the clever answers from the Internet that I don't quite
  ### Understand.
  ###########################################################################
  ### Purpose: Return a data frame without the columns listed in removeCols
  ###########################################################################
  ### Variables #############################################################
  ###########################################################################
  ### input_df: Data Frame that needs columns removed
  ### removeCols: Vector of columns names as strings to be removed:
  ###             Example: c("date","Location","col1","col2")
  ###########################################################################
  
  
  ### Build a new vector of names by excluding values in removeCols
  newColNames <- lapply(colnames(input_df), function(x){
    
    if ( (x %in% removeCols) == FALSE) {return(x)}
  })
  
  ### Initialize output data frame with the first column from input. 
  ### This allows us to cbind in the loop. The first column will be
  ### removed later
  output_df = data.frame(input_df[1])
  
  ### Build output data frame by adding in columns from newColNames
  ### For each new column name
  for (i in 1:length(newColNames)){
    
    ########################################################################################
    ### Build column name
    ########################################################################################
    ### Not exactly sure why I need to unlist.
    ### probably should use a different function from lapply. Maybe capply? But this works
    ########################################################################################
    thisColName <- unlist(newColNames[i])
    
    output_df <- cbind(output_df,input_df[thisColName])
    
  }### END Each New Column Name
  
  return(output_df)
  
}### END RemoveColsFromDF

computeCI <- function(input_data){
  ##############################################
  #### Compute Confidence Intervals 
  ##############################################
  #### The best way to learn it is to do it!
  #### Calculation confirmed with t.test()
  ##############################################
  # Calculate Standard Error in R 
  # Standard Deviation divided by the sqrt of the sample size
  # using the sd (input_data) / sqrt(length(input_data))
  ### Determine standard error
  
  se <- sd(input_data) / sqrt(length(input_data))
  thisMean <- mean(input_data)
  ##############################################################################
  ### The boundaries are a function of the standard error and 95% of zScores lie
  ### between -1.96 and 1.96
  ##############################################################################
  lowerBoundary <- thisMean - (1.96 * se)
  upperBoundary <- thisMean + (1.96 * se)
  
  return (data.frame(
    
    lowerBoundary = lowerBoundary,
    mean = thisMean,
    upperBoundary = upperBoundary,
    lowerValue = (thisMean - lowerBoundary),
    upperValue = (thisMean + upperBoundary)
    
  ))
}



## Load the housing data
## We'll keep the raw table for access to the unaltered data. 
## This helps to keep housing_df (more) svelte (or svelter)
raw_housing_df <- read_excel("week-6-housing.xlsx")

housing_df <- removeColsFromDF(raw_housing_df,c("lon",
                                                "lat",
                                                "addr_full",
                                                "ctyname",
                                                "postalctyn",
                                                "prop_type",
                                                "year_renovated",
                                                "current_zoning",
                                                "bath_full_count",
                                                "bath_half_count",
                                                "bath_3qtr_count",
                                                "sitetype",
                                                "sale_warning",
                                                "present_use",
                                                "sale_reason",
                                                "sale_instrument",
                                                "Sale Date"))[,-1]

#####################################################
### Convert spaces to periods in column names
#####################################################
thisNames <- colnames(housing_df)
colnames(housing_df) <- gsub(" ",".",thisNames)

thisNames <- colnames(raw_housing_df)
colnames(raw_housing_df) <- gsub(" ",".",thisNames)


#### Combine the bathrooms - using weighted values
bath_total <- raw_housing_df$bath_full_count + (raw_housing_df$bath_half_count *.5) + (raw_housing_df$bath_3qtr_count *.75)


housing_df <- cbind(housing_df,bath_total)

##########################################################################################################
#### Build house_age. This is the age of the house at Sale. 
##########################################################################################################
#### Get Year build and coerce into a number
year_built <- as.numeric(raw_housing_df$year_built)

#### Coerce the Date into a Date value and return just the year
sale_year <- format(as.Date(raw_housing_df$Sale.Date, format="%m/%d/%Y"),"%Y")

#### Convert Date (year) into Date (number)
sale_year <- as.numeric(sale_year)

#### The difference between the sale_year and year_built. Is the dwellings age.
house_age <- (sale_year - year_built)

################################################################################################################################
#### There are negative number in house_age. Some sale_dates are listing as sold before the date of building. 
#### I'm going to assume this is a data entry issue. Although there could be an issue where houses were actually sold before
#### they were built. A sampling of the sale dates all reference 2006, with multi-year differences in some cases.
#### I'm treating this as a data entry issue and converting all negative house_age values to 0. 
################################################################################################################################
house_age <- vapply(house_age, function(x){
  if (x < 0 ) { return(0)}
  return(x)
},numeric(1))

housing_df <- cbind(housing_df,house_age)


```

#### a. Explain why you chose to remove data points from your ‘clean’ dataset.
As a currently active house hunter the primary factors that affect price are:

  1. Size
  1. Location
  1. Bedrooms
  1. Bathrooms
  1. House Quality

Other factors that may have a lesser effect on price

  - Age of the home - There may be a premium based on the age of the home. 
  - Sale Warning - May indicate other issues with the property or repairs, that may deflate the selling price.
  - Zoning variation - Is the location zoned differently than use? This implies a redevlopment scenario, where an older home on a larger lot is replaced with a higher density structure(s). Although it is a possibility I looked at the relationship between zoning and present use. A unique list of zones was generated. The current_zoning column was converted to a numeric vector based on the index value in the unique zone list. 
The numeric zones were correlated with the present_use column. 

The resulting value:
``` {r zoning, echo=FALSE }

## Convert current_zoning into a numeric value.
## Build a number vector based on the unique values in current_zoning
zoning_zones <- unique(raw_housing_df$current_zoning)

### Return the index Value from zoning_zones

 coded_zoning <- vapply(raw_housing_df$current_zoning, function(x){
 thisIndex <- match(x,zoning_zones)
  return(thisIndex)},numeric(1))

print(cor(coded_zoning,raw_housing_df$present_use))

```
portrays no significant link between these variables. 

All other categories unrelated categories were removed.

One other note on the data set. The three bathroom variables have been summed into a new variable bath_total. Using the following weighted values:
  bath_full_count + (bath_half_count \*.5) + (bath_3qtr_count \*.75)

The bath_total is more in line with current listing standards and makes the relationship of bathroom count to the overall price easier to interpret.


#### b. Create two variables; one that will contain the variables Sale Price and Square Foot of Lot (same variables used from previous assignment on simple regression) and one that will contain Sale Price and several additional predictors of your choice. Explain the basis for your additional predictor selections.

1. The an increase in total living space should increase the sale price since structure costs increase with size. 
1. Building Grade denotes the overall quality of the structure which should directly affect price
1. Bedrooms and Bathrooms directly reflect the needs of the American buyer which assumes individual space demands a higher premium than the overall shared space of the dwelling. 
1. The age of the house should affect price. It is expected that an increasing house age will have negative effect of Sales Price. 

Correlation confirms these assumptions.
``` {r lets_correlate,echo=TRUE}

cor(housing_df)[1,]

```

Based on the correlation, variables with a correlation of greater than .20 are good candidates to work with. I was genuinely surprised at the correlation value of bathrooms to sale price. A linear model can built with these variables.
``` {r improved_linear_model, echo=TRUE}

salePrice_base_lm <-  lm(Sale.Price ~ sq_ft_lot, data=housing_df)
salePrice_house_age_lm <-    lm(Sale.Price ~ square_feet_total_living + building_grade + bedrooms + bath_total + house_age, data=housing_df)

```

Taking the age of the house into account feels like an important element of the model. I'm struggling with which is a better metric house_age, or year built. Year built has a slightly better Adjusted R squared and F score, which indicates it would be a slightly better metric. However, house age appears to affect the model as expected, the older the house the more negative impact on price. Since the parameter seems to better follow expectations (and the impact is small) i'm sticking with house age as the appropriate metric.

#### c. Execute a summary() function on two variables defined in the previous step to compare the model results. What are the R2 and Adjusted R2 statistics? Explain what these results tell you about the overall model. Did the inclusion of the additional predictors help explain any large variations found in Sale Price?
``` {r lm summary, echo=FALSE}
summary(salePrice_base_lm)
summary(salePrice_house_age_lm)

```
Price by Lot size Model           - Multiple R-squared:  0.01435, Adjusted R-squared:  0.01428
Price by Living Space ~ House Age - Multiple R-squared:   0.22,   Adjusted R-squared:  0.2197 

The Adjusted R-squared values significantly improved using additional predictors. The adjusted R-squared value for using Lot size as a predictor had barely any significance on predicted price. As indicated by the correlation calculation the Building Grade accounted for the largest variability in predicting price. I am a little surprised that bedrooms and bath_total have a negative coefficient using the year_built mode. The House age model reflects negative coefficents for house age and bedrooms. A negative relationship with house_age makes sense, and older house would be expected to have some negative pricing affect.

#### d. Considering the parameters of the multiple regression model you have created. What are the standardized betas for each parameter and what do the values indicate?

+---------------+----------------------------+------------------+----------------+--------------+--------------+--------------+
|               | square_feet_total_living   | building_grade   | bedrooms       |bath_total    | house_age    | year_built   |
+===============+============================+==================+================+==============+==============+==============+
| house_age     | 0.373945132                |  0.087315978     | -0.022322776   | 0.004951635  | -0.084478319 |              |
+---------------+----------------------------+------------------+----------------+--------------+--------------+--------------+

For every increase in standard deviation of a given variable, the sale price will change by a stand deviation of sale price * Beta. Assuming the other variables remain constant.

For Example:
Looking at the standard deviations for the variables:
``` {r sale_price_Std_Deviation, echo=FALSE}
std_deviation_df <- data.frame ( 
  variable = 
    c("Sale Price", 
      "sqft Living", 
      "Building Grade",
      "bedrooms",
      "bathrooms",
      "house age"),
  std_dev = 
    c( sd(raw_housing_df$Sale.Price),
       sd(housing_df$square_feet_total_living),
       sd(housing_df$building_grade),
       sd(housing_df$bedrooms),
       sd(housing_df$bath_total),
       sd(housing_df$house_age)
)) 
print(std_deviation_df)

```

    Predicted Sale Price increases **$151,216** per **989.817** increase in square_feet_total_living
      - 989.817 std deviation of square_feet_total_living.
      - 151,216 = 404,381.10 (sale price standard deviation) \* .373945132 (square foot living Beta)

    Predicted Sale Price increases **$35308.93** per **1.09** increase in building_grade
      - 1.09 std deviation of building_grade
      - 35308.93 = 404,381.10 (sale price standard deviation) * 0.087315978 (building_grade Beta)

    Predicted Sale Price decreases **$9026.90** per **.88** increase in bedrooms
      - .88 std deviation of building_grade
      - $9026.90 = 404,381.10 (sale price standard deviation) * -0.022322776 (bedrooms Beta)

    Predicted Sale Price increases **$2002.35** per **.7** increase in bathrooms
      - .7 std deviation of building_grade
      - $2002.35 = 404,381.10 (sale price standard deviation) * 0.004951635 (bathrooms Beta)

    Predicted Sale Price decreases **$-34,212.12** per **17.5** increase in house_age
      - 17.5 std deviation of building_grade
      - $-34,212.12 = 404,381.10 (sale price standard deviation) * -0.084478319 (house_age Beta)
      
All of these predicted values assume the other variables remain constant. This also indicates the variable bath_total has < .001 influence on the predicted sale price. Making it a good candidate for elimination.

#### e. Calculate the confidence intervals for the parameters in your model and explain what the results indicate.

The 95% confidence intervals for Sales Price looks like this.

``` {r CIPrice, echo=FALSE }
price_CI <- computeCI(salePrice_house_age_lm$model$Sale.Price)
print(price_CI)
sqft_CI <- computeCI(salePrice_house_age_lm$model$square_feet_total_living)
grade_CI <- computeCI(salePrice_house_age_lm$model$building_grade)
bath_CI <- computeCI(salePrice_house_age_lm$model$bath_total)
bedroom_CI <- computeCI(salePrice_house_age_lm$model$bedrooms)
age_CI <- computeCI(salePrice_house_age_lm$model$house_age)
```
The lower and upper bounds indicate a range from the mean where 95% of the Sales Price values lie. 95% of the Sales Price data lies between \$6,987.83 and \$1,328,463. (If this data set was from the Bay Area, $1.3m would not be an outlier).

Looking at the 95% confidence interval for Square Feet Living Space
``` {r CISQft, echo=FALSE}
print(sqft_CI)
```
The range of values for Square feet living is notable that the lower bounds is 17sqft which is an impractical size. There may be data entry errors or properties that are lot only (0 Square feet living). These are outliers that should be taken into consideration. The upper value of 5096 sqft indicates there very well may be some very large properties for sale. Considering wealth and income distribution issues in America, these may be a reflection of wealth distribution, therefore i'm not sure if should be considered outliers for the purposes of the model.

Looking at the 95% confidence interval for Building Grade
``` {r CI_BlgdGrade, echo=FALSE}
print(grade_CI)
```
A lower value of near zero likely indicates there are properties sold without buildings. I suspect this would align with properties with 0sq Ft. An upper bound of 16, indicates there are definitely some fancy homes in the top 5% of building grades.

Looking at the 95% confidence interval for bedrooms
``` {r CI_Bedrooms, echo=FALSE}
print(bedroom_CI)
```

Looking at the 95% confidence interval for bathrooms
``` {r CI_bath, echo=FALSE}
print(bath_CI)
```

Looking at the 95% confidence interval for House Age
``` {r age_CI, echo=FALSE}
print(age_CI)
```
95% of the homes are 0 - 36 years old.

#### f. Assess the improvement of the new model compared to your original model (simple regression model) by testing whether this change is significant by performing an analysis of variance.
Looking at the analysis of variance
``` {r anova_base, echo=FALSE}
print(anova(salePrice_base_lm,salePrice_house_age_lm))
```

The model shows an improved F score over the original model

#### g. Perform casewise diagnostics to identify outliers and/or influential cases, storing each function's output in a dataframe assigned to a unique variable name.
``` {r setup_casewise, echo=TRUE}
casewise_df <- housing_df
casewise_df$cooks.distance <- cooks.distance(salePrice_house_age_lm)
casewise_df$leverage <- hatvalues(salePrice_house_age_lm)
casewise_df$covariance.ratios <- covratio(salePrice_house_age_lm)
```

#### h. Calculate the standardized residuals using the appropriate command, specifying those that are +-2, storing the results of large residuals in a variable you create.
``` {r generate_large_residuals, echo=TRUE}

large_residuals <- rstandard(salePrice_house_age_lm) > 2 | rstandard(salePrice_house_age_lm) < -2

```

#### i. Use the appropriate function to show the sum of large residuals.
There are 325 large residuals, which represents 0.02526234 of the data. In a normal distribution, having more than 5% of the data attributed to large residuals is an indicator of undue influence on the model.
``` {r large_residual_count, echo=TRUE}
sum(large_residuals)
sum(large_residuals) / nrow(housing_df)
```

#### j. Which specific variables have large residuals (only cases that evaluate as TRUE)?
Large_residuals is a logistical vector which can be used to select data that evaluates to TRUE. This method is used to create a data frame containing only the large outliers.
``` {r 5percentCI, echo=TRUE}

large_residuals_df <- housing_df[large_residuals,c(
  "Sale.Price",
  "building_grade",
  "square_feet_total_living",
  "bedrooms",
  "bath_total",
  "house_age")]

```
Each parameter can be compared to the upper and lower bounds of the confidence interval to identify the specific outlier values.
``` {r the_other_5_percent, echo=FALSE}

sales_count_low <- nrow(large_residuals_df[which(large_residuals_df$Sale.Price < price_CI$lowerValue),])
sales_count_high <- nrow(large_residuals_df[which(large_residuals_df$Sale.Price > price_CI$upperValue),])

sqft_count_low <- nrow(large_residuals_df[which(large_residuals_df$square_feet_total_living < sqft_CI$lowerValue),])
sqft_count_high <- nrow(large_residuals_df[which(large_residuals_df$square_feet_total_living > sqft_CI$upperValue),])

bg_count_low <- nrow(large_residuals_df[which(large_residuals_df$building_grade < grade_CI$lowerValue),])
bg_count_high <- nrow(large_residuals_df[which(large_residuals_df$building_grade > grade_CI$upperValue),])

bath_count_low <- nrow(large_residuals_df[which(large_residuals_df$bath_total < bath_CI$lowerValue),])
bath_count_high <- nrow(large_residuals_df[which(large_residuals_df$bath_total > bath_CI$upperValue),])

bed_count_low <- nrow(large_residuals_df[which(large_residuals_df$bedrooms < bedroom_CI$lowerValue),])
bed_count_high <- nrow(large_residuals_df[which(large_residuals_df$bedrooms > bedroom_CI$upperValue),])

age_count_low <- nrow(large_residuals_df[which(large_residuals_df$house_age < age_CI$lowerValue),])
age_count_high <- nrow(large_residuals_df[which(large_residuals_df$house_age > age_CI$upperValue),])

print(paste0("Sales.Price - ",(sales_count_low + sales_count_high), " Total outliers"))
print(paste0("              ", sales_count_low, " contain values less than ",round(price_CI$lowerValue,0) ) )
print(paste0("              ", sales_count_high, " contain values more than ",round(price_CI$upperValue,0) ) ) 

print(paste0("Building Grade - ",(bg_count_low + bg_count_high), " Total outliers"))

print(paste0("Square Foot Living - ",(sqft_count_low + sqft_count_high), " Total outliers"))
print(paste0("              ", sqft_count_high, " contain values more than ",round(sqft_CI$upperValue,0) ) ) 

print(paste0("Bedrooms - ",(bed_count_low + bed_count_high), " Total outliers"))
print(paste0("              ", bed_count_low, " contain values less than ",round(bedroom_CI$lowerValue,2) ) )
print(paste0("              ", bed_count_high, " contain values more than ",round(bedroom_CI$upperValue,0) ) ) 

print(paste0("Bathrooms - ",(bath_count_low + bath_count_high), " Total outliers"))
print(paste0("              ", bath_count_low, " contain values less than ",round(bath_CI$lowerValue,2) ) )
print(paste0("              ", bath_count_high, " contain values more than ",round(bath_CI$upperValue,0) ) ) 

print(paste0("House Age - ",(age_count_low + age_count_high), " Total outliers"))
print(paste0("              ", age_count_low, " contain values less than ",round(age_CI$lowerValue,2) ) )
print(paste0("              ", age_count_high, " contain values more than ",round(age_CI$upperValue,0) ) ) 

```
The predominant outliers appear to be large expensive homes.

#### k. Investigate further by calculating the leverage, cooks distance, and covariance rations. Comment on all cases that are problematics.
There are no data with a Cooks Distance greater than one, therefore none of the data is having an outsized effect on the model.
``` { r cooks_analysis, echo=TRUE}
sum(casewise_df$cooks.distance > 1)
```

Checking leverage indicates a number of outliers. Nearly 5% of the data is attributed to having double the average leverage. Along with 2.5% attributed to triple the average leverage. Taking into account no data has a Cooks Distance greater than one, and the abnormal leverage is limited to 5% the model is still valid.

``` {r leverage, echo=FALSE}
k <-5
n <- nrow(casewise_df)

average_leverage <- (k + 1) / n

print("Number of cases with more than double the leverage:")
sum(casewise_df$leverage > average_leverage *2)
print("Representative percentage of the data")
sum(casewise_df$leverage > average_leverage * 2) / n

print("Number of cases with more than triple the leverage:")
sum(casewise_df$leverage > average_leverage *3)
print("Representative percentage of the data")
sum(casewise_df$leverage > average_leverage * 3) / n 

```

There are 472 cases that deviate significantly above the the covariance ratios, and 251 that deviate below. Again considering Cooks distance and relatively small sample size this should not have an outsized effect on the model. 

``` {r covariance_ration, echo=TRUE}

#### Checking if the covariance ratios deviate 3 times from the average leverage
upperCov <- 1 + 3*(k + 1)/n
lowerCov <- 1 - 3*(k + 1)/n

sum(casewise_df$covariance.ratios > upperCov)
sum(casewise_df$covariance.ratios < lowerCov)

```
#### l. Perform the necessary calculations to assess the assumption of independence and state if the condition is met or not.
``` {r dwt, echo=TRUE}

dwt(salePrice_house_age_lm)
```
The Durbin Watson statistic is less than 1 with a p of zero indicating the independence of errors condition is not met.

#### m. Perform the necessary calculations to assess the assumption of no multicollinearity and state if the condition is met or not.
vif() is used to test for no multicollinarity. The vif value and tolerance generate no cause for concern. The mean indicates there may be some bias in the data. The textbook states if the value substantially deviates from 1 there may be bias. There is no reference or scale for substantial, so i'm not sure how to intrepret a value of 2.
``` {r vif, echo=TRUE}

### VIF > 10 is cause for Concern
vif(salePrice_house_age_lm)

### VIF Tolerance < 0.2 cause for concern
1/vif(salePrice_house_age_lm)

### VIF mean substantially greater than 1 may be biased
mean(vif(salePrice_house_age_lm))

```
#### n. Visually check the assumptions related to the residuals using the plot() and hist() functions. Summarize what each graph is informing you of and if any anomalies are present.

Plotting the linear model generates four plots.

Plot#1 - Residuals vs fitted. The expected outcome is a random distribution of datapoints. The plotted residuals tend to cluster near the fitted values indicating there is an issue linearity and randomness in the data.

Plot#2 - Q-Q plot that show standardized residuals in deviations from normal. There is a significant portion of the model that deviates from the norm.

Plot#3 Square root of the standardized residuals vs fitted values - Values closer to the line indicate a normal distribution. As with other plots there are significant indications that housing in not normally distributed.

Plot#4 Standardized residuals vs leverage. The values indicate no there are no cases that exert undue influence over the model.

a Histogram of the standardized residuals indicates a leptokurtic distribution. 

``` { r plot_hist, echo=TRUE }
plot(salePrice_house_age_lm)
hist(salePrice_house_age_lm)
```
#### o. Overall, is this regression model unbiased? If an unbiased regression model, what does this tell us about the sample vs. the entire population model?

This regression model appears to be relatively unbiased. Cooke's distance indicates there are no data points exerting undue influence although there are abnormalities for the leverage and covariance ratios. The data appears to indicate that housing prices are not normally distributed.

 